547. **Beyond Log-concavity: Provable Guarantees for Sampling Multi-modal Distributions using Simulated Tempering Langevin Monte Carlo** --*HOLDEN LEE &middot; Andrej Risteski &middot; Rong Ge*
 > A key task in Bayesian machine learning is sampling from distributions that are only specified up to a partition function (i.e., constant of proportionality). One prevalent example of this is sampling posteriors in parametric  distributions, such as latent-variable generative models.  However sampling (even very approximately) can be #P-hard.
548. **Distributed $k$-Clustering for Data with Heavy Noise** --*Shi Li &middot; Xiangyu Guo*
 > 	In this paper, we consider the $k$-center/median/means clustering with outliers problems (or the $(k, z)$-center/median/means problems) in the distributed setting.  Most previous distributed algorithms have their communication costs linearly depending on $z$, the number of outliers.  Recently Guha et al. overcame this dependence issue by considering bi-criteria approximation algorithms that output solutions with $2z$ outliers.  For the case where $z$ is large, the extra $z$ outliers discarded by the algorithms might be too large, considering that the data gathering process might be costly. In this paper, we improve the number of outliers to the best possible $(1+\epsilon)z$, while maintaining the $O(1)$-approximation ratio and independence of communication cost on $z$.  The problems we consider include the $(k, z)$-center problem, and $(k, z)$-median/means problems in Euclidean metrics. Implementation of the our algorithm for $(k, z)$-center shows that it outperforms many previous algorithms, both in terms of the communication cost and quality of the output solution.  
549. **Preference Based Adaptation for Learning Objectives** --*Yao-Xiang Ding &middot; Zhi-Hua Zhou*
 > In many real-world learning tasks, it is hard to directly optimize the true performance measures, meanwhile choosing the right surrogate objectives is also difficult. Under this situation, it is desirable to incorporate an optimization of objective process into the learning loop based on weak modeling of the relationship between the true measure and the objective. In this work, we discuss the task of objective adaptation, in which the learner iteratively adapts the learning objective to the underlying true objective based on the preference feedback from an oracle. We show that when the objective can be linearly parameterized, this preference based learning problem can be solved by utilizing the dueling bandit model. A novel sampling based algorithm DL^2M is proposed to learn the optimal parameter, which enjoys strong theoretical guarantees and efficient empirical performance. To avoid learning a hypothesis from scratch after each objective function update, a boosting based hypothesis adaptation approach is proposed to efficiently adapt any pre-learned element hypothesis to the current objective. We apply the overall approach to multi-label learning, and show that the proposed approach achieves significant performance under various multi-label performance measures.
550. **Neural Architecture Optimization** --*Renqian Luo &middot; Fei Tian &middot; Tao Qin &middot; Enhong Chen &middot; Tieyan Liu*
 > Automatic neural architecture design has shown its potential in discovering powerful neural network architectures. Existing methods, no matter based on reinforcement learning or evolutionary algorithms (EA), conduct architecture search in a discrete space, which is highly inefficient. In this paper, we propose a simple and efficient method to automatic neural architecture design based on continuous optimization. We call this new approach neural architecture optimization (NAO). There are three key components in our proposed approach: (1) An encoder embeds/maps neural network architectures into a continuous space. (2) A predictor takes the continuous representation of a network as input and predicts its accuracy. (3) A decoder maps a continuous representation of a network back to its architecture. The performance predictor and the encoder enable us to perform optimization in the continuous space to find the embedding of a new architecture with potentially better accuracy. Such a better embedding is then decoded to a network by the decoder. Experiments show that the architecture discovered by our method is very competitive for image classification task on CIFAR-10 and language modeling task on PTB, outperforming or on par with the best results of previous architecture search methods. Furthermore, the computational resource is 10 times fewer than typical methods based on RL and EA.
551. **Learning Libraries of Subroutines for Neurally–Guided Bayesian Program Learning** --*Kevin Ellis &middot; Lucas Morales &middot; Mathias Meyer &middot; Armando Solar-Lezama &middot; Josh Tenenbaum*
 > Successful approaches to program induction require a hand-engineered   domain-specific language (DSL), constraining the space of allowed   programs and imparting prior knowledge of the domain.  We contribute   a program induction algorithm that learns a DSL while   jointly training a neural network to efficiently search for programs   in the learned DSL.  We use our model to synthesize functions on lists,   edit text, and solve symbolic regression problems, showing how the   model learns a domain-specific library of program components for   expressing solutions to problems in the domain.
552. **Constrained Graph Variational Autoencoders for Molecule Design** --*Qi Liu &middot; Miltiadis Allamanis &middot; Marc Brockschmidt &middot; Alexander Gaunt*
 > Graphs are ubiquitous data structures for representing interactions between entities. With an emphasis on the use of graphs to represent chemical molecules, we explore the task of learning to generate graphs that conform to a distribution observed in training data. We propose a variational autoencoder model in which both encoder and decoder are graph-structured. Our decoder assumes a sequential ordering of graph extension steps and we discuss and analyze design choices that mitigate the potential downsides of this linearization.  Experiments compare our approach with a wide range of baselines on the molecule generation task and show that our method is more successful at matching the statistics of the original dataset on semantically important metrics. Furthermore, we show that by using appropriate shaping of the latent space, our model allows us to design molecules that are (locally) optimal in desired properties.
553. **Deep State Space Models for Time Series Forecasting** --*Syama Sundar Rangapuram &middot; Matthias W Seeger &middot; Jan Gasthaus &middot; Lorenzo Stella &middot; Bernie Wang &middot; Tim Januschowski*
 > We present a novel approach to probabilistic time series forecasting that combines state space models with deep learning. By parametrizing a per-time-series linear state space model with a jointly-learned recurrent neural network, our method retains desired properties of state space models such as data efficiency and interpretability, while making use of the ability to learn complex patterns from raw data offered by deep learning approaches. Our method scales gracefully from regimes where little training data is available to regimes where data from millions of time series can be leveraged to learn accurate models. We provide qualitative as well as quantitative results with the proposed method, showing that it compares favorably to the state-of-the-art.
554. **Towards Robust Interpretability with Self-Explaining Neural Networks** --*David Alvarez Melis &middot; Tommi Jaakkola*
 > Most recent work on interpretability of complex machine learning models has focused on estimating a-posteriori explanations for previously trained models around specific predictions. Self-explaining models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general -- explicitness, faithfulness, and stability -- and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classifiers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization specifically tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability.
557. **Breaking the Activation Function Bottleneck through Adaptive Parameterization** --*Sebastian Flennerhag*
 > Standard neural network architectures are non-linear only by virtue of a simple element-wise activation function, making them both brittle and excessively large. In this paper, we consider methods for making the feed-forward layer more flexible while preserving its basic structure. We develop simple drop-in replacements that learn to adapt their parameterization conditional on the input, thereby increasing statistical efficiency significantly. We present an adaptive LSTM that advances the state of the art for the Penn Treebank and Wikitext-2 word-modeling tasks while using fewer parameters and converging in half as many iterations.
558. **On Neuronal Capacity** --*Pierre Baldi &middot; Roman Vershynin*
 > We define the capacity of a learning machine to be the logarithm of the number (or volume) of the functions it can implement. We review known results, and derive new results, estimating the capacity of several neuronal models:  linear and polynomial threshold gates, linear and polynomial threshold gates with constrained weights (binary weights, positive weights), and ReLU neurons. We also derive capacity estimates and bounds for fully recurrent networks and layered feedforward networks.
560. **Adversarial Scene Editing: Automatic Object Removal from Weak Supervision** --*Rakshith Shetty &middot; Mario Fritz &middot; Bernt Schiele*
 > While great progress has been made recently in automatic image manipulation, it has been limited to object centric images like faces or structured scene datasets. In this work, we take a step towards general scene-level image editing by developing an automatic interaction-free object removal model. Our model learns to find and remove objects from general scene images using image-level labels and unpaired data in a generative adversarial network (GAN) framework. We achieve this with two key contributions: a two-stage editor architecture consisting of a mask generator and image in-painter that co-operate to remove objects, and a novel GAN based prior for the mask generator that allows us to flexibly incorporate knowledge about object shapes. We experimentally show on two datasets that our method effectively removes a wide variety of objects using weak supervision only.
561. **Understanding Batch Normalization** --*Johan Björck &middot; Carla P Gomes &middot; Bart Selman*
 > Batch normalization is a ubiquitous deep learning technique that normalizes activations in intermediate layers. It is associated with improved accuracy and faster learning, but despite its enormous success there is little consensus regarding why it works. We aim to rectify this and take an empirical approach to understanding batch normalization. Our primary observation is that the higher learning rates that batch normalization enables have a regularizing effect that dramatically improves generalization of normalized networks, which is both demonstrated empirically and motivated theoretically. We show how both activations and gradient information becomes less input dependent across spatial dimensions and examples within a mini-batch for deep unnormalized networks, and show how this limit possible learning rates. Motivated by recent results in random matrix theory, we argue that this ill-conditioning is due to fluctuations in random initialization, shedding new light on classical initialization schemes and their consequences.
562. **Scalar Posterior Sampling with Applications ** --*Georgios Theocharous &middot; Zheng Wen &middot; Yasin Abbasi &middot; Nikos Vlassis*
 > We propose a practical  non-episodic PSRL algorithm that unlike recent state-of-the-art PSRL algorithms  uses a deterministic,  model-independent episode switching schedule. Our algorithm termed deterministic schedule PSRL (DS-PSRL) is efficient in terms of time, sample, and space complexity.  We prove a Bayesian regret bound under mild assumptions.  Our result is more generally applicable to multiple parameters and continuous state action problems.  We compare our algorithm with state-of-the-art PSRL algorithms on standard discrete and continuous problems from the literature.  Finally, we show how the assumptions of our algorithm satisfy a sensible  parameterization  for a  large class of problems in sequential recommendations.
563. **Training Deep Neural Networks with 8-bit Floating Point Numbers** --*Naigang Wang &middot; Jungwook Choi &middot; Daniel Brand &middot; Chia-Yu Chen &middot; Kailash Gopalakrishnan*
 > The state-of-the-art hardware platforms for training deep neural networks are moving from traditional single precision (32-bit) computations towards 16 bits of precision - in large part due to the high energy efficiency and smaller bit storage associated with using reduced-precision representations. However, unlike inference, training with numbers represented with less than 16 bits has been challenging due to the need to maintain fidelity of the gradient computations during back-propagation. Here we demonstrate, for the first time, the successful training of deep neural networks using 8-bit floating point numbers while fully maintaining the accuracy on a spectrum of deep learning models and datasets. In addition to reducing the data and computation precision to 8 bits, we also successfully reduce the arithmetic precision for additions (used in partial product accumulation and weight updates) from 32 bits to 16 bits through the introduction of a number of key ideas including chunk-based accumulation and floating point stochastic rounding. The use of these novel techniques lays the foundation for a new generation of hardware training platforms with the potential for 2-4 times improved throughput over today's systems.
564. **Depth-Limited Solving for Imperfect-Information Games** --*Noam Brown &middot; Tuomas Sandholm &middot; Brandon Amos*
 > A fundamental challenge in imperfect-information games is that states do not have well-defined values. As a result, depth-limited search algorithms used in single-agent settings and perfect-information games do not apply. This paper introduces a principled way to conduct depth-limited solving in imperfect-information games by allowing the opponent to choose among a number of strategies for the remainder of the game at the depth limit. Each one of these strategies results in a different set of values for leaf nodes. This forces an agent to be robust to the different strategies an opponent may employ. We demonstrate the effectiveness of this approach by building a master-level heads-up no-limit Texas hold'em poker AI that defeats two prior top agents using only a 4-core CPU and 16 GB of memory. Developing such a powerful agent would have previously required a supercomputer.
565. **Communication Compression for Decentralized Training** --*Hanlin Tang &middot; Shaoduo Gan &middot; Ce Zhang &middot; Ji Liu*
 > Optimizing distributed learning systems is an art of balancing between computation and communication. There have been two lines of research that try to deal with slower networks: {\em communication  compression} for low bandwidth networks, and {\em decentralization} for high latency networks. In this paper, We explore a natural question: {\em can the combination of both techniques lead to a system that is robust to both bandwidth and latency?}  Although the system implication of such combination is trivial, the underlying theoretical principle and algorithm design is challenging:  unlike centralized algorithms, simply compressing {\rc exchanged information, even in an unbiased stochastic way,  within the decentralized network would accumulate the error and cause divergence.}  In this paper, we develop a framework of quantized, decentralized training and propose two different strategies, which we call {\em extrapolation compression} and {\em difference compression}. We analyze both algorithms and prove  both converge at the rate of $O(1/\sqrt{nT})$  where $n$ is the number of workers and $T$ is the number of iterations, matching the convergence rate for full precision, centralized training. We validate  our algorithms and find that our proposed algorithm outperforms the best of merely decentralized and merely quantized algorithm significantly for networks with {\em both}  high latency and low bandwidth.
566. **Sparse Attentive Backtracking: Temporal Credit Assignment Through Reminding** --*Nan Ke &middot; Anirudh Goyal ALIAS PARTH GOYAL &middot; Olexa Bilaniuk &middot; Jonathan Binas &middot; Laurent Charlin &middot; Michael Mozer &middot; Chris Pal &middot; Yoshua Bengio*
 > Learning long-term dependencies in extended temporal sequences requires credit assignment to events far back in the past. The most common method for training recurrent neural networks, back-propagation through time (BPTT), requires credit information to be propagated backwards through every single step of the forward computation, potentially over thousands or millions of time steps. This becomes computationally expensive or even infeasible when used with long sequences. Importantly, biological brains are unlikely to perform such detailed reverse replay over very long sequences of internal states (consider days, months, or years.) However, humans are often reminded of past memories or mental states which are associated with the current mental state. We consider the hypothesis that such memory associations between past and present could be used for credit assignment through arbitrarily long sequences, propagating the credit assigned to the current state to the associated past state. Based on this principle, we study a novel algorithm which only back-propagates through a few of these temporal skip connections, realized by a learned attention mechanism that associates current states with relevant past states. We demonstrate in experiments that our method matches or outperforms regular BPTT and truncated BPTT in tasks involving particularly long-term dependencies, but without requiring the biologically implausible backward replay through the whole history of states. Additionally, we demonstrate that the proposed method transfers to longer sequences significantly better than LSTMs trained with BPTT and LSTMs trained with full self-attention.
567. **Improved Algorithms for Collaborative PAC Learning** --*Huy Nguyen &middot; Lydia Zakynthinou*
 > We study a recent model of collaborative PAC learning where $k$ players with $k$ different tasks collaborate to learn a single classifier that works for all tasks. Previous work showed that when there is a classifier that has very small errors on all tasks, there is a collaborative algorithm that finds a single classifier for all tasks and it uses $O((\ln (k))^2)$ times the sample complexity to learn a single task. In this work, we design new algorithms for both the realizable and the non-realizable settings using only $O(\ln (k))$ times the sample complexity to learn a single task. The sample complexity upper bounds of our algorithms match previous lower bounds and in some range of parameters, are even better than previous algorithms that are allowed to output different classifiers for different tasks.
568. **Rectangular Bounding Process** --*Xuhui Fan &middot; Bin Li &middot; Scott SIsson*
 > Stochastic partition models divide a multi-dimensional space into a number of rectangular regions, such that the data within each region exhibit certain types of homogeneity. Due to the nature of their partition strategy, existing partition models may create many unnecessary divisions in sparse regions when trying to describe data in dense regions. To avoid this problem we introduce a new parsimonious partition model -- the Rectangular Bounding Process (RBP) -- to efficiently partition multi-dimensional spaces, by employing a bounding strategy to enclose data points within rectangular bounding boxes. Unlike existing approaches, the RBP possesses several attractive theoretical properties that make it a powerful nonparametric partition prior on a hypercube. In particular, the RBP is self-consistent and as such can be directly extended from a finite hypercube to infinite (unbounded) space. We apply RBP to regression trees and relational models as a flexible partition prior. The experimental results validate the merit of RBP {in rich yet parsimonious expressiveness} compared to the state-of-the-art methods.
569. **VideoCapsuleNet: A Simplified Network for Action Detection** --*Kevin Duarte &middot; Yogesh Rawat &middot; Mubarak Shah*
 > The recent advances in Deep Convolutional Neural Networks (DCNNs) have shown extremely good results for video human action classification, however, action detection is still a challenging problem. The current action detection approaches follow a complex pipeline which involves multiple tasks such as tube proposals, optical flow, and tube classification. In this work, we present a more elegant solution for action detection based on the recently developed capsule network. We propose a 3D capsule network for videos, called VideoCapsuleNet: a unified network for action detection which can jointly perform pixel-wise action segmentation along with action classification. The proposed network is a generalization of capsule network from 2D to 3D, which takes a sequence of video frames as input. The 3D generalization drastically increases the number of capsules in the network, making capsule routing computationally expensive. We introduce capsule-pooling in the convolutional capsule layer to address this issue which makes the voting algorithm tractable. The routing-by-agreement in the network inherently models the action representations and various action characteristics are captured by the predicted capsules. This inspired us to utilize the capsules for action localization and the class-specific capsules predicted by the network are used to determine a pixel-wise localization of actions. The localization is further improved by parameterized skip connections with the convolutional capsule layers and the network is trained end-to-end with a classification as well as localization loss.
570. **Edward2: Simple, Dynamic, Accelerated** --*Dustin Tran &middot; Matthew Hoffman &middot; Dave Moore &middot; Christopher Suter &middot; Srinivas Vasudevan &middot; Alexey Radul &middot; Matthew Johnson &middot; Rif A. Saurous*
 > We describe Edward2, a probabilistic programming language (PPL) extending Edward. Edward2 distills the core of Edward down to a single abstraction—the random variable—while expanding its feature set. By blurring the line between modeling and computation, Edward2 enables numerous applications not possible in existing PPLs: the grammar VAE; learning to learn by variational inference by gradient descent; and GPU-accelerated NUTS. In a benchmark on VAEs, Edward2 sees a 5x speedup running on TPUs compared to GPU. In a benchmark on NUTS, Edward2 sees a 20x speedup over Stan and 7x over PyMC3.
571. **Diffusion Maps for Textual Network Embedding** --*Xinyuan Zhang &middot; Yitong Li &middot; Dinghan Shen &middot; Lawrence Carin*
 > Textual network embedding leverages rich text information associated with the network to learn low-dimensional vectorial representations of vertices. Rather than using typical natural language processing (NLP) approaches, recent research exploits the relationship of texts on the same edge to graphically embed text. However, these models neglect to measure the complete level of connectivity between any two texts in the graph. We present diffusion maps for textual network embedding (DMTE), integrating global structural information of the graph to capture the semantic relatedness between texts, with a diffusion-convolution operation applied on the text inputs. In addition, a new objective function is designed to efficiently preserve the high-order proximity using the graph diffusion. Experimental results show that the proposed approach outperforms state-of-the-art methods on the vertex-classification and link-prediction tasks.
572. **Blackbox Matrix×Matrix Gaussian Process Inference** --*Jacob Gardner &middot; Geoff Pleiss &middot; Kilian Weinberger &middot; David Bindel &middot; Andrew G Wilson*
 > Despite numerous advances in scalable models, the inference tools used for Gaussian processes (GPs) have yet to fully capitalize on recent trends in machine learning hardware. In this paper, we present an efficient and general approach to GP inference based on Blackbox Matrix-Matrix multiplication (BBMM). BBMM uses a modified batched version of the conjugate gradients algorithm to derive all terms required for training and inference in a single call. Adapting this algorithm to complex models simply requires a routine for efficient matrix-matrix multiplication with the kernel and its derivative. In addition, BBMM utilizes a specialized preconditioner that substantially speeds up convergence. In experiments, we show that BBMM efficiently utilizes GPU hardware, speeding up GP inference by an order of magnitude on a variety of popular GP models compared to existing approaches.
573. **cpSGD: Communication-efficient and differentially-private distributed SGD** --*Naman Agarwal &middot; Ananda Theertha Suresh &middot; Felix Xinnan Yu &middot; Sanjiv Kumar &middot; Brendan McMahan*
 >  Distributed stochastic gradient descent is an important subroutine in distributed learning. A setting of particular interest is when the clients are mobile devices, where two important concerns are communication efficiency and the privacy of the clients. Several recent works have focused on reducing the communication cost or introducing privacy guarantees, but none of the proposed communication efficient methods are known to be privacy preserving and none of the known privacy mechanisms are known to be communication efficient. To this end, we study algorithms that achieve both communication efficiency and differential privacy. For $d$ variables and $n \approx d$ clients, the proposed method uses $\cO(\log \log(nd))$ bits of communication per client per coordinate and ensures constant privacy.  We also improve previous analysis of the \emph{Binomial mechanism} showing that it achieves nearly the same utility as the Gaussian mechanism, while requiring fewer representation bits, which can be of independent interest.
574. **Towards Text Generation with Adversarially Learned Neural Outlines** --*Sandeep Subramanian &middot; Sai Rajeswar Mudumba &middot; Alessandro Sordoni &middot; Adam Trischler &middot; Aaron C Courville &middot; Chris Pal*
 > Recent progress in deep generative models has been fueled by two paradigms -- autoregressive and adversarial models. We propose a combination of both approaches with the goal of learning generative models of text. Our method first produces a high-level outline and then generates words sequentially, conditioning on both the outline and the previous output. We generate outlines with an adversarial model trained to approximate the distribution of sentences in a latent space induced by general-purpose sentence encoders. This provides strong, informative conditioning for the autoregressive stage. Our qualitative results show that this generative procedure yields natural-looking sentences and interpolations. Quantitative results suggest that conditioning information from generated outlines effectively guides the autoregressive model to produce realistic samples even at high temperatures with multinomial sampling.
575. **Generalisation in humans and deep neural networks** --*Robert Geirhos &middot; Carlos R. M. Temme &middot; Jonas Rauber &middot; Heiko H. Schütt &middot; Matthias Bethge &middot; Felix A. Wichmann*
 > Human visual object recognition is most often rapid and seemingly effortless. Until very recently, animate visual systems were the only ones capable of this remarkable computational feat. This has changed with the rise of deep neural networks (DNNs) achieving human-level object recognition performance. Intriguingly, a growing number of studies report similarities in the way DNNs and the human visual system process objects, suggesting that current DNNs may be good models of human visual object recognition. Yet there clearly exist important architectural and processing differences between state-of-the-art DNNs and the primate visual system. The potential behavioural consequences of these differences are not well understood. We aim to address this issue by conducting a large-scale behavioural comparison of human and DNN generalisation abilities towards twelve different image degradations. We find the human visual system to be more robust to nearly all of the tested image manipulations. In addition, we observe progressively diverging classification error-patterns between humans and three DNNs (ResNet-152, VGG-19, GoogLeNet) when the signal gets weaker. Furthermore, we show that DNNs trained directly on distorted images consistently surpass human performance on the exact distortion types they were trained on, yet they display extremely poor generalisation abilities when tested on other distortion types. Our results, based on some 83K psychophysical trials, indicate that there may be marked differences in the way humans and current DNNs perform visual object recognition. We envision that our findings as well as our carefully measured and freely available behavioural datasets provide a new useful benchmark for the computer vision community to improve the robustness of DNNs and a motivation for neuroscientists to identify mechanisms in the brain that could underly this robustness.
576. **Non-Adversarial Mapping with VAEs** --*Yedid Hoshen*
 > The study of cross-domain mapping without supervision has recently attracted much attention. Much of the recent progress was enabled by the use of adversarial training as well as cycle constraints. The practical difficulty of adversarial training motivates research into non-adversarial methods. In a recent paper, Hoshen and Wolf, showed that cross-domain mapping is possible without the use of cycles or GANs. Although promising, their approach suffers from several drawbacks including costly inference and an optimization variable for every training example preventing their method from using large training sets. We present an alternative approach which is able to achieve non-adversarial mapping using a novel form of Variational Auto-Encoder. Our method is much faster at inference time, is able to leverage large datasets and has a simple interpretation.
577. **Knowledge Distillation by On-the-Fly Native Ensemble** --*xu lan &middot; Xiatian Zhu &middot; Shaogang Gong*
 > Knowledge distillation is effective to train small and generalisable network models for meeting the low-memory and fast running requirements. Existing offline distillation methods rely on a strong pre-trained teacher, which enables favourable knowledge discovery and transfer but requires a complex two-phase training stage. Online alternatives address this limitation at the price of lacking a high-capacity teacher. In this work, we present an On-the-fly Native Ensemble (ONE) strategy for one-stage online distillation. Specifically, ONE trains only a single multi-branch network while simultaneously establishing a strong teacher on-the-fly to supervise the target network. Extensive evaluations show that ONE improves the generalisation performance a variety of deep neural networks more significantly than alternative methods on four image classification dataset: CIFAR10, CIFAR100, SVHN, and ImageNet, whilst having the computational efficiency advantages.
579. **Generative modeling for protein structures** --*Namrata Anand &middot; Possu  Huang*
 > Analyzing the structure and function of proteins is a key part of understanding biology at the molecular and cellular level. In addition, a major engineering challenge is to design new proteins in a principled and methodical way. Current computational modeling methods for protein design are slow and often require human oversight and intervention. Here, we apply Generative Adversarial Networks (GANs) to the task of generating protein structures, toward application in fast de novo protein design. We encode protein structures in terms of pairwise distances between alpha-carbons on the protein backbone, which eliminates the need for the generative model to learn translational and rotational symmetries. We then introduce a convex formulation of corruption-robust 3D structure recovery to fold the protein structures from generated pairwise distance maps, and solve these problems using the Alternating Direction Method of Multipliers. We test the effectiveness of our models by predicting completions of corrupted protein structures and show that the method is capable of quickly producing biochemically viable solutions. 
580. **Forecasting Treatment Responses Over Time Using Recurrent Marginal Structural Networks** --*Bryan Lim &middot; Ahmed Alaa &middot; Mihaela van der Schaar*
 > Electronic health records provide a rich source of data for machine learning methods to learn dynamic treatment responses over time. However, any direct estimation is hampered by the presence of time-dependent confounding, where actions taken are dependent on time-varying variables related to the outcome of interest. Drawing inspiration from marginal structural models, a class of methods in epidemiology which use propensity weighting to adjust for time-dependent confounders, we introduce the Recurrent Marginal Structural Network - a sequence-to-sequence architecture for forecasting a patient's expected response to a series of planned treatments. Using simulations of a state-of-the-art pharmacokinetic-pharmacodynamic (PK-PD) model of tumor growth, we demonstrate the ability of our network to accurately learn unbiased treatment responses from observational data, exhibiting robustness to changes in the policy of treatment assignments, and performance gains over benchmarks.
581. **Adaptive Learning with Unknown Information Flows** --*Yonatan Gur &middot; Ahmadreza Momeni*
 > An agent facing sequential decisions that are characterized by partial feedback needs to strike a balance between maximizing immediate payoffs based on available information, and acquiring new information that may be essential for maximizing future payoffs. This trade-off is captured by the multi-armed bandit (MAB) framework that has been studied and applied when at each time epoch payoff observations are collected on the actions that are selected at that epoch. In this paper we introduce a new, generalized MAB formulation in which additional information on each arm may appear arbitrarily throughout the decision horizon, and study the impact of such information flows on the achievable performance and the design of efficient decision-making policies. By obtaining matching lower and upper bounds, we characterize the (regret) complexity of this family of MAB problems as a function of the information flows. We introduce an adaptive exploration policy that, without any prior knowledge of the information arrival process, attains the best performance (in terms of regret rate) that is achievable when the information arrival process is a priori known. Our policy uses dynamically customized virtual time indexes to endogenously control the exploration rate based on the realized information arrival process.
582. **Multi-Agent Generative Adversarial Imitation Learning** --*Jiaming Song &middot; Hongyu Ren &middot; Dorsa Sadigh &middot; Stefano Ermon*
 > Imitation learning algorithms can be used to learn a policy from expert demonstrations without access to a reward signal. However, most existing approaches are not applicable in multi-agent settings due to the existence of multiple (Nash) equilibria and non-stationary environments. We propose a new framework for multi-agent imitation learning for general Markov games, where we build upon a generalized notion of inverse reinforcement learning. We further introduce a practical multi-agent actor-critic algorithm with good empirical performance. Our method can be used to imitate complex behaviors in high-dimensional environments with multiple cooperative or competing agents.
583. **Plug-in Estimation in High-Dimensional Linear Inverse Problems: A Rigorous Analysis** --*Alyson Fletcher &middot; Sundeep Rangan &middot; Subrata Sarkar &middot; Philip Schniter*
 > Estimating a vector $\mathbf{x}$ from noisy linear measurements $\mathbf{Ax+w}$ often requires use of prior knowledge or structural constraints on $\mathbf{x}$ for accurate reconstruction. Several recent works have considered combining linear least-squares estimation with a generic or plug-in ``denoiser" function that can be designed in a modular manner based on the prior knowledge about $\mathbf{x}$. While these methods have shown excellent performance, it has been difficult to obtain rigorous performance guarantees. This work considers plug-in denoising combined with the recently-developed Vector Approximate Message Passing (VAMP) algorithm, which is itself derived via Expectation Propagation techniques. It shown that the mean squared error of this ``plug-in"  VAMP can be exactly predicted for a large class of high-dimensional random $\Abf$ and denoisers. The method is illustrated in image reconstruction and parametric bilinear estimation.
584. **A Bayesian Approach to Generative Adversarial Imitation Learning** --*Wonseok Jeon &middot; Seokin Seo &middot; Kee-Eung Kim*
 > Generative adversarial training for imitation learning has shown promising results on high-dimensional and continuous control tasks. This paradigm is based on reducing the imitation learning problem to the density matching problem, where the agent iteratively refines the policy to match the empirical state-action visitation frequency of the expert demonstration. Although this approach has shown to robustly learn to imitate even with scarce demonstration, one must still address the inherent challenge that collecting trajectory samples in each iteration is a costly operation. To address this issue, we first propose a Bayesian formulation of generative adversarial imitation learning (GAIL), where the imitation policy and the cost function are represented as stochastic neural networks. Then, we show that we can significantly enhance the sample efficiency of GAIL leveraging the predictive density of the cost, on an extensive set of imitation learning tasks with high-dimensional states and actions.
585. **Constant Regret, Generalized Mixability, and Mirror Descent** --*Zakaria Mhammedi &middot; Robert C Williamson*
 > We consider the setting of prediction with expert advice; a learner makes predictions by aggregating those of a group of experts. Under this setting, and for the right choice of loss function and ``mixing'' algorithm, it is possible for the learner to achieve a constant regret regardless of the number of prediction rounds. For example, a constant regret can be achieved for \emph{mixable} losses using the \emph{aggregating algorithm}. The \emph{Generalized Aggregating Algorithm} (GAA) is a name for a family of algorithms parameterized by convex functions on simplices (entropies), which reduce to the aggregating algorithm when using the \emph{Shannon entropy}. For a given entropy $\Phi$, losses for which constant regret is possible using the GAA are called $\Phi$-mixable. Which losses are $\Phi$-mixable was previously left as an open question. We fully characterize $\Phi$-mixability and answer other open questions posed by \cite{Reid2015}. Additionally, by leveraging the connection between the \emph{mirror descent algorithm} and the update step of the GAA, we suggest a new \emph{adaptive} generalized aggregating algorithm and analyze its performance in terms of the regret bound.
586. **Hunting for Discriminatory Proxies in Linear Regression Models** --*Samuel Yeom &middot; Anupam Datta &middot; Matt Fredrikson*
 > A machine learning model may exhibit discrimination when used to make decisions involving people. One potential cause for such outcomes is that the model uses a statistical proxy for a protected demographic attribute. In this paper we formulate a definition of proxy use for the setting of linear regression and present algorithms for detecting proxies. Our definition follows recent work on proxies in classification models, and characterizes a model's constituent behavior that: 1) correlates closely with a protected random variable, and 2) is causally influential in the overall behavior of the model. We show that proxies in linear regression models can be efficiently identified by solving a second-order cone program, and further extend this result to account for situations where the use of a certain input variable is justified as a ``business necessity''. Finally, we present empirical results on two law enforcement datasets that exhibit varying degrees of racial disparity in prediction outcomes, demonstrating that proxies shed useful light on the causes of discriminatory behavior in models.
587. **Adaptive Sampling Towards Fast Graph Representation Learning** --*Wenbing Huang &middot; Tong Zhang &middot; Yu Rong &middot; Junzhou Huang*
 > Graph Convolutional Networks (GCNs) have become a crucial tool on learning representations of graph vertices. The main challenge of adapting GCNs on large-scale graphs is the scalability issue that it incurs heavy cost both in computation and memory due to the uncontrollable neighborhood expansion across layers. In this paper, we accelerate the training of GCNs through developing an adaptive layer-wise sampling method. By constructing the network layer by layer in a top-down passway, we sample the lower layer conditioned on the top one, where the sampled neighborhoods are shared by different parent nodes and the over expansion is avoided owing to the fixed-size sampling. More importantly, the proposed sampler is adaptive and applicable for explicit variance reduction, which in turn enhances the training of our method. Furthermore, we propose a novel and economical approach to promote the message passing over distant nodes by applying skip connections. Intensive experiments on several benchmarks verify the effectiveness of our method regarding the classification accuracy while enjoying faster convergence speed.
588. **MiME: Multilevel Medical Embedding of Electronic Health Records for Predictive Healthcare** --*Edward Choi &middot; Cao Xiao &middot; Jimeng Sun*
 > Deep learning models exhibit state-of-the-art performance for many predictive healthcare tasks using electronic health records (EHR) data, but these models typically require training data volume that exceeds the capacity of most healthcare systems. External resources such as medical ontologies are used to bridge the data volume constraint, but this approach is often not directly applicable or useful because of inconsistencies with terminology. To solve the data insufficiency challenge, we leverage the inherent multilevel structure of EHR data and, in particular, the encoded relationships among medical codes. We propose Multilevel Medical Embedding (MiME) which learns the multilevel embedding of EHR data while jointly performing auxiliary prediction tasks that rely on this inherent EHR structure without the need for external labels.  We conducted two prediction tasks, heart failure prediction and sequential disease prediction, where MiME outperformed baseline methods in diverse evaluation settings. In particular, MiME consistently outperformed all baselines when predicting heart failure on datasets of different volumes, especially demonstrating the greatest performance improvement (15% relative gain in PR-AUC over the best baseline) on the smallest dataset, demonstrating its ability to effectively model the multilevel structure of EHR data.
589. **COLA: Decentralized Linear Learning** --*lie he &middot; An Bian &middot; Martin Jaggi*
 > Decentralized machine learning is a promising emerging technique in view of global challenges of data ownership and privacy. We consider learning of linear classification and regression models, in the setting where the training data is decentralized over many user devices, and the learning algorithm must run on-device, on an arbitrary communication network, without a central coordinator. We propose COLA, a new decentralized training algorithm with strong theoretical guarantees and superior practical performance. Our scheme overcomes many limitations of existing methods in the distributed setting, and achieves communication efficiency, scalability, as well as elasticity and resilience to changes in user's data and participating devices.
590. **Third-order Smoothness Helps: Faster Stochastic Optimization Algorithms for Finding Local Minima** --*Yaodong Yu &middot; Pan Xu &middot; Quanquan Gu*
 > We propose stochastic optimization algorithms that can find local minima faster than existing algorithms for nonconvex optimization problems, by exploiting the third-order smoothness to escape non-degenerate saddle points more efficiently. More specifically, the proposed algorithm only needs $\tilde{O}(\epsilon^{-10/3})$ stochastic gradient evaluations to converge to an approximate local minimum $\mathbf{x}$, which satisfies $\|\nabla f(\mathbf{x})\|_2\leq\epsilon$ and $\lambda_{\min}(\nabla^2 f(\mathbf{x}))\geq -\sqrt{\epsilon}$ in the general stochastic optimization setting, where $\tilde{O}(\cdot)$ hides logarithm polynomial terms and constants. This improves upon the $\tilde{O}(\epsilon^{-7/2})$ gradient complexity achieved by the state-of-the-art stochastic local minima finding algorithms by a factor of $\tilde{O}(\epsilon^{-1/6})$. For the nonconvex finite-sum optimization, our algorithm also outperforms the best known algorithms in a certain regime. 
591. **Explaining Deep Learning Models -- A Bayesian Non-parametric Approach** --*Wenbo Guo &middot; Sui Huang &middot; Yunzhe Tao &middot; Xinyu Xing &middot; Lin Lin*
 > Understanding and interpreting how machine learning (ML) models make decisions have been a big challenge. While recent research has proposed various technical approaches to provide some clues as to how an ML model makes individual predictions, they cannot provide users with an ability to inspect a model as a complete entity. In this work, we propose a novel technical approach that augments a Bayesian non-parametric regression mixture model with multiple elastic nets. Using the enhanced mixture model, we can extract generalizable insights for a target model through a global approximation. To demonstrate the utility of our approach, we evaluate it on different ML models in the context of image recognition. The empirical results indicate that our proposed approach not only outperforms the state-of-the-art techniques in explaining individual decisions but also provides users with an ability to discover the vulnerabilities of the target ML models. 
592. **Lifelong Inverse Reinforcement Learning** --*Jorge Armando Mendez Mendez &middot; Shashank Shivkumar &middot; Eric Eaton*
 > Methods for learning from demonstration (LfD) have shown success in acquiring behavior policies by imitating a user. However, even for a single task, LfD may require numerous demonstrations. For versatile agents that must learn many tasks via demonstration, this process would substantially burden the user if each task were learned in isolation. To address this challenge, we introduce the novel problem of lifelong learning from demonstration, which allows the agent to continually build upon knowledge learned from previously demonstrated tasks to accelerate the learning of new tasks, reducing the amount of demonstrations required.  As one solution to this problem, we propose the first lifelong learning approach to inverse reinforcement learning, which learns consecutive tasks via demonstration, continually transferring knowledge between tasks to improve performance. 
593. **Expanding Holographic Embeddings for Knowledge Completion** --*Yexiang Xue &middot; Yang Yuan &middot; Ashish Sabharwal*
 > Neural models operating over structured spaces such as knowledge graphs require a continuous embedding of the discrete elements of this space (such as entities) as well as the relationships between them. Relational embeddings with high expressivity, however, have high model complexity, making them computationally difficult to train. We propose a new family of embeddings for knowledge graphs that interpolate between a method with high model complexity and one, namely Holographic embeddings, with low dimensionality and high training efficiency. This interpolation, termed HolEx, is achieved by concatenating several linearly perturbed copies of the original holographic embedding. We formally characterize the number of perturbed copies to provably recover the full relational interaction matrix between entities, leveraging ideas from Haar wavelets and compressed sensing. In practice, we find that using just a handful of perturbation vectors results in a much stronger knowledge completion system. On the Freebase FB15K dataset, HolEx outperforms original holographic embeddings by 13.7% on the HITS@10 metric, and the current state-of-the-art by 3.1% (absolute).
594. **Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis** --*Ye Jia &middot; Yu Zhang &middot; Ron Weiss &middot; Jonathan Shen &middot; Yonghui Wu &middot; zhifeng Chen*
 > We describe a neural network-based system for text-to-speech (TTS) synthesis that is able to generate speech audio in the voice of many different speakers, including those unseen during training. Our system consists of three independently trained components: (1) a speaker encoder network, trained on a speaker verification task using an independent dataset of noisy speech from thousands of speakers without transcripts, to generate a fixed-dimensional embedding vector from seconds of reference speech from a target speaker; (2) a sequence-to-sequence synthesis network based on Tacotron 2, which generates a mel spectrogram from text, conditioned on the speaker embedding; (3) an auto-regressive WaveNet-based vocoder that converts the mel spectrogram into a sequence of time domain waveform samples. We demonstrate that the proposed model is able to transfer the knowledge of speaker variability learned by the discriminatively-trained speaker encoder to the new task, and is able to synthesize natural speech from speakers that were not seen during training. We quantify the importance of training the speaker encoder on a large and diverse speaker set in order to obtain the best generalization performance. Finally, we show that randomly sampled speaker embeddings can be used to synthesize speech in the voice of novel speakers dissimilar from those used in training, indicating that the model has learned a high quality speaker representation.
595. **Importance Weighting and Varational Inference** --*Justin Domke &middot; Daniel Sheldon*
 > Recent work used importance sampling ideas for better variational bounds on likelihoods. We clarify the applicability of these ideas to pure probabilistic inference, by showing the resulting Importance Weighted Variational Inference (IWVI) technique is an instance of augmented variational inference, thus identifying the looseness in previous work. Experiments confirm IWVI's practicality for probabilistic inference. As a second contribution, we investigate inference with elliptical distributions, which improves accuracy in low dimensions, and convergence in high dimensions.
596. **Exponentiated Strongly Rayleigh Distributions** --*Zelda Mariet &middot; Suvrit Sra &middot; Stefanie Jegelka*
 > Strongly Rayleigh (SR) measures are discrete probability distributions over the subsets of a ground set. They enjoy strong negative dependence properties, as a result of which they assign higher probability to subsets of diverse elements. We introduce in this paper Exponentiated Strongly Rayleigh (ESR) measures, which sharpen (or smoothen) the negative dependence property of SR measures via a single parameter (the exponent) that can intuitively understood as an inverse temperature. We develop efficient MCMC procedures for approximate sampling from ESRs, and obtain explicit mixing time bounds for two concrete instances: exponentiated versions of Determinantal Point Processes and Dual Volume Sampling. We illustrate some of the potential of ESRs, by applying them to a few machine learning tasks; empirical results confirm that beyond their theoretical appeal, ESR-based models hold significant promise for these tasks.
597. **Sparsified SGD with Memory** --*Sebastian Stich &middot; Jean-Baptiste Cordonnier &middot; Martin Jaggi*
 > Nowadays machine learning applications require stochastic optimization algorithms that can be implemented on distributed systems. The communication overhead of the algorithms is a key bottleneck that hinders perfect scalability. Various recent works proposed to use quantization or sparsification techniques to reduce the amount of data that needs to be communicated, for instance by only sending the most significant entries of the stochastic gradient (top-k sparsification). Whilst this scheme shows good performance in practice it eluded theoretical analysis so far.
598. **End-to-end Symmetry Preserving Inter-atomic Potential Energy Model for Finite and Extended Systems** --*Linfeng Zhang &middot; Jiequn Han &middot; Han Wang &middot; Wissam  Saidi &middot; Roberto Car &middot; Weinan E*
 > Machine learning models are changing the paradigm of molecular modeling, which is a fundamental tool for material science, chemistry, and computational biology. Of particular interest is the inter-atomic potential energy surface (PES). Here we develop Deep Potential - Smooth Edition (DeepPot-SE), an end-to-end machine learning-based PES model, which is able to efficiently represent the PES for a wide variety of systems with the accuracy of ab initio quantum mechanics models. By construction, DeepPot-SE is extensive and continuously differentiable, scales linearly with system size, and preserves all the natural symmetries of the system. Further, we show that DeepPot-SE describes finite and extended systems including organic molecules, metals, semiconductors, and insulators with high fidelity.
599. **Semi-Supervised Learning with Declaratively Specified Entropy Constraints** --*Haitian Sun &middot; William W Cohen &middot; Lidong Bing*
 > We propose a technique for declaratively specifying strategies for semi-supervised learning (SSL).  The proposed method can be used to specify ensembles of semi-supervised learners, as well as agreement constraints and entropic regularization constraints between these learners, and can be used to model both well-known heuristics such as co-training, and novel domain-specific heuristics.  Our technique achieves consistent improvements over prior frameworks for specifying SSL techniques on a suite of well-studied SSL benchmarks, and obtains a new state-of-the-art result on a difficult relation extraction task.
600. **Limited Memory Kelley's Method Converges for Composite Convex and Submodular Objectives** --*Song Zhou &middot; Swati Gupta &middot; Madeleine Udell*
 > The original simplicial method ({\sc OSM}), a variant of the classic Kelley's cutting plane method, has been shown to converge to the minimizer of composite convex and submodular objectives, though no rate of convergence for this method was known. Moreover, {\sc OSM} is required to solve subproblems in each iteration whose size grows linearly in the number of iterations. We propose a limited memory version of Kelley's method ({\sc L-KM}) that is a novel adaptation of {\sc OSM} and requires limited memory independent of the iteration (at most $n+1$ constraints for an $n$-dimensional problem), while maintaining convergence to the optimal solution. We further show that the dual method of {\sc L-KM} is a special case of the Fully-Corrective Frank-Wolfe ({\sc FCFW}) method with approximate correction, thereby deriving a limited memory version of {\sc FCFW} method and proving a rate of convergence for {\sc L-KM}. Though we propose {\sc L-KM} for minimizing composite convex and submodular objectives, our results on limited memory version of FCFW hold for general polytopes, which is of independent interest.
601. **Maximum Causal Tsallis Entropy Imitation Learning** --*Kyungjae Lee &middot; Sungjoon Choi &middot; Songhwai Oh*
 > In this paper, we propose a novel maximum causal Tsallis entropy (MCTE) framework for imitation learning which can efficiently learn a sparse multi-modal policy distribution from demonstrations. We provide the full mathematical analysis of the proposed framework. First, the optimal solution of an MCTE problem is shown to be a sparsemax distribution, whose supporting set can be adjusted.  The proposed method has advantages over a softmax distribution in that it can exclude unnecessary actions by assigning zero probability. Second, we prove that an MCTE problem is equivalent to robust Bayes estimation in the sense of the Brier score. Third, we propose a maximum causal Tsallis entropy imitation learning (MCTEIL) algorithm with a sparse mixture density network (sparse MDN) by modeling mixture weights using a sparsemax distribution. In particular, we show that the causal Tsallis entropy of an MDN encourages exploration and efficient mixture utilization while Boltzmann Gibbs entropy is less effective. We validate the proposed method in two simulation studies and MCTEIL outperforms existing imitation learning methods in terms of average returns and learning multi-modal policies.
602. **Amortized Inference Regularization** --*Rui Shu &middot; Hung Bui &middot; Shengjia Zhao &middot; Mykel J Kochenderfer &middot; Stefano Ermon*
 > The variational autoencoder (VAE) is a popular model for density estimation and representation learning. Canonically, the variational principle suggests to prefer an expressive inference model so that the variational approximation is accurate. However, it is often overlooked that an overly-expressive inference model can be detrimental to the test set performance of both the amortized posterior approximator and, more importantly, the generative density estimator. In this paper, we leverage the fact that VAEs rely on amortized inference and propose techniques for amortized inference regularization (AIR) that control the smoothness of the inference model. We demonstrate that, by applying AIR, it is possible to improve VAE generalization on both inference and generative performance. Our paper challenges the belief that amortized inference is simply a mechanism for approximating maximum likelihood training and illustrates that regularization of the amortization family provides a new direction for understanding and improving generalization in VAEs.
603. **Top-k lists: Models and Algorithms** --*Flavio Chierichetti &middot; Anirban Dasgupta &middot; Shahrzad Haddadan &middot; Ravi Kumar &middot; Silvio Lattanzi*
 > The classic Mallows model is a widely-used tool to realize distributions on permutations.  Motivated by common practical situations, in this paper, we generalize Mallows to model distributions on \topk lists by using a suitable distance measure between \topk lists.  Unlike many earlier work, our model is both analytically tractable and computationally efficient.  We demonstrate this by studying two basic problems in this model, namely, sampling and reconstruction, from both algorithmic and practical points of view.
604. **The Physical Systems Behind Optimization Algorithms** --*Lin Yang &middot; Vladimir braverman &middot; Raman Arora &middot; Tuo Zhao*
 > We use differential equations based approaches to provide some {\it \textbf{physics}} insights into analyzing the dynamics of popular optimization algorithms in machine learning. In particular, we study gradient descent, proximal gradient descent, coordinate gradient descent, proximal coordinate gradient, and Newton's methods as well as their Nesterov's accelerated variants in a unified framework motivated by a natural connection of optimization algorithms to physical systems. Our analysis is applicable to more general algorithms and optimization problems {\it \textbf{beyond}} convexity and strong convexity, e.g. Polyak-\L ojasiewicz and error bound conditions (possibly nonconvex).
605. **Mean-field theory of graph neural networks in graph partitioning** --*Tatsuro Kawamoto*
 > A theoretical performance analysis of the graph neural network (GNN) is presented. For classification tasks, the neural network approach has the advantage in terms of flexibility that it can be employed in a data-driven manner, whereas Bayesian inference requires the assumption of a specific model. A fundamental question is then whether GNN has a high accuracy in addition to this flexibility. Moreover, whether the achieved performance is predominately a result of the backpropagation or the architecture itself is a matter of considerable interest. To gain a better insight into these questions, a mean-field theory of a minimal GNN architecture is developed for the graph partitioning problem. This demonstrates a good agreement with numerical experiments. 
606. **Adding One Neuron Can Eliminate All Bad Local Minima** --*SHIYU LIANG &middot; Ruoyu Sun &middot; Jason Lee &middot; R. Srikant*
 > One of the main difficulties in analyzing neural networks is the non-convexity of the loss function which may have many bad local minima. In this paper, we study the landscape of neural networks for binary classification tasks. Under mild assumptions, we prove that after adding one special neuron with a skip connection to the output, or one special neuron per layer, every local minimum is a global minimum. 
607. **Optimization of Smooth Functions with Noisy Observations: Local Minimax Rates** --*Yining Wang &middot; Sivaraman Balakrishnan &middot; Aarti Singh*
 > We consider the problem of global optimization of an unknown non-convex smooth function with noisy zeroth-order feedback. We propose a local minimax framework to study the fundamental difficulty of optimizing smooth functions with adaptive function evaluations. We show that for functions with fast growth around their global minima, carefully designed optimization algorithms can identify a near global minimizer with many fewer queries than worst-case global minimax theory predicts. For the special case of strongly convex and smooth functions, our implied convergence rates match the ones developed for zeroth-order convex optimization problems. On the other hand, we show that in the worst case no algorithm can converge faster than the minimax rate of estimating an unknown functions in linf-norm. Finally, we show that non-adaptive algorithms, although optimal in a global minimax sense, do not attain the optimal local minimax rate.
608. **Completing State Representations using Spectral Learning** --*Nan Jiang &middot; Alex Kulesza &middot; Satinder Singh*
 > A central problem in dynamical system modeling is state discovery---that is, finding a compact summary of the past that captures the information needed to predict the future. Predictive State Representations (PSRs) enable clever spectral methods for state discovery; however, while consistent in the limit of infinite data, these methods often suffer from poor performance in the low data regime. In this paper we develop a novel algorithm for incorporating domain knowledge, in the form of an imperfect state representation, as side information to speed spectral learning for PSRs. We prove theoretical results characterizing the relevance of a user-provided state representation, and design spectral algorithms that can take advantage of a relevant representation. Our algorithm utilizes principal angles to extract the relevant components of the representation, and is robust to mis-specification. Empirical evaluation on synthetic HMMs, an aircraft identification domain, and a gene splice dataset shows that, even with weak domain knowledge, the algorithm can significantly outperform standard PSR learning.
609. **A Bridging Framework for Model Optimization and Deep Propagation** --*  &middot; Shichao Cheng &middot; xiaokun liu &middot; Long Ma &middot; Xin Fan &middot; Zhongxuan Luo*
 > Optimizing designed mathematical models is the most fundamental methodology in statistic and learning areas. However, the generally designed schematic models may hard to extract complex data distributions in real-world scenarios. Recently, training deep propagations (i.e., networks) has gained promising performance in some particular applications. Unfortunately, existing network structures are often built in heuristic way, thus lack of principled interpretation and solid theoretical support. In this work, we provide a new framework, named Propagative Convergent Network (PCN), to bridge the gaps between these two different methodologies (i.e., model optimization and deep propagation) in a collaborative manner. On the one hand, we demonstrate how to utilize PCN as a deeply-trained solver for nonconvex model optimization. Different from other network-based iterations, which often lack theoretical investigations, we can successfully prove the strict convergence and estimate the convergence rate of PCN. On the other hand, by relaxing the constraints and performing end-to-end training, we also develop a PCN-based strategy to integrate domain knowledge (formulated as models) and data distributions (learned by networks), resulting in a generic ensemble learning framework for challenging applications. Extensive experiments verify our theoretical results and show the superiority of PCN against state-of-the-art methods.
610. **Submodular Field Grammars: Representation, Inference, and Application to Image Parsing** --*Abram L Friesen &middot; Pedro Domingos*
 > Natural scenes contain many layers of part-subpart structure, and distributions over them are thus naturally represented by stochastic image grammars, with one production per decomposition of a part. Unfortunately, in contrast to language grammars, where the number of possible split points for a production $A \rightarrow BC$ is linear in the length of $A$, in an image there is an exponential number of ways to split a region into subregions. This makes parsing intractable and requires image grammars to be severely restricted in practice, for example by allowing only rectangular regions. In this paper, we address this problem by associating with each production a submodular Markov random field whose labels are the subparts and whose labeling segments the current object into these subparts. We call the result a submodular field grammar (SFG). Finding the MAP split of a region into subregions is now tractable, and by exploiting this we develop an efficient approximate algorithm for MAP parsing of images with SFGs. Empirically, we present promising improvements in accuracy when using SFGs for scene understanding, and show exponential improvements in inference time compared to traditional methods, while returning comparable minima.
611. **Differentially Private Contextual Linear Bandits** --*Roshan Shariff &middot; Or Sheffet*
 > We study the contextual linear bandit problem, a version of the standard stochastic multi-armed bandit (MAB) problem, where a learner sequentially selects actions to maximize a reward which depends also on a user provided per-round context. Though the context is chosen arbitrarily or adversarially, the reward is assumed to be a stochastic function of a feature vector that encodes the context and selected action. Our goal is to devise private learners for the contextual linear bandit problem. We first show that using the standard definition of differential privacy results in linear regret. So instead, we adopt the notion of joint differential privacy, where we assume that the action chosen on day t is only revealed to user t and thus needn’t be kept private that day, only on following days. We give a general scheme converting the classic linear-UCB algorithm into a joint differentially private algorithm using the tree-based algorithm. We then apply either Gaussian noise or Wishart noise to acheive joint-differentially private algorithms and bound the resulting algorithms’ regrets. In addition, we give the first lower bound on the additional regret any private algorithms for the MAB problem must incur.
612. **SimplE Embedding for Link Prediction in Knowledge Graphs** --*Seyed Mehran Kazemi &middot; David Poole*
 > Knowledge graphs contain knowledge about the world and provide a structured representation of this knowledge. Current knowledge graphs contain only a small subset of what is true in the world. Link prediction approaches aim at predicting new links for a knowledge graph given the existing links between the entities. Tensor factorization approaches have proved promising for such link prediction problems. Proposed in 1927, Canonical Polyadic (CP) decomposition is among the first tensor factorization approaches. CP generally performs poorly for link prediction as it learns two independent embedding vectors for each entity, whereas they are really tied. We present a simple enhancement (which we call SimplE) of CP to allow the two embeddings of each entity to be learned dependently. The complexity of SimplE grows linearly with the size of embeddings. The embeddings learned through SimplE are interpretable, and certain types of background knowledge in terms of logical rules can be incorporated into these embeddings through weight tying. We prove SimplE is fully-expressive and derive a bound on the size of its embeddings for full expressivity. We show empirically that, despite its simplicity, SimplE outperforms several state-of-the-art tensor factorization techniques.
613. **Binary Rating Estimation with Graph Side Information** --*Kwangjun Ahn &middot; Kangwook Lee &middot; Hyunseung Cha &middot; Changho Suh*
 > While rich experimental evidences show that one can better estimate users' unknown ratings with the aid of graph side information such as social graphs, its theoretical understanding is still lacking. In this work, we study the binary rating estimation problem to understand the fundamental value of graph side information. Considering a simple correlation model between a rating matrix and a graph, we characterize the sharp threshold on the number of observed entries required to recover the rating matrix (called the optimal sample complexity) as a function of the quality of graph side information (to be detailed). To the best of our knowledge, we are the first to reveal when and by how much the graph side information reduces sample complexity. Further, we propose a computationally efficient algorithm that achieves the limit. Our experimental results on synthetic and real-world data demonstrate the superiority of our algorithm over the state of the arts.
614. **Can We Gain More from Orthogonality Regularizations in Training Deep Networks?** --*Nitin Bansal &middot; Zhangyang Wang*
 > This paper seeks to answer the question: as the (near-) orthogonality of weights is found to be a favorable property for training deep convolutional neural networks, how we can enforce it in more effective and easy-to-use ways? We develop novel orthogonality regularizations on training deep CNNs, utilizing various advanced analytical tools such as mutual coherence and restricted isometry property. These plug-and-play regularizations can be conveniently incorporated into training almost any CNN without extra hassle. We then benchmark their effects on three state-of-the-art models: ResNet, WideResNet, and ResNeXt, on CIFAR-10 and CIFAR-100 datasets. We observe consistent performance gains after applying those proposed regularizations, in terms of both the final accuracies achieved, and accelerated and more stable convergences.
615. **Inexact trust-region algorithm on Riemannian manifolds** --*Hiroyuki Kasai &middot; Bamdev Mishra*
 > We consider an inexact variant of the popular Riemannian trust-region algorithm for structured big-data minimization problems. The proposed algorithm approximates the gradient and the Hessian in addition to the solution of a (trust-region) sub-problem. We provide a total complexity bound to achieve $¥epsilon$-approximate second-order optimality under mild conditions on inexact gradient and Hessian. Addressing large-scale finite-sum problems, a sub-sampled algorithm is proposed as a practical algorithm, where gradient and Hessian are generated by a random sampling technique. Numerical evaluations of the principal components analysis (PCA) and the matrix completion (MC) problem demonstrate that the proposed algorithm outperforms state-of-the-art Riemannian deterministic and stochastic gradient algorithms. 
616. **BML: A High-performance, Low-cost Gradient Synchronization Algorithm for DML Training** --*  &middot; Dan Li &middot; Yang Cheng &middot; Jinkun Geng &middot; Yanshu Wang &middot; Shuai Wang &middot; Shu-Tao Xia &middot; Jianping Wu*
 > In distributed machine learning (DML), the network performance between machines significantly impacts the speed of iterative training. In this paper we propose BML, a new gradient synchronization algorithm with higher network performance and lower network cost than the current practice. BML runs on BCube network, instead of using the traditional Fat-Tree topology. BML algorithm is designed in such a way that, compared to the parameter server (PS) algorithm on a Fat-Tree network connecting the same number of server machines, BML achieves theoretically1/k of the gradient synchronization time, with k/5 of switches (the typical number of k is 2∼4). Experiments of MNIST and VGG-19 benchmarks on a testbed with 9 dual-GPU servers show that, BML reduces the job completion time of DML training by up to 56.4%
617. **Integrated accounts of behavioral and neuroimaging data using flexible recurrent neural network models** --*Amir Dezfouli &middot; Richard Morris &middot; Fabio Ramos &middot; Peter Dayan &middot; Bernard Balleine*
 > Neuroscience studies of human decision-making abilities commonly involve subjects completing a decision-making task while BOLD signals are recorded using fMRI. Hypotheses are tested about which brain regions mediate the effect of past experience, such as rewards, on future actions. One standard approach to this is model-based fMRI data analysis, in which a model is fitted to the behavioral data, i.e., a subject's choices, and then the neural data are parsed to find brain regions whose BOLD signals are related to the model's internal signals. However, the internal mechanics of such purely behavioral models are not constrained by the neural data, and therefore might miss or mischaracterize aspects of the brain. To address this limitation, we introduce a new method using recurrent neural network models that are flexible enough to be fitted jointly to the behavioral and neural data. We trained a model so that its internal states were suitably related to neural activity during the task, while at the same time its output predicted the next action a subject would execute. We then used the fitted model to create a novel visualization of the relationship between the activity in brain regions at different times following a reward and the choices the subject subsequently made. Finally, we validated our method using a previously published dataset. We showed that the model was able to recover the underlying neural substrates that were discovered by explicit model engineering in the previous work, and also derived new results regarding the temporal pattern of brain activity.
618. **Scalable Coordinated Exploration in Concurrent Reinforcement Learning** --*Maria Dimakopoulou &middot; Ian Osband &middot; Benjamin Van Roy*
 > We consider a team of reinforcement learning agents that concurrently operate in a common environment, and we develop an approach to efficient coordinated exploration that is suitable for problems of practical scale. Our approach builds on the seed sampling concept introduced in Dimakopoulou and Van Roy (2018) and on a randomized value function learning algorithm from Osband et al. (2016). We demonstrate that, for simple tabular contexts, the approach is competitive with those previously proposed in Dimakopoulou and Van Roy (2018) and with a higher-dimensional problem and a neural network value function representation, the approach learns quickly with far fewer agents than alternative exploration schemes.
619. **Differentially Private Uniformly Most Powerful Tests for Binomial Data** --*Jordan Awan*
 > We derive uniformly most powerful (UMP) tests for simple and one-sided hypotheses for a population proportion within the framework of Differential Privacy (DP), optimizing finite sample performance. We show that in general, DP hypothesis tests for exchangeable data can always be expressed as a function of the empirical distribution. Using this structure, we prove a ‘Neyman-Pearson lemma’ for binomial data under DP, where the DP-UMP only depends on the sample sum. Our tests can also be stated as a post-processing of a random variable, whose distribution we coin “Truncated-Uniform-Laplace” (Tulap), a generalization of the Staircase and discrete Laplace distributions. Furthermore, we obtain exact p-values, which are easily computed in terms of the Tulap random variable. We show that our results also apply to distribution-free hypothesis tests for continuous data. Our simulation results demonstrate that our tests have exact type I error, and are more powerful than current techniques.
620. **Bilevel Distance Metric Learning for Robust Image Recognition** --*Jie Xu &middot; Lei Luo &middot; Cheng Deng &middot; Heng Huang*
 > Metric learning, aiming to learn a discriminative Mahalanobis distance matrix M that can effectively reflect the similarity between data samples, has been widely studied in various image recognition problems. Most of the existing metric learning methods input the features extracted directly from the original data in the preprocess phase. What's worse, these features usually take no consideration of the local geometrical structure of the data and the noise existed in the data, thus they may not be optimal for the subsequent metric learning task. In this paper, we integrate both feature extraction and metric learning into one joint optimization framework and propose a new bilevel distance metric learning model. Specifically,  the lower level characterizes the intrinsic data structure using graph regularized sparse coefficients, while the upper level forces the data samples from the same class to be close to each other and pushes those from different classes far away.   In addition, leveraging the KKT conditions and the alternating direction method (ADM), we derive an efficient algorithm to solve the proposed new model. Extensive experiments on various occluded datasets demonstrate the effectiveness and robustness of our method.
621. **Regret Bounds for Robust Adaptive Control of the Linear Quadratic Regulator** --*Sarah Dean &middot; Horia Mania &middot; Nikolai Matni &middot; Benjamin Recht &middot; Stephen Tu*
 > We consider adaptive control of the Linear Quadratic Regulator (LQR), where an unknown linear system is controlled subject to quadratic costs. Leveraging recent developments in the estimation of linear systems and in robust controller synthesis, we present the first provably polynomial time algorithm that achieves sub-linear regret on this problem. We further study the interplay between regret minimization and parameter estimation by proving a lower bound on the expected regret in terms of the exploration schedule used by any algorithm. Finally, we conduct a numerical study comparing our robust adaptive algorithm to other methods from the adaptive LQR literature, and demonstrate the flexibility of our proposed method by extending it to a demand forecasting problem subject to state constraints.
622. **The Price of Privacy for Low-rank Factorization** --*Jalaj Upadhyay*
 > In this paper, we study what  price one has to pay to release \emph{differentially private low-rank factorization} of a matrix. We consider various settings that are close to the real world applications of low-rank factorization: (i) the manner in which matrices are updated (row by row or in an arbitrary manner), (ii) whether matrices are distributed or not, and (iii) how the output is produced (once at the end of all updates, also known as \emph{one-shot algorithms}  or continually). Even though these settings are well studied without privacy, surprisingly, there are no  private algorithm for these settings (except when a matrix is updated row by row). We present the first set of differentially private algorithms for all these settings.  
623. **Flexible and accurate inference and learning for deep generative models** --*Eszter Vértes &middot; Maneesh Sahani*
 > We introduce new approach to learning in hierarchical latent-variable generative models called the “distributed distributional code Helmholtz machine”, which emphasises flexibility and accuracy in the inferential process. In common with the original Helmholtz machine and later variational autoencoder algorithms (but unlike adverserial methods) our approach learns an explicit inference or “recognition” model to approximate the posterior distribution over the latent variables. Unlike in these earlier methods, the posterior representation is not limited to a narrow tractable parameterised form (nor is it represented by samples). To train the generative and recognition models we develop an extended wake-sleep algorithm inspired by the original Helmholtz Machine. This makes it possible to learn hierarchical latent models with both discrete and continuous variables, where an accurate posterior representation is essential. We demonstrate that the new algorithm outperforms current state-of-the-art methods on synthetic, natural image patch and the MNIST data sets.
624. **An Information-Theoretic Analysis of Thompson Sampling for Large Action Spaces** --*Shi Dong &middot; Benjamin Van Roy*
 > Information-theoretic Bayesian regret bounds of Russo and Van Roy capture the dependence of regret on prior uncertainty. However, this dependence is through entropy, which can become arbitrarily large as the number of actions increases.  We establish new bounds that depend instead on a notion of rate-distortion.  Among other things, this allows us to recover through information-theoretic arguments a near-optimal bound for the linear bandit.  We also offer a bound for the logistic bandit that dramatically improves on the best previously available, though this bound depends on an information-theoretic statistic that we have only been able to quantify via computation.
625. **Meta-Learning MCMC Proposals** --*Tongzhou Wang &middot; YI WU &middot; David Moore &middot; Stuart Russell*
 > Effective implementations of sampling-based probabilistic inference often require manually constructed, model-specific proposals. Inspired by recent progresses in meta-learning for training learning agents that can generalize to unseen environments, we propose a meta-learning approach to building effective and generalizable MCMC proposals. We parametrize the proposal as a neural network to provide fast approximations to block Gibbs conditionals. The learned neural proposals generalize to occurrences of common structural motifs across different models, allowing for the construction of a library of learned inference primitives that can accelerate inference on unseen models with no model-specific training required. We explore several applications including open-universe Gaussian mixture models, in which our learned proposals outperform a hand-tuned sampler, and a real-world named entity recognition task, in which our sampler yields higher final F1 scores than classical single-site Gibbs sampling.
626. **Differentially Private Robust PCA** --*Jalaj Upadhyay &middot; Raman Arora &middot; Vladimir braverman*
 > In this paper, we initiate the study of the following problem: given a private matrix $A \in \R^{n \times d}$, output a rank-$k$ matrix $B$, while satisfying differential privacy, such that  $ \norm{  A - B }_p \leq \alpha \mathsf{OPT}_k(A) + \gamma,$ where  $\norm{  M }_p$ is the entry-wise $\ell_p$ norm  and $\mathsf{OPT}_k(A):=\min_{\mathsf{rank}(X) \leq k} \norm{  A - X}_p$.  It is well known that low-rank approximation w.r.t. entrywise $\ell_p$-norm, for $p \in [1,2)$, yields robustness to gross outliers in the data.   We propose an algorithm that guarantees $\alpha=\widetilde{O}(k^2), \gamma=\widetilde{O}(k(n+d)/\varepsilon)$, runs in $\widetilde O((n+d)\poly~k)$ time and uses $O(k(n+d)\log k)$ space. This is an {\em exponential improvement}  in $\alpha$ over  known differentially private algorithms for $p=2$. We also study extensions to the streaming setting where entries of the matrix arrive in an arbitrary order and output is produced at the very end or continually.  We also study the related problem of differentially private robust subspace learning that requires us to output a rank-$k$ projection matrix $\Pi$ such that  $\norm{  A - A \Pi }_p \leq \alpha \mathsf{OPT}_k(A) + \tau.$ 
627. **JCNN-sLDA: Joint constraint neural networks (JCNN), a novel factored neural network structure with applications to supervised text classification** --*Harsh Shrivastava &middot; Eugene Bart &middot; Bob Price &middot; Hanjun Dai &middot; Bo Dai &middot; Srinivas Aluru*
 > We demonstrate a novel method of computing deep latent representations using joint constraints represented as a set of neural networks. The model is more flexible than traditional graphical models based on exponential family distributions, but incorporates more domain specific prior structure than traditional deep networks or variational autoencoders. We illustrate the technique by showing we can transfer the constraints inherent in the joint distribution of the popular Latent Dirichlet Allocation (LDA) model to a novel form of neural network we call a joint constraint neural network (JCNN). We show that the theoretical advantages of the method are realized in practice: the JCNN demonstrates superior text classification performance compared to both previous supervised LDA and deep neural approaches.
628. **TETRIS: TilE-matching the TRemendous Irregular Sparsity** --*Yu Ji &middot; Ling Liang &middot; Lei Deng &middot; Youyang Zhang &middot; Youhui Zhang &middot; Yuan Xie*
 > Compressing neural networks by pruning weights with small magnitudes can significantly reduce the computation and storage cost. Although pruning makes the model smaller, it is difficult to get practical speedup in modern computing platforms such as CPU and GPU due to the irregularity. Recent efforts on hardware-friendly pruning involve structured sparsity with different granularity and dimensionality.  Simply increasing the sparsity granularity can lead to better hardware utilization, but it will compromise the sparsity for maintaining accuracy. In this work, we propose a novel method, TETRIS, to achieve both better hardware utilization and higher sparsity. Just like a tile-matching game, we cluster the irregularly distributed weights with small value into structured blocks by reordering the input/output dimension and structurally prune them. Results show that it can achieve comparable sparsity with the irregular element-wise pruning and demonstrate negligible accuracy loss. Ideal speedup, proportional to the sparsity, is experimentally demonstrated. Our proposed method provides a new solution toward algorithm and architecture co-optimization for accuracy-efficiency trade-off.
629. **Efficient Projection onto the Perfect Phylogeny Model** --*Bei Jia &middot;  Surjyendu Ray &middot; Sam Safavi &middot; José Bento*
 > Several algorithms build on the perfect phylogeny model to infer evolutionary trees. This problem is particularly hard, when evolutionary trees are inferred from the fraction of genomes that have mutations in different positions, across different samples. Existing algorithms require exhaustive searches over the space of possible trees. At the center of these algorithms is a projection problem that assigns a fitness cost to phylogenetic trees. In order to perform a wide search over the space of the trees, it is critical to solve this projection problem fast. In this paper, we use Moreau's decomposition for proximal operators, and a tree reduction scheme, to develop a new algorithm to compute this projection. Our algorithm terminates with an exact solution in a finite number of steps, and is extremely fast. In particular, it can search over all evolutionary trees with fewer than 11 nodes, a size relevant for several biological problems (more than 2 billion trees) in about 2 hours.
630. **Parallel Weight Consolidation: A Brain Segmentation Case Study** --*Patrick McClure &middot; Charles Zheng &middot; Francisco Pereira &middot; Jakub Kaczmarzyk &middot; John Rogers-Lee &middot; Satra Ghosh &middot; Dylan Nielson &middot; Peter A Bandettini*
 > Collecting large datasets needed to train deep neural networks can be very difficult, particularly for the many applications for which sharing and pooling data is complicated by practical, ethical, or legal concerns. However, it may be the case that derivative datasets or predictive models developed within individual sites can be shared and combined with fewer restrictions. Training on distributed datasets and combining the resulting networks is often viewed as continual learning, but these methods require networks to be trained sequentially. In this paper, we introduce parallel weight consolidation (PWC), a continual learning method to consolidate the weights of neural networks trained in parallel on independent datasets. We perform a brain segmentation case study using PWC to consolidate several dilated convolutional neural networks trained in parallel on independent structural magnetic resonance imaging (sMRI) datasets from different sites. We found that PWC led to increased performance on held-out test sets from the different sites, as well as on a very large and completely independent multi-site dataset. This demonstrates the feasibility of PWC for combining the knowledge learned by networks trained on different datasets.
631. **Beauty-in-averageness and its contextual modulations: A Bayesian statistical account** --*Chaitanya Ryali &middot; Angela J Yu*
 > Understanding how humans perceive the likability of high-dimensional ``objects'' such as faces is an important problem in both cognitive science and AI/ML. Existing models of human preferences generally assume these preferences to be fixed. However, human assessment of facial attractiveness have been found to be highly context-dependent. Specifically, the classical Beauty-in-Averageness (BiA) effect, whereby a face blended from two original faces is judged to be more attractive than the originals, is significantly diminished or reversed when the original faces are recognizable, or when the morph is mixed-race/mixed gender and the attractiveness judgment is preceded by a race/gender categorization. This effect, dubbed Ugliness-in-Averageness (UiA), has previously been attributed to a disfluency account, which is both qualitative and clumsy in explaining BiA. We hypothesize, instead, that these contextual influences on face processing result from the dependence of attractiveness perception on an element of statistical typicality, and from an attentional mechanism that restricts face representation to a task-relevant subset of features, thus redefining typicality within that subspace. Furthermore, we propose a principled explanation of why statistically atypical objects are less likable: they incur greater encoding or processing cost associated with a greater prediction error, when the brain uses predictive coding to compare the actual stimulus properties with those expected from its associated categorical prototype. We use simulations to show  our model provides a parsimonious, statistically grounded, and quantitative account of contextual dependence of attractiveness. We also validate our model using experimental data from a gender categorization task. Finally, we make model predictions for a proposed experiment that can disambiguate the previous disfluency account and our statistical typicality theory.
632. **Neural Networks Trained to Solve Differential Equations Learn General Representations** --*Martin Magill &middot; Faisal Qureshi &middot; Hendrick de Haan*
 > We introduce a technique based on the singular vector canonical correlation analysis (SVCCA) for measuring the generality of neural network layers across a continuously-parametrized set of tasks. We illustrate this method by studying generality in neural networks trained to solve parametrized boundary value problems based on the Poisson partial differential equation. We find that the first hidden layer is general, and that deeper layers are successively more specific. Next, we validate our method against an existing technique that measures layer generality using transfer learning experiments. We find excellent agreement between the two methods, and note that our method is much faster, particularly for continuously-parametrized problems. Finally, we visualize the general representations of the first layers, and interpret them as generalized coordinates over the input domain.
633. **GumBolt: Extending Gumbel trick to Boltzmann priors** --*Amir H Khoshaman &middot; Mohammad Amin*
 > Boltzmann machines (BMs) are appealing candidates as powerful priors in variational autoencoders (VAEs), as they are capable of capturing nontrivial and multi-modal distributions over discrete variables. However, indifferentiability of the discrete units prohibits using the reparameterization trick, essential for low-noise back propagation. The Gumbel trick resolves this problem in a consistent way by relaxing the variables and distributions, but it is incompatible with BM priors. Here, we propose the GumBolt, a model that extends the Gumbel trick to BM priors in VAEs. GumBolt is significantly simpler than the recently proposed methods with BM prior and outperforms them by a considerable margin. It achieves state-of-the-art performance on permutation invariant MNIST and OMNIGLOT datasets in the scope of models with only discrete latent variables.  Moreover, the performance can be further improved by allowing multi-sampled (importance-weighted) estimation of log-likelihood in training, which was not possible with previous works. 
634. **KONG: Kernels for ordered-neighborhood graphs** --*Moez Draief &middot; Konstantin Kutzkov &middot; Kevin Scaman &middot; Milan Vojnovic*
 > We present novel graph kernels for graphs with node and edge labels that have ordered neighborhoods, i.e. when neighbor nodes follow an order. Graphs with ordered neighborhoods are a natural data representation for evolving graphs where edges are created over time, which induces an order. Combining convolutional subgraph kernels and string kernels, we design new scalable algorithms for generation of explicit graph feature maps using sketching techniques. We obtain precise bounds for the approximation accuracy and computational complexity of the proposed approaches and demonstrate their applicability on real datasets.  In particular, our experiments demonstrate that neighborhood ordering results in more informative features. For the special case of general graphs, i.e. graphs without ordered neighborhoods, the new graph kernels yield efficient and simple algorithms for the comparison of label distributions between graphs. 
635. **The streaming rollout of deep networks - towards fully model-parallel execution** --*Volker Fischer &middot; Jan Koehler &middot; Thomas Pfeil*
 > Deep neural networks, and in particular recurrent networks, are promising candidates to control autonomous agents that interact in real-time with the physical world. However, this requires a seamless integration of temporal features into the network's architecture. For the training of and inference with recurrent neural networks, they are usually rolled out over time, and different rollouts exist. Conventionally, during inference the layers of a network are computed in a sequential manner resulting in sparse temporal integration of information and long response times. In this study, we present a theoretical framework to describe the set of all rollouts and demonstrate their differences in solving specific tasks. We prove that certain rollouts, also with only skip and no recurrent connections, enable earlier and more frequent responses, and show empirically that these early responses have better performance. The streaming rollout maximizes these properties and, in addition, enables a fully parallel execution of the network reducing the runtime on massively parallel devices. Additionally, we provide an open-source toolbox to design, train, evaluate, and online-interact with streaming rollouts.
636. **Probabilistic Neural Programmed Networks for Scene Generation** --*Zhiwei Deng &middot; Jiacheng Chen &middot; YIFANG FU &middot; Greg Mori*
 > In this paper we address the text to scene image generation problem. Generative models that capture the variability in complicated scenes containing rich semantics is a grand goal of image generation. Complicated scene images contain rich visual elements, compositional visual concepts, and complicated relations between objects. Generative models, as an analysis-by-synthesis process, should encompass the following three core components: 1) the generation process that composes the scene; 2) what are the primitive visual elements and how are they composed; 3) the rendering of abstract concepts into their pixel-level realizations.  We propose PNP-Net, a variational auto-encoder framework that addresses these three challenges: it flexibly composes images with a dynamic network structure, learns a set of distribution transformers that can compose distributions based on semantics, and decodes samples from these distributions into realistic images.
637. **Conditional Image Generation for Learning the Structure of Visual Objects** --*Tomas Jakab &middot; Ankush Gupta &middot; Hakan Bilen &middot; Andrea Vedaldi*
 > In this paper, we consider the problem of learning landmarks for object categories without any manual annotations. We cast this as the problem of conditionally generating an image of an object from another one, where the images differ by acquisition time and/or viewpoint. The process is aided by providing the generator with a keypoint-like representation extracted from the target image through a tight bottleneck. This encourages the representation to distil information about the object geometry, which changes from source to target, while the appearance, which is shared by the source and target, is read off from the source alone. Conditioning simplifies the generation task significantly, to the point that adopting a simple perceptual loss instead of more sophisticated approaches such as adversarial training is sufficient to learn landmarks. We show that our method is applicable to a large variety of datasets  --- faces, people, 3D objects, and digits --- without any modifications.  We further demonstrate that we can learn landmarks from synthetic image deformations or videos, all without manual supervision, while outperforming state-of-the-art unsupervised landmark detectors.
638. **Heterogeneous Bitwidth Binarization in Convolutional Neural Networks** --*Joshua Fromm &middot; Shwetak Patel &middot; Matthai Philipose*
 > Recent work has shown that performing inference with fast, very-low-bitwidth (e.g., 1 to 2 bits) representations of values can yield surprisingly accurate results. When coupled with FPGAs or custom hardware, the performance of binary models has been shown to be staggering. We seek to improve upon these designs by leveraging custom hardware’s ability to perform true bitwise operations. To this end, we introduce the "middle-out" algorithm, which allows us to jointly learn the value of each parameter with it’s individual bitwidth, effectively allowing a model to have a fractional bitwidth. We find that heterogeneous representations are fundamentally more expressive than their integer counterparts. We verify this finding by training several models on ImageNet and show that with an average of 1.4 bits we are able to out perform state-of-the-art 2-bit architectures.
639. **Solving Non-smooth Constrained Programs with Lower Complexity than $\mathcal{O}(1/\varepsilon)$: A Primal-Dual Homotopy Smoothing Approach** --*Xiaohan Wei &middot; Hao Yu &middot; Qing Ling &middot; Michael Neely*
 > We propose a new primal-dual homotopy smoothing algorithm for a linearly constrained convex program, where neither the primal nor the dual function has to be smooth or strongly convex. The best known iteration complexity solving such a non-smooth problem is $\mathcal{O}(\varepsilon^{-1})$. In this paper,  we show that by leveraging a local error bound condition on the dual function, the proposed algorithm can achieve a better primal convergence time of  $\mathcal{O}\l(\varepsilon^{-2/(2+\beta)}\log_2(\varepsilon^{-1})\r)$, where $\beta\in(0,1]$ is a local error bound parameter.  As an example application, we show that the distributed geometric median problem, which can be formulated as a constrained convex program, has its dual function non-smooth but satisfying the aforementioned local error bound condition with $\beta=1/2$, therefore enjoying a convergence time of $\mathcal{O}\l(\varepsilon^{-4/5}\log_2(\varepsilon^{-1})\r)$. This result improves upon the $\mathcal{O}(\varepsilon^{-1})$ convergence time bound achieved by existing distributed optimization algorithms. Simulation experiments also demonstrate the performance of our proposed algorithm. 
640. **Early Stopping for Nonparametric Testing ** --*Meimei Liu &middot; Guang Cheng*
 > Early stopping of iterative algorithms is an algorithmic regularization method to avoid over-fitting in estimation and classification. In this paper, we show that early stopping can also be applied to obtain the minimax optimal testing in a general non-parametric setup. Specifically, a Wald-type test statistic is obtained based on an iterated estimate produced by functional gradient descent algorithms in a reproducing kernel Hilbert space. A notable contribution is to establish a ``sharp'' stopping rule: when the number of iterations achieves an optimal order, testing optimality is achievable; otherwise, testing optimality becomes impossible. As a by-product, a similar sharpness result is also derived for minimax optimal estimation under early stopping. All obtained results hold for various kernel classes, including Sobolev smoothness classes and Gaussian kernel classes. 
641. **Deep Generative Markov State Models** --*Hao Wu &middot; Andreas Mardt &middot; Luca Pasquali &middot; Frank Noe*
 > We propose a deep generative Markov State Model (DeepGenMSM) learning framework for inference of metastable dynamical systems and prediction of trajectories. After unsupervised training on time series data, the model contains (i) a probabilistic encoder that maps from high-dimensional configuration space to a small-sized vector indicating the membership to metastable (long-lived) states, (ii) a Markov chain that governs the transitions between metastable states and facilitates analysis of the long-time dynamics, and (iii) a generative part that samples the conditional distribution of configurations in the next time step. The model can be operated in a recursive fashion to generate trajectories to predict the system evolution from a defined starting state and propose new configurations. The DeepGenMSM is demonstrated to provide accurate estimates of the long-time kinetics and generate valid distributions for molecular dynamics (MD) benchmark systems. Remarkably, we show that DeepGenMSMs are able to make long time-steps in molecular configuration space and generate physically realistic structures in regions that were not seen in training data.
642. **RetGK: Graph Kernels based on Return Probabilities of Random Walks** --*Zhen Zhang &middot; Mianzhi Wang &middot; Yijian Xiang &middot; Yan Huang &middot; Arye Nehorai*
 > Graph-structured data arise in wide applications, such as computer vision, bioinformatics, and social networks. Quantifying similarities among graphs is a fundamental problem. In this paper, we develop a framework for computing graph kernels, based on return probabilities of random walks. The advantages of our proposed kernels are that they can effectively exploit various node attributes, while being scalable to large datasets. We conduct extensive graph classification experiments to evaluate our graph kernels. The experimental results show that our graph kernels significantly outperform other state-of-the-art approaches in both accuracy and computational efficiency.
643. **Learning from discriminative feature feedback** --*Sanjoy Dasgupta &middot; Sivan Sabato*
 > We consider the problem of learning a multi-class classifier from labels as well as simple explanations that we call "discriminative features". We show that such explanations can be provided whenever the target concept is a decision tree, or more generally belongs to a particular subclass of DNF formulas. We present an efficient online algorithm for learning from such feedback and we give tight bounds on the number of mistakes made during the learning process. These bounds depend only on the size of the target concept and not on the overall number of available features, which could be infinite. We also demonstrate the learning procedure experimentally.
644. **TopRank: A practical algorithm for online stochastic ranking** --*Tor Lattimore &middot; Branislav Kveton &middot; Shuai Li &middot; Csaba Szepesvari*
 > Online learning to rank is a sequential decision-making problem where in each round the learning agent chooses a list of items and receives feedback in the form of clicks from the user. Many sample-efficient algorithms have been proposed for this problem that assume a specific click model connecting rankings and user behavior. We propose a generalized click model that encompasses many existing models, including the position-based and cascade models. Our generalization motivates a novel online learning algorithm based on topological sort, which we call TopRank. TopRank is (a) more natural than existing algorithms, (b) has stronger regret guarantees than existing algorithms with comparable generality, (c) has a more insightful proof that leaves the door open to many generalizations, (d) outperforms existing algorithms empirically.
645. **Faster Neural Networks Straight from JPEG** --*Lionel Gueguen &middot; Alex Sergeev &middot; Rosanne Liu &middot; Jason Yosinski*
 > The simple, elegant approach of training convolutional neural   networks (CNNs) directly from RGB pixels has enjoyed overwhelming   empirical success. But can more performance be squeezed out of   networks by using different input representations?  In this paper we   propose and explore a simple idea: train CNNs directly on the   blockwise discrete cosine transform (DCT) coefficients computed and   available in the middle of the JPEG codec. Intuitively, when   processing JPEG images using CNNs, it seems unnecessary to   decompress a blockwise frequency representation to an expanded pixel   representation, shuffle it from CPU to GPU, and then process it with   a CNN that will learn something similar to a transform back to   frequency representation in its first layers. Why not skip both   steps and feed the frequency domain into the network directly?  In   this paper we modify \libjpeg to produce DCT coefficients directly,   modify a ResNet-50 network to accommodate the differently sized and   strided input, and evaluate performance on ImageNet. We find   networks that are both faster and more accurate, as well as networks   with about the same accuracy but 1.77x faster than ResNet-50.
646. **Stochastic Nested Variance Reduced Gradient Descent for Nonconvex Optimization** --*Dongruo Zhou &middot; Pan Xu &middot; Quanquan Gu*
 > We study finite-sum nonconvex optimization problems, where the objective function is an average of $n$ nonconvex functions. We propose a new stochastic gradient descent algorithm based on nested variance reduction. Compared with conventional stochastic variance reduced gradient (SVRG) algorithm that uses two reference points to construct a semi-stochastic gradient with diminishing variance in each epoch, our algorithm uses $K+1$ nested reference points to build an semi-stochastic gradient to further reduce its variance in each epoch. For smooth functions, the proposed algorithm converges to an approximate first order stationary point (i.e., $\|\nabla F(\xb)\|_2\leq \epsilon$) within $\tO(n\land \epsilon^{-2}+\epsilon^{-3}\land n^{1/2}\epsilon^{-2})$\footnote{$\tO(\cdot)$ hides the logarithmic factors} number of stochastic gradient evaluations, where $n$ is the number of component functions, and $\epsilon$ is the optimization error. This improves the best known gradient complexity of SVRG $O(n+n^{2/3}\epsilon^{-2})$ and the best gradient complexity of SCSG $O(\epsilon^{-5/3}\land n^{2/3}\epsilon^{-2})$. For gradient dominated functions, our algorithm achieves $\tO(n\land \tau\epsilon^{-1}+\tau\cdot (n^{1/2}\land (\tau\epsilon^{-1})^{1/2})$ gradient complexity, which again beats the existing best gradient complexity $\tO(n\land \tau\epsilon^{-1}+\tau\cdot (n^{1/2}\land (\tau\epsilon^{-1})^{2/3})$ achieved by SCSG. Thorough experimental results on different nonconvex optimization problems back up our theory.
647. **Adversarial Examples that Fool both Computer Vision and Time-Limited Humans** --*Gamaleldin Elsayed &middot; Shreya Shankar &middot; Brian Cheung &middot; Nicolas Papernot &middot; Alexey Kurakin &middot; Ian Goodfellow &middot; Jascha Sohl-Dickstein*
 > Machine learning models are vulnerable to adversarial examples: small changes to images can cause computer vision models to make mistakes such as identifying a school bus as an ostrich. However, it is still an open question whether humans are prone to similar mistakes. Here, we address this question by leveraging recent techniques that transfer adversarial examples from computer vision models with known parameters and architecture to other models with unknown parameters and architecture, and by matching the initial processing of the human visual system. We find that adversarial examples that strongly transfer across computer vision models influence the classifications made by time-limited human observers.
648. **Direct Runge-Kutta Discretization Achieves Acceleration** --*Jingzhao Zhang &middot; Aryan Mokhtari &middot; Suvrit Sra &middot; Ali Jadbabaie*
 >   We study gradient-based optimization methods obtained by directly discretizing a second-order ordinary differential equation (ODE) related to the continuous limit of Nesterov's accelerated gradient method. When the function is smooth enough, we show that acceleration can be achieved by a stable discretization of this ODE using standard Runge-Kutta integrators. Specifically, we prove that under Lipschitz-gradient, convexity and order-$(s+2)$ differentiability assumptions, the sequence of iterates generated by discretizing the proposed second-order ODE converges to the optimal solution at a rate of $\mathcal{O}({N^{-2\frac{s}{s+1}}})$, where $s$ is the order of the Runge-Kutta numerical integrator. Furthermore, we introduce a new local flatness condition on the objective, under which rates even faster than $\mathcal{O}(N^{-2})$ can be achieved with low-order integrators and only gradient information. Notably, this flatness condition is satisfied by several standard loss functions used in machine learning. We provide numerical experiments that verify the theoretical rates predicted by our results.
649. **Faster Online Learning of Optimal Threshold for  Consistent F-measure Optimization** --*Xiaoxuan Zhang &middot; Mingrui Liu &middot; Xun Zhou &middot; Tianbao Yang*
 > In this paper, we consider online F-measure optimization (OFO). Unlike traditional performance metrics (e.g., classification error rate), F-measure is non-decomposable over training examples and is a non-convex function of model parameters, making it much more difficult to be optimized in an online fashion. Most existing results of OFO usually suffer from high memory/computational costs and/or lack  statistical consistency  guarantee for optimizing F-measure at the population level. To advance OFO, we propose an efficient online algorithm based on simultaneously learning a posterior probability of class and learning an optimal threshold by minimizing  a stochastic strongly convex function with unknown strong convexity parameter. A key component of the proposed method is  a novel stochastic algorithm with low memory and computational costs, which can enjoy a  convergence rate of $\widetilde O(1/\sqrt{n})$ for learning the optimal threshold under a mild condition on the convergence of the posterior probability,  where $n$ is the number of processed examples. It is provably  faster than its predecessor based on a heuristic for updating the threshold.   The experiments verify  the efficiency of the proposed algorithm in comparison with state-of-the-art OFO algorithms. 
650. **Learning sparse neural networks via sensitivity-driven regularization** --*Enzo Tartaglione &middot; Skjalg  Lepsøy &middot; Attilio Fiandrotti*
 > The ever-increasing number of parameters in deep neural networks poses challenges for memory-limited applications. Regularize-and-prune methods aim at meeting these challenges by sparsifying the network weights. In this context we quantify the output sensitivity to the parameters (i.e. their relevance to the network output) and introduce a regularization term that gradually lowers the absolute value of parameters with low sensitivity. Thus, a very large fraction of the parameters approach zero and are eventually set to zero by simple thresholding. Our method surpasses recent techniques both in terms of sparsity and error rates. In some cases, the method reaches twice the sparsity obtained by other techniques at equal error rates.
