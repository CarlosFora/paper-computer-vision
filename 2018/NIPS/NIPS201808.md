851. **Genetic-Gated Networks for Deep Reinforcement Learning** --*Simyung Chang &middot; John Yang &middot;   &middot; Nojun Kwak*
 > We introduce the Genetic-Gated Networks (G2Ns), simple neural networks that combine a gate vector composed of binary genetic genes in the hidden layer(s) of networks. Our method can take both advantages of gradient-free optimization and gradient-based optimization methods, of which the former is effective for problems with multiple local minima, while the latter can quickly find local minima. In addition, multiple chromosomes can define different models, making it easy to construct multiple models and can be effectively applied to problems that require multiple models. We show that this G2N can be applied to typical reinforcement learning algorithms to achieve a large improvement in sample efficiency and performance.
852. **Neural Guided Constraint Logic Programming for Program Synthesis** --*Lisa Zhang &middot; Gregory Rosenblatt &middot; Ethan Fetaya &middot; Renjie Liao &middot; William Byrd &middot; Matthew Might &middot; Raquel Urtasun &middot; Richard Zemel*
 > Synthesizing programs using example input/outputs is a classic problem in artificial intelligence. We present a method for solving Programming By Example problems by using a neural model to guide the search of a constraint logic programming system called miniKanren.  Internally, miniKanren represents a PBE problem as recursive constraints imposed by the provided examples. We present a Recurrent Neural Network model and a Gated Graph Neural Network model, both of which use these constraints as input to score candidate programs. We further present a transparent version of miniKanren that can be driven by an external agent, suitable for use by other researchers. We show that our neural-guided approach using constraints can synthesize problems faster in many cases, and has the potential to generalize to larger problems.
853. **Learning to Exploit Stability for 3D Scene Parsing** --*Yilun Du &middot; Jiajun Wu &middot; Zhijian Liu &middot; Hector Basevi &middot; Ales Leonardis &middot; Bill Freeman &middot; Josh Tenenbaum*
 > Human scene understanding uses a variety of visual and non-visual cues to perform inference on object types, poses, and relations. Physics is a rich and universal cue which we exploit to enhance scene understanding. We integrate the physical cue of stability into the learning process using a REINFORCE approach coupled to a physics engine, and apply this to the problem of producing the 3D bounding boxes and poses of objects in a scene. We first show that applying physics supervision to an existing scene understanding model increases performance, produces more stable predictions, and allows training to an equivalent performance level with fewer annotated training examples. We then present a novel architecture for 3D scene parsing named Prim R-CNN. With physics supervision, Prim R-CNN outperforms existing scene understanding approaches on this problem. Finally, we show that applying physics supervision on unlabeled real images improves real domain transfer of models training on synthetic data.
854. **Distilled Wasserstein Learning for Word Embedding and Topic Modeling** --*Hongteng Xu &middot; Wenlin Wang &middot; Wei Liu &middot; Lawrence Carin*
 > We propose a novel Wasserstein method with a distillation mechanism, yielding joint learning of word embeddings and topics.  The proposed method is based on the fact that the Euclidean distance between word embeddings may be employed as the underlying distance in the Wasserstein topic model.  The word distributions of topics, their optimal transport to the word distributions of documents, and the embeddings of words are learned in a unified framework.  When learning the topic model, we leverage a distilled ground-distance matrix to update the topic distributions and smoothly calculate the corresponding optimal transports.  Such a strategy provides the updating of word embeddings with robust guidance, improving algorithm convergence.  As an application, we focus on patient admission records, in which the proposed method embeds the codes of diseases and procedures and learns the topics of admissions, obtaining superior performance on clinically-meaningful disease network construction, mortality prediction as a function of admission codes, and procedure recommendation.
855. **Video Prediction via Selective Sampling** --*Jingwei Xu &middot; Bingbing Ni &middot; Xiaokang Yang*
 > Most adversarial learning based video prediction methods suffer from image blur, since the commonly used adversarial and regression loss pair work rather in a competitive way than collaboration, yielding compromised blur effect.    In the meantime, as often relying on a single-pass architecture, the predictor is inadequate to explicitly capture the forthcoming uncertainty.   Our work involves two key insights:   (1) Video prediction can be approached as a stochastic process: we sample a collection of proposals conforming to possible frame distribution at following time stamp, and one can select the final prediction from it.   (2) De-coupling combined loss functions into dedicatedly designed sub-networks encourages them to work in a collaborative way.   Combining above two insights we propose a two-stage network called VPSS (\textbf{V}ideo \textbf{P}rediction via \textbf{S}elective \textbf{S}ampling). <br />   Specifically a \emph{Sampling} module produces a collection of high quality proposals, facilitated by a multiple choice adversarial learning scheme, yielding diverse frame proposal set.    Subsequently a \emph{Selection} module selects high possibility candidates from proposals and combines them to produce final prediction. <br />   Extensive experiments on diverse challenging datasets    demonstrate the effectiveness of proposed video prediction approach, i.e., yielding more diverse proposals and accurate prediction results.
856. **Foreground Clustering for Joint Segmentation and Localization in Videos and Images** --*Abhishek Sharma*
 > This paper presents a novel framework in which video/image segmentation and localization are cast into a single optimization problem that integrates information from low level appearance cues with that of high level localization cues in a very weakly supervised manner. The proposed framework leverages two representations at different levels, exploits the spatial relationship between bounding boxes and superpixels as linear constraints and  simultaneously discriminates between foreground and background at box and superpixel level. Different from previous approaches that mainly rely on discriminative clustering, we incorporate a foreground model that minimizes the histogram difference of an object across all image frames. Exploiting the geometric relation between the superpixels and bounding boxes enables the transfer of segmentation cues to improve localization output and vice-versa. Inclusion of the foreground model generalizes our discriminative framework to video data where the background tends to be similar and thus, not discriminative. We demonstrate the effectiveness of our unified framework on the YouTube Objects video dataset, Internet Object Discovery dataset and Pascal VOC 2007.
857. **Bayesian Semi-supervised Learning with Graph Gaussian Processes** --*Yin Cheng Ng &middot; Ricardo Silva*
 > We propose a data-efficient Gaussian process-based Bayesian approach to the semi-supervised learning problem on graphs. The proposed model shows extremely competitive performance when compared to the state-of-the-art graph neural networks on semi-supervised learning benchmark experiments, and outperforms the neural networks in active learning experiments where labels are scarce. Furthermore, the model does not require a validation data set for early stopping to control over-fitting. Our model can be viewed as an instance of empirical distribution regression weighted locally by network connectivity. We further motivate the intuitive construction of the model with a Bayesian linear model interpretation where the node features are filtered by an operator related to the graph Laplacian. The method can be easily implemented by adapting off-the-shelf scalable variational inference algorithms for Gaussian processes.
858. **Non-Local Recurrent Network for Image Restoration** --*Ding Liu &middot; Bihan Wen &middot; Yuchen Fan &middot; Chen Change Loy &middot; Thomas Huang*
 > Many classic methods have shown that non-local self-similarity in natural images to be an effective prior for image restoration. However, it remains unclear and challenging to make use of this intrinsic property via deep networks. In this paper, we propose a non-local recurrent network (NLRN) as the first attempt to incorporate non-local operations into a recurrent neural network (RNN) for image restoration. The main contributions of this work are: (1) Unlike existing methods that measure self-similarity in an isolated manner, the proposed non-local module can be flexibly integrated into existing deep networks for end-to-end training to capture deep feature correlation between each location and its neighborhood. (2) We fully employ the RNN structure for its parameter efficiency and allow deep feature correlation to be propagated along adjacent recurrent states. This new design boosts robustness against inaccurate correlation estimation due to severely degraded images. (3) We show that it is essential to maintain a confined neighborhood for computing deep feature correlation given degraded images. This is in contrast to existing practice that deploys the whole image. Extensive experiments on both image denoising and super-resolution tasks are conducted. Thanks to the recurrent non-local operations and correlation propagation, the proposed NLRN achieves superior results to state-of-the-art methods with much fewer parameters.
859. **Relating Leverage Scores and Density using Regularized Christoffel Functions** --*Edouard Pauwels &middot; Francis Bach &middot; Jean-Philippe Vert*
 > Statistical leverage scores emerged as a fundamental tool for matrix sketching and column sampling with applications to low rank approximation, regression, random feature learning and quadrature. Yet, the very nature of this quantity is barely understood. Borrowing ideas from the orthogonal polynomial literature, we introduce the regularized Christoffel function associated to a positive definite kernel. This uncovers a variational formulation for leverage scores for kernel methods and allows to elucidate their relationships with the chosen kernel as well as population density. Our main result quantitatively describes a decreasing relation between leverage score and population density for a broad class of kernels on Euclidean spaces. Numerical simulations support our findings.
860. **Neighbourhood Consensus Networks** --*Ignacio Rocco &middot; Mircea Cimpoi &middot; Relja Arandjelović &middot; Akihiko Torii &middot; Tomas Pajdla &middot; Josef Sivic*
 > We address the problem of finding reliable dense correspondences between a pair of images. This is a challenging task due to strong appearance differences between the corresponding scene elements and ambiguities generated by repetitive patterns. The contributions of this work are threefold. First, inspired by the classic idea of disambiguating feature matches using semi-local constraints,  we develop an end-to-end trainable convolutional neural network architecture that identifies sets of spatially consistent  matches by analyzing neighbourhood consensus patterns in the 4D space of all possible correspondences between a pair of images without the need for a global geometric model. <br /> Second, we demonstrate that the model can be trained effectively from weak supervision in the form of matching and non-matching image pairs without the need for costly manual annotation of point to point correspondences. Third, we show the proposed neighbourhood consensus network can be applied to a range of matching tasks including both category- and instance-level matching, obtaining state-of-the-art results on the PF Pascal dataset and the InLoc indoor visual localization benchmark.
861. **Conditional Adversarial Domain Adaptation** --*Mingsheng Long &middot; ZHANGJIE CAO &middot; Jianmin Wang &middot; Michael Jordan*
 > Adversarial learning has been embedded into deep networks to learn transferable representations for domain adaptation. Existing adversarial domain adaptation methods may struggle to align different domains of multimode distributions that are native in classification problems. In this paper, we present conditional adversarial domain adaptation, a new framework that conditions the adversarial adaptation models on discriminative information conveyed in the classifier predictions. Conditional domain adversarial networks are proposed to enable discriminative adversarial adaptation of multimode domains. Experiments testify that the proposed approaches exceed the state-of-the-art results on three domain adaptation datasets.
862. **DifNet: Semantic Segmentation by Diffusion Networks** --*PENG JIANG &middot; Fanglin Gu &middot; Yunhai Wang &middot; Changhe Tu &middot; Baoquan Chen*
 > Deep Neural Networks (DNNs) have recently shown state of the art performance on semantic segmentation tasks, however they still suffer from the problem of poor boundary localization and spatial fragmented predictions. The difficulties lie in the requirement of making dense predictions from a long path model all at once, since details are hard to keep when data goes through deeper layers. Instead, in this work, we decompose this difficult task into two relative simple sub-tasks: seed detection which is required to predict initial predictions without need of wholeness and preciseness, and similarity estimation which estimates the possibility of any two nodes that belong to the same class without need of knowing which class they are. We use one branch network for one sub-task each, and apply a cascade of random walk operations base on hierarchical semantics to approximate a complex diffusion process which propagates seed information to the whole image according to the estimated similarities. The proposed DifNet consistently produces an improvement over the baseline models with same depth and equivalent number of parameters, and also get promising performance on Pascal VOC 2012 and Pascal Context dataset. Our DifNet is trained end-to-end without complex loss functions.
863. **Accelerated Stochastic Matrix Inversion:  General Theory and  Speeding up BFGS Rules for Faster Second-Order Optimization** --*Robert Gower &middot; Filip Hanzely &middot; Peter Richtarik &middot; Sebastian Stich*
 > We present the first accelerated randomized algorithm for solving linear systems in Euclidean spaces. One essential problem of this type is the matrix inversion problem. In particular, our algorithm can be specialized to invert positive definite matrices in such a way that all iterates (approximate solutions) generated by the algorithm are positive definite matrices themselves. This opens the way for many applications in the field of optimization and machine learning.  As an application of our general theory, we develop the first  accelerated (deterministic and stochastic) quasi-Newton updates. Our updates lead to provably more aggressive approximations of the inverse Hessian, and lead to speed-ups over classical non-accelerated rules in numerical experiments. Experiments with empirical risk minimization show that our rules can accelerate training of machine learning models.
864. **Learning Versatile Filters for Efficient Convolutional Neural Networks** --*Yunhe Wang &middot; Chang Xu &middot; Chunjing XU &middot; Chao Xu &middot; Dacheng Tao*
 > This paper introduces versatile filters to construct efficient convolutional neural network. Considering the demands of efficient deep learning techniques running on cost-effective hardware, a number of methods have been developed to learn compact neural networks. Most of these works aim to slim down filters in different ways, e.g., investigating small, sparse or binarized filters. In contrast, we treat filters from an additive perspective. A series of secondary filters can be derived from a primary filter. These secondary filters all inherit in the primary filter without occupying more storage, but once been unfolded in computation they could significantly enhance the capability of the filter by integrating information extracted from different receptive fields. Besides spatial versatile filters, we additionally investigate versatile filters from the channel perspective. The new techniques are general to upgrade filters in existing CNNs. Experimental results on benchmark datasets and neural networks demonstrate that CNNs constructed with our versatile filters are able to achieve comparable accuracy as that of original filters, but require less memory and FLOPs.
865. **Multivariate Time Series Imputation with Generative Adversarial Networks** --*Yonghong Luo &middot; Xiangrui Cai &middot; Ying ZHANG &middot; Jun Xu &middot; Yuan xiaojie*
 > Multivariate time series are characterized by a variety of missing values and the advanced analysis suffers. Conventional methods for imputing missing values, such as mean and zero imputation, deletion and matrix factorization, are not capable to model the temporal dependencies and the complex distribution of the multivariate time series. In this paper, we regard missing value imputation as a data generation problem. Inspired by the success of Generative Adversarial Networks (GAN) in image generation, we adopt GAN to learn the overall distribution of a multivariate time series dataset and to generate the missing values for each sample. Different from image completion where GAN is trained on complete images, we cannot obtain complete time series due to the nature of data recording process. Therefore, a modified Gate Recurrent Unit (GRU) model is employed in GAN to model the temporal irregularity of the incomplete time series. Experiments on two multivariate time series datasets show that our method outperforms the baselines in terms of accuracy of missing value imputation. Additionally, a simple model on the imputed data achieves state-of-the-art results on the two prediction tasks, which demonstrates the benefit of our imputation method for downstream applications.
866. **Multi-Class Learning: From Theory to Algorithm** --*Jian Li &middot; Yong Liu &middot; Rong Yin &middot; Hua Zhang &middot; Lizhong Ding &middot; Weiping Wang*
 > In this paper, we study the generalization performance of multi-class classification and obtain a shaper data-dependent generalization error bound with fast convergence rate, substantially improving the state-of-art bounds in the existing data-dependent generalization analysis. The theoretical analysis motivates us to devise two effective multi-class kernel learning algorithms with statistical guarantee and fast convergence rate. Experimental results show that our proposed methods can significantly outperform the existing multi-class classification methods.
867. **Parsimonious Quantile Regression of Asymmetrically Heavy-tailed Financial Return Series** --*Xing Yan &middot; Weizhong Zhang &middot; Lin Ma &middot; Wei Liu &middot; Qi Wu*
 > We propose a parsimonious quantile regression framework to learn the dynamic tail behavior of financial asset returns. Our method captures well both the time-varying characteristic and the asymmetrical heavy-tail property of financial time series. It combines the merits of the popular sequential neural network model, i.e. LSTM, with a novel parametric quantile function that we construct to represent conditional returns of asset prices. Our method also captures individually the serial dependence of higher moments, rather than just the volatility. Across a wide range of asset classes, the out-of-sample forecasts of our model outperform the GARCH family. Further, the approach does not suffer from the issue of quantile crossing nor does it expose to the ill-posedness comparing to the parametric probability density function approach.
868. **Bilinear Attention Networks** --*Jin-Hwa Kim &middot; Jaehyun Jun &middot; Byoung-Tak Zhang*
 > Attention networks in multimodal learning provide an efficient way to utilize given visual information selectively.  However, the computational cost to learn attention distributions for every pair of multimodal input channels is prohibitively expensive. To solve this problem, co-attention builds two separate attention distributions for each modality neglecting the interaction between multimodal inputs. In this paper, we propose bilinear attention networks (BAN) that find bilinear attention distributions to utilize given vision-language information seamlessly. BAN considers bilinear interactions among two groups of input channels, while low-rank bilinear pooling extracts the joint representations for each pair of channels. Furthermore, we propose a variant of multimodal residual networks to exploit eight-attention maps of the BAN efficiently. We quantitatively and qualitatively evaluate our model on visual question answering (VQA 2.0) and Flickr30k Entities datasets, showing that BAN significantly outperforms previous methods and achieves new state-of-the-arts on both datasets.
869. **Hybrid Knowledge Routed Modules for Large-scale Object Detection** --*ChenHan Jiang &middot; Hang Xu &middot; Xiaodan Liang &middot; Liang Lin*
 > The dominant object detection approaches treat the recognition of each region separately and overlook crucial semantic correlations between objects in one scene. This paradigm leads to substantial performance drop when facing heavy long-tail problems, where very few samples are available for rare classes and plenty of confusing categories exists. We exploit diverse human commonsense knowledge for reasoning over large-scale object categories and reaching semantic coherency within one image. Particularly, we present Hybrid Knowledge Routed Modules (HKRM) that incorporates the reasoning routed by two kinds of knowledge forms: an explicit knowledge module for structured constraints that are summarized with linguistic knowledge (e.g. shared attributes, relationships) about concepts; and an implicit knowledge module that depicts some implicit constraints (e.g. common spatial layouts). By functioning over a region-to-region graph, both modules can be individualized and adapted to coordinate with visual patterns in each image, guided by specific knowledge forms. HKRM are light-weight, general-purpose and extensible by easily incorporating multiple knowledge to endow any detection networks the ability of global semantic reasoning. Experiments on large-scale object detection benchmarks show HKRM obtains around 34.5% improvement on VisualGenome (1000 categories) and 30.4% on ADE in terms of mAP.
870. **Overcoming Language Priors in Visual Question Answering with Adversarial Regularization** --*Sainandan Ramakrishnan &middot; Aishwarya Agrawal &middot; Stefan Lee*
 > Modern Visual Question Answering (VQA) models have been shown to rely heavily on superficial correlations between question and answer words learned during training -- \eg overwhelmingly reporting the type of room as kitchen or the sport being played as tennis, irrespective of the image. Most alarmingly, this shortcoming is often not well reflected during evaluation because the same strong priors exist in test distributions; however, a VQA system that fails to ground questions in image content would likely perform poorly in real-world settings. 
871. **Hybrid Retrieval-Generation Reinforced Agent for Medical Image Report Generation** --*Yuan Li &middot; Xiaodan Liang &middot; Zhiting Hu &middot; Eric Xing*
 > Generating long and topic-coherent reports to describe medical images poses challenges to bridging visual patterns with informative human linguistic descriptions. We propose a novel Hybrid Retrieval-Generation Reinforced Agent (HRGR-Agent) which reconciles traditional retrieval-based approaches populated with human prior knowledge, and modern learning-based approaches to achieve structured, robust and diverse report generation. HRGR-Agent employs a retrieval policy module to generate a sequence of actions to decide between retrieving appropriate template sentences from an off-the-shelf template database, and automatic generating sentences by a generation module. Thus multiple sentences are sequentially generated via hierarchical decision-making. Our HRGR-Agent is updated via reinforcement learning, guided by sentence-level and word-level rewards. Experiments show that HRGR-Agent achieves the state-of-the-art results on two medical report datasets, generating a well balanced structured and complicated sentences with robust coverage of heterogeneous medical report contents. In addition to automatic evaluations, we demonstrate that our model achieves the highest detection accuracy of medical terminologies, and best human evaluation performance.
872. **Stochastic Composite Mirror Descent: Optimal Bounds with High Probabilities** --*Yunwen Lei &middot; Ke Tang*
 > We study stochastic composite mirror descent, a class of scalable algorithms able to exploit the geometry and composite structure of a problem. We consider both convex and strongly convex objectives, for each of which we establish high-probability convergence rates optimal up to a logarithmic factor. We apply the derived computational error bounds to investigate generalization bounds of stochastic gradient descent (SGD) in a non-parametric setting, which refine the existing bounds in expectation by either removing smoothness assumptions on loss functions or improving the specific learning rates. We show that the involved estimation errors scale logarithmically with respect to the number of iterations provided that the step size sequence is square-summable, which justifies the ability of SGD to overcome overfitting. Our analysis removes boundedness assumptions on subgradients often imposed in the literature. Numerical results are reported to support our findings.
873. **Variational Memory Encoder-Decoder** --*Hung Le &middot; Truyen Tran &middot; Thin Nguyen &middot; Svetha Venkatesh*
 > Introducing variability while maintaining coherence is a core task in learning to generate utterances in conversation. Standard neural encoder-decoder models and their extensions using conditional variational autoencoder often result in either trivial or digressive responses. To overcome this, we explore a novel approach that injects variability into neural encoder-decoder via the use of external memory as a mixture model, namely Variational Memory Encoder-Decoder (VMED). By associating each memory read with a mode in the latent mixture distribution at each timestep, our model can capture the variability observed in sequential data such as natural conversations. We empirically compare the proposed model against other recent approaches on various conversational datasets. The results show that VMED consistently achieves significant improvement over others in both metric-based and qualitative evaluations. 
874. **PacGAN: The power of two samples in generative adversarial networks** --*Zinan Lin &middot; Ashish Khetan &middot; Giulia Fanti &middot; Sewoong Oh*
 > Generative adversarial networks (GANs) are a technique for learning generative models of complex data distributions from samples. Despite remarkable advances in generating  realistic images, a major shortcoming of GANs is the fact that they tend to produce samples with little diversity, even when trained on diverse datasets. This  phenomenon, known as mode collapse, has been the focus of much recent work. We study a principled approach to handling mode collapse, which we call packing. The main idea is to modify the discriminator to make decisions based on multiple samples from the same class, either real or artificially generated. We draw analysis tools from binary hypothesis testing---in particular the seminal result of Blackwell---to prove a fundamental connection between packing and mode collapse. We show that packing  naturally penalizes generators with mode collapse,  thereby favoring generator distributions with less mode collapse during the training process. Numerical experiments on benchmark datasets suggest that packing provides significant improvements. 
875. **A loss framework for calibrated anomaly detection** --*Aditya Menon &middot; Robert C Williamson*
 > Given samples from a probability distribution, anomaly detection is the problem of determining if a given point lies in a low-density region. This paper concerns calibrated anomaly detection, which is the practically relevant extension where we additionally wish to produce a confidence score for a point being anomalous. Building on a classification framework for anomaly detection, we show how minimisation of a suitably modified proper loss produces density estimates only for anomalous instances. We then show how to incorporate quantile control by relating our objective to a generalised version of the pinball loss. Finally, we show how to efficiently optimise the objective with kernelised scorer, by leveraging a recent result from the point process literature. The resulting objective captures a close relative of the one-class SVM as a special case. 
876. **Understanding the Role of Adaptivity in Machine Teaching: The Case of Version Space Learners** --*Yuxin Chen &middot; Adish Singla &middot; Oisin Mac Aodha &middot; Pietro Perona &middot; Yisong Yue*
 > In real-world applications of education, an effective teacher adaptively chooses the next example to teach based on the learner's current state. However, most existing work in algorithmic machine teaching focus on the batch setting, where adaptivity plays no role. In this paper, we study the case of teaching consistent, version space learners in an interactive setting. At any time step, the teacher provides an example, the learner performs an update, and the teacher observes the learner's new state. We highlight that adaptivity does not speed up the teaching process when considering existing models of version space learners, such as "worst-case" (the learner picks the next hypothesis randomly from the version space) and "preference-based" (the learner picks hypothesis according to some global preference). Inspired by human teaching, we propose a new model where the learner picks hypotheses according to some local preference defined by the current hypothesis. We show that our model exhibits several desirable properties, e.g., adaptivity plays a key role, and the learner’s transitions over hypotheses are smooth/interpretable. We develop efficient teaching algorithms, and demonstrate our results via simulation and user studies.
877. **Designing by Training: Acceleration Neural Network for Fast High-Dimensional Convolution** --*Longquan Dai &middot; Jinhui Tang*
 > The high-dimensional convolution is widely used in various disciplines but has a serious performance problem due to its high computational complexity. Over the decades, people took a handmade approach to design fast algorithms for the Gaussian convolution. Recently, requirements for various non-Gaussian convolutions have emerged and are contentiously getting higher. However, the handmade acceleration approach is no longer feasible for so many different convolutions since it is a time-consuming and painstaking job. Instead, we propose an Acceleration Network (AccNet) which turns the work of designing new fast algorithms to training the AccNet. This is done by: 1, interpreting splatting, blurring, slicing operations as convolutions; 2, turning these convolutions to $g$CP layers to build AccNet. After training,  the activation function $g$ together with AccNet weights automatically define the new splatting, blurring and slicing operations. Experiments demonstrate AccNet is able to design acceleration algorithms for a ton of convolutions including Gaussian/non-Gaussian convolutions and produce state-of-the-art results.
878. **Where Do You Think You're Going?: Inferring Beliefs about Dynamics from Behavior** --*Sid Reddy &middot; Sergey Levine &middot; Anca Dragan*
 > Inferring intent from observed behavior has been studied extensively within the frameworks of Bayesian inverse planning and inverse reinforcement learning. These methods infer a goal or reward function that best explains the actions of the observed agent, typically a human demonstrator. Another agent can use this inferred intent to predict, imitate, or assist the human user. However, a central assumption in inverse reinforcement learning is that the demonstrator is close to optimal. While models of suboptimal behavior exist, they typically assume that suboptimal actions are the result of some type of random noise or a known cognitive bias, like temporal inconsistency. In this paper, we take an alternative approach, and model suboptimal behavior as the result of internal model misspecification: the reason that user actions might deviate from near-optimal actions is that the user has an incorrect set of beliefs about the rules -- the dynamics -- governing how actions affect the environment. Our insight is that while demonstrated actions may be suboptimal in the real world, they may actually be near-optimal with respect to the user's internal model of the dynamics. By estimating these internal beliefs from observed behavior, we arrive at a new method for inferring intent. We demonstrate in simulation and in a user study with 12 participants that this approach enables us to more accurately model human intent, and can be used in a variety of applications, including offering assistance in a shared autonomy framework and inferring human preferences.
879. **Generalizing Tree Probability Estimation via Bayesian Networks** --*Cheng Zhang &middot; Frederick A Matsen IV*
 > Probability estimation is one of the fundamental tasks in statistics and machine learning. However, standard methods for probability estimation on discrete objects do not handle object structure in a satisfactory manner. In this paper, we derive a general Bayesian network formulation for probability estimation on leaf-labeled trees that enables flexible approximations which can generalize beyond observations. We show that efficient algorithms for learning Bayesian networks can be easily extended to probability estimation on this challenging structured space. Experiments on both synthetic and real data show that our methods greatly outperform the current practice of using the empirical distribution, as well as a previous effort for probability estimation on trees.
880. **Gradient Descent for Spiking Neural Networks** --*Dongsung Huh &middot; Terrence J Sejnowski*
 > Much of studies on neural computation are based on network models of static neurons that produce analog output, despite the fact that information processing in the brain is predominantly carried out by dynamic neurons that produce discrete pulses called spikes. Research in spike-based computation has been impeded by the lack of efficient supervised learning algorithm for spiking networks. Here, we present a gradient descent method for optimizing spiking network models by introducing a differentiable formulation of spiking networks and deriving the exact gradient calculation. For demonstration, we trained recurrent spiking networks on two dynamic tasks: one that requires optimizing fast (≈ millisecond) spike-based interactions for efficient encoding of information, and a delayed-memory XOR task over extended duration (≈ second). The results show that our method indeed optimizes the spiking network dynamics on the time scale of individual spikes as well as the behavioral time scales. In conclusion, our result offers a general purpose supervised learning algorithm for spiking neural networks, thus advancing further investigations on spike-based computation.
881. **On Oracle-Efficient PAC RL with Rich Observations** --*Christoph Dann &middot; Nan Jiang &middot; Akshay Krishnamurthy &middot; Alekh Agarwal &middot; John Langford &middot; Robert Schapire*
 > We study the computational tractability of provably sample-efficient (PAC) reinforcement learning in episodic environments with rich observations. We present new sample-efficient algorithms for environments with deterministic hidden state dynamics and stochastic rich observations. These methods operate in an oracle model of computation—accessing policy and value function classes exclusively through standard optimization primitives—and therefore represent computationally efficient alternatives to prior algorithms that require enumeration. In the more general stochastic transition setting, we prove that the only known sample-efficient algorithm, OLIVE [1], cannot be implemented in our oracle model. We also present several examples that illustrate fundamental challenges of tractable PAC reinforcement learning in such general settings.
882. **SLAYER: Spike Layer Error Reassignment in Time** --*Sumit Shrestha &middot; Garrick Orchard*
 > Configuring deep Spiking Neural Networks (SNNs) is an exciting research avenue for low power spike event based computation. However, the spike generation function is non-differentiable and therefore not directly compatible with the standard error backpropagation algorithm. In this paper, we introduce a new general backpropagation mechanism for learning synaptic weights and axonal delays which overcomes the problem of non-differentiability of the spike function and uses a temporal credit assignment policy for backpropagating error to preceding layers. We describe and release a GPU accelerated software implementation of our method which allows training both fully connected and convolutional neural network (CNN) architectures. Using our software, we compare our method against existing SNN based learning approaches and standard ANN to SNN conversion techniques and show that our method achieves state of the art performance for an SNN on the MNIST, NMNIST, DVS Gesture, and TIDIGITS datasets.
883. **Geometry Based Data Generation** --*Ofir Lindenbaum &middot; Jay Stanley &middot; Guy Wolf &middot; Smita Krishnaswamy*
 > We propose a new type of generative model of high-dimensional data that learns a manifold geometry of the data, rather than density, and can generate points evenly along this manifold. This is in contrast to existing generative models that represent data density, and are strongly affected by noise and other artifacts of data collection. We demonstrate how this approach corrects sampling biases and artifacts, and improves several downstream data analysis tasks such as clustering and classification. Finally, we demonstrate that this approach is especially useful in biology where, despite the advent of single-cell technologies, rare subpopulations and gene-interaction relationships are affected by biased sampling. We show that SUGAR can generate hypothetical populations and reveal intrinsic patterns and mutual-information relationships between genes on a single cell dataset of hematopoiesis. 
884. **Multitask Boosting for Survival Analysis with Competing Risks** --*ALEXIS BELLOT &middot; Mihaela van der Schaar*
 > The co-occurrence of multiple diseases among the general population is an important problem as those patients have more risk of complications and represent a large share of health care expenditure. Learning to predict time-to-event probabilities for these patients is a challenging problem because the risks of events are correlated (there are competing risks) with often only few patients experiencing individual events of interest, and of those only a fraction are actually observed in the data. We introduce in this paper a survival model with the flexibility to leverage a common representation of related events that is designed to correct for the strong imbalance in observed outcomes. The procedure is sequential: outcome-specific survival distributions form the components of nonparametric multivariate estimators which we combine into an ensemble in such a way as to ensure accurate predictions on all outcome types simultaneously. Our algorithm is general and represents the first boosting-like method for time-to-event data with multiple outcomes. We demonstrate the performance of our algorithm on synthetic and real data.
885. **Regularization Learning Networks** --*Ira Shavitt &middot; Eran Segal*
 > Despite their impressive performance, Deep Neural Networks (DNNs) typically underperform Gradient Boosting Trees (GBTs) on many tabular-dataset learning tasks. We propose that applying a different regularization coefficient to each weight might boost the performance of DNNs by allowing them to make more use of the more relevant inputs. However, this will lead to an intractable number of hyperparameters. Here, we introduce Regularization Learning Networks (RLNs), which overcome this challenge by introducing an efficient hyperparameter tuning scheme that minimizes a new Counterfactual Loss. Our results show that RLNs significantly improve DNNs on tabular datasets, and achieve comparable results to GBTs, with the best performance achieved with an ensemble that combines GBTs and RLNs. RLNs produce extremely sparse networks, eliminating up to 99.8% of the network edges and 82% of the input features, thus providing more interpretable models and reveal the importance that the network assigns to different inputs. RLNs could efficiently learn a single network in datasets that comprise both tabular and unstructured data, such as in the setting of medical imaging accompanied by electronic health records.
886. **Joint Active Feature Acquisition and Classification with Variable-Size Set Encoding** --*Hajin Shim &middot; Sung Ju Hwang &middot; Eunho Yang*
 > We consider the problem of active feature acquisition where the goal is to sequentially select the subset of features in order to achieve the maximum prediction performance in the most cost-effective way at test time. In this work, we formulate this active feature acquisition as a jointly learning problem of training both the classifier (environment) and the RL agent that decides either to <code>stop and predict' or</code>collect a new feature' at test time, in a cost-sensitive manner. We also introduce a novel encoding scheme to represent acquired subsets of features by proposing an order-invariant set encoding at the feature level, which also significantly reduces the search space for our agent. We evaluate our model on a carefully designed synthetic dataset for the active feature acquisition as well as several medical datasets. Our framework shows meaningful feature acquisition process for diagnosis that complies with human knowledge, and outperforms all baselines in terms of prediction performance as well as feature acquisition cost. 
887. **Found Graph Data and Planted Vertex Covers** --*Austin Benson &middot; Jon Kleinberg*
 > A typical way in which network data is recorded is to measure all interactions involving a specified set of core nodes, which produces a graph containing this core together with a potentially larger set of fringe nodes that link to the core. Interactions between nodes in the fringe, however, are not present in the resulting graph data. For example, a phone service provider may only record calls in which at least one of the participants is a customer; this can include calls between a customer and a non-customer, but not between pairs of non-customers. Knowledge of which nodes belong to the core is crucial for interpreting the dataset, but this metadata is unavailable in many cases, either because it has been lost due to difficulties in data provenance, or because the network consists of ``found data'' obtained in settings such as counter-surveillance. This leads to an algorithmic problem of recovering the core set. Since the core is a vertex cover, we essentially have a planted vertex cover problem, but with an arbitrary underlying graph. We develop a framework for analyzing this planted vertex cover problem, based on the theory of fixed-parameter tractability, together with algorithms for recovering the core. Our algorithms are fast, simple to implement, and out-perform several baselines based on core-periphery structure on various real-world datasets.
888. **Generative Neural Machine Translation** --*Harshil Shah &middot; David Barber*
 > We introduce Generative Neural Machine Translation (GNMT), a latent variable architecture which is designed to model the semantics of the source and target sentences. We modify an encoder-decoder translation model by adding a latent variable as a language agnostic representation which is encouraged to learn the meaning of the sentence. GNMT achieves competitive BLEU scores on pure translation tasks, and is superior when there are missing words in the source sentence. We augment the model to facilitate multilingual translation and semi-supervised learning without adding parameters. This framework significantly reduces overfitting when there is limited paired data available, and is effective for translating between pairs of languages not seen during training.
889. **Improving Word Embedding by Adversarial Training** --*Chengyue Gong &middot; Di He &middot; Xu Tan &middot; Tao Qin &middot; Liwei Wang &middot; Tieyan Liu*
 > Continuous word representation (aka word embedding) is a basic building block in many neural network-based models used in natural language processing tasks. Although it is widely accepted that words with similar semantics should be close to each other in the embedding space, we find that word embeddings learned in several tasks are biased towards word frequency: the embeddings of high-frequency and low-frequency words lie in different subregions of the embedding space, and the embedding of a rare word and a popular word can be far from each other even if they are semantically similar. This makes learned word embeddings ineffective, especially for rare words, and consequently limits the performance of these neural network models. In order to mitigate the issue, in this paper, we propose a neat, simple yet effective adversarial training method to blur the boundary between the embeddings of high-frequency words and low-frequency words. We conducted comprehensive studies on ten datasets across four natural language processing tasks, including word similarity, language modeling, machine translation and text classification. Results show that we achieve higher performance than the baselines in all tasks.
890. **Adaptive Online Learning in Dynamic Environments** --*Lijun Zhang &middot; Shiyin Lu &middot; Zhi-Hua Zhou*
 > In this paper, we study online convex optimization in dynamic environments, and aim to bound the dynamic regret with respect to any sequence of comparators. Existing work have shown that online gradient descent enjoys an $O(\sqrt{T}(1+P_T))$ dynamic regret, where $T$ is the number of iterations and $P_T$ is the path-length of the comparator sequence.  However, this result is unsatisfactory, as there exists a large gap from the $\Omega(\sqrt{T(1+P_T)})$ lower bound established in our paper. To address this limitation, we develop a novel online method, namely adaptive learning for dynamic environment (Ader), which achieves an $O(\sqrt{T(\log \log T+P_T)})$ dynamic regret, matching the lower bound up to a double logarithmic factor. The basic idea is to maintain a set of experts, each attaining an optimal dynamic regret for a specific path-length, and combines them with an expert-tracking algorithm.  Furthermore, we propose an improved Ader based on the surrogate loss, and in this way the number of gradient evaluations per round is reduced from $O(\log T)$ to $1$. Finally, we extend Ader to the setting that a sequence of dynamical models is available to characterize the comparators.
891. **Revisiting Multi-Task Learning with ROCK: a Deep Residual Auxiliary Block for Visual Detection** --*Taylor Mordan &middot; Nicolas THOME &middot; Gilles Henaff &middot; Matthieu Cord*
 > Multi-Task Learning (MTL) is appealing for deep learning regularization. In this paper, we tackle a specific MTL context denoted as primary MTL, where the ultimate goal is to improve the performance of a given primary task by leveraging several other auxiliary tasks. Our main methodological contribution is to introduce ROCK, a new generic multi-modal fusion block for deep learning tailored to the primary MTL context. ROCK architecture is based on a residual connection, which makes forward prediction explicitly impacted by the intermediate auxiliary representations. The auxiliary predictor's architecture is also specifically designed to our primary MTL context, by incorporating intensive pooling operators for maximizing complementarity of intermediate representations. Extensive experiments on NYUv2 dataset (object detection with scene classification, depth prediction, and surface normal estimation as auxiliary tasks) validate the relevance of the approach and its superiority to flat MTL approaches. Our method outperforms state-of-the-art object detection models on NYUv2 by a large margin, and is also able to handle large-scale heterogeneous inputs (real and synthetic images) and missing annotation modalities.
892. **Gradient Sparsification for Communication-Efficient Distributed Optimization** --*Jianqiao Wangni &middot; Jialei Wang &middot; Ji Liu &middot; Tong Zhang*
 > Modern large-scale machine learning applications require stochastic optimization algorithms to be implemented on distributed computational architectures. A key bottleneck is the communication overhead for exchanging information such as stochastic gradients among different workers. In this paper,  to reduce the communication cost, we propose a convex optimization formulation to minimize the coding length of stochastic gradients. The key idea is to randomly drop out coordinates of the stochastic gradient vectors and amplify the remaining coordinates appropriately to ensure the sparsified gradient to be unbiased. To solve the optimal sparsification efficiently, several simple and fast algorithms are proposed for an approximate solution, with a theoretical guarantee for sparseness.  Experiments on $\ell_2$ regularized logistic regression, support vector machines, and convolutional neural networks validate our sparsification approaches.
893. ** Image-to-image translation for cross-domain disentanglement** --*Abel Gonzalez-Garcia &middot; Joost van de Weijer &middot; Yoshua Bengio*
 > Deep image translation methods have recently shown excellent results, outputting high-quality images covering multiple modes of the data distribution. There has also been increased interest in disentangling the internal representations learned by deep methods to further improve their performance and achieve a finer control. In this paper, we bridge these two objectives and introduce the concept of cross-domain disentanglement. We aim to separate the internal representation into three parts. The shared part contains information for both domains. The exclusive parts, on the other hand, contain only factors of variation that are particular to each domain. We achieve this through bidirectional image translation based on Generative Adversarial Networks and cross-domain autoencoders, a novel network component. The obtained model offers multiple advantages. We can output diverse samples covering multiple modes of the distributions of both domains. We can perform cross-domain retrieval without the need of labeled data. Finally, we can perform domain-specific image transfer and interpolation. We compare our model to the state-of-the-art in multi-modal image translation and achieve better results.
894. **Global Gated Mixture of Second-order Pooling for Improving Deep Convolutional Neural Networks** --*Qilong Wang &middot; Zilin Gao &middot; Jiangtao Xie &middot; Wangmeng Zuo &middot; Peihua Li*
 > In most of existing deep convolutional neural networks (CNNs) for classification, global average (first-order) pooling (GAP) has become a standard module to summarize activations of the last convolution layer as final representation for prediction. Recent researches show integration of higher-order pooling (HOP) methods clearly improves performance of deep CNNs. However, both GAP and existing HOP methods assume unimodal distributions, which cannot fully capture statistics of convolutional activations, limiting representation ability of deep CNNs, especially for samples with complex contents. To overcome the above limitation, this paper proposes a global Gated Mixture of Second-order Pooling (GM-SOP) method to further improve representation ability of deep CNNs. To this end, we introduce a sparsity-constrained gating mechanism and propose a novel parametric SOP as component of mixture model. Given a bank of SOP candidates, our method can adaptively choose Top-$K (K>1)$ candidates for each input sample through the sparsity-constrained gating module, and performs weighted sum of outputs of $K$ selected candidates as representation of the sample. The proposed GM-SOP can flexibly accommodate a large number of personalized SOP candidates in an efficient way, leading to richer representations. The deep networks with our GM-SOP can be end-to-end trained, having potential to characterize complex, multi-modal distributions. The proposed methods are evaluated on two large scale image benchmarks (i.e., downsampled ImageNet-1K and Places365), and experimental results show our GM-SOP is superior to its counterparts and achieves very competitive performance.
895. **Fairness Behind a Veil of Ignorance: A Welfare Analysis for Automated Decision Making** --*Hoda Heidari &middot; Claudio Ferrari &middot; Krishna Gummadi &middot; Andreas Krause*
 > We draw attention to an important, yet largely overlooked aspect of evaluating fairness for automated decision making systems---namely risk and welfare considerations. Our proposed family of measures corresponds to the long-established formulations of cardinal social welfare in economics. We come to this proposal by taking the perspective of a rational, risk-averse individual who is going to be subject to algorithmic decision making and is faced with the task of choosing between several algorithmic alternatives behind a Rawlsian veil of ignorance. The convex formulation of our measures allows us to integrate them as a constraint into any convex loss minimization pipeline. Our empirical analysis reveals interesting trade-offs between our proposal and (a) prediction accuracy, (b) group discrimination, and (c) Dwork et al.'s notion of individual fairness. Furthermore and perhaps most importantly, our work provides both theoretical and empirical evidence suggesting that a lower-bound on our measures often leads to bounded inequality in algorithmic outcomes; hence presenting the first computationally feasible mechanism for bounding individual-level (un)fairness.
896. **Unsupervised Learning of View-invariant Action Representations** --*Junnan Li &middot; Wong Yongkang &middot; Qi Zhao &middot; Mohan Kankanhalli*
 > The recent success in human action recognition with deep learning methods mostly adopt the supervised learning paradigm, which requires significant amount of manually labeled data to achieve good performance. However, label collection is an expensive and time-consuming process. In this work, we propose an unsupervised learning framework, which exploits unlabeled data to learn video representations. Different from previous works in video representation learning, our unsupervised learning task is to predict 3D motion in multiple target views using video representation from a source view. By learning to extrapolate cross-view motions, the representation can capture view-invariant motion dynamics which is discriminative for the action. In addition, we propose a view-adversarial training method to enhance learning of view-invariant features. We demonstrate the effectiveness of the learned representations for action recognition on multiple datasets.
897. **The Lingering of Gradients: How to Reuse Gradients Over Time** --*Zeyuan Allen-Zhu &middot; Xinshang Wang &middot; David Simchi-Levi*
 > Classically, the time complexity of a first-order method is estimated by its number of gradient computations. In this paper, we study a more refined complexity by taking into account the ``lingering'' of gradients: once a gradient is computed at $x_k$, the additional time to compute gradients at $x_{k+1},x_{k+2},\dots$ may be reduced.  We show how this improves the running time of gradient descent and SVRG. For instance, if the ``additional time'' scales linearly with respect to the traveled distance, then the ``convergence rate'' of gradient descent can be improved from $1/T$ to $\exp(-T^{1/3})$.   On the empirical side, we solve a hypothetical revenue management (a.k.a. resource allocation) LP problem on the Yahoo! Front Page Today Module application with 4.6m users to $10^{-6}$ error (or $10^{-12}$ dual error) using 6 passes of the dataset. 
898. **New Insight into Hybrid Stochastic Gradient Descent: Beyond With-Replacement Sampling and Convexity** --*Pan Zhou &middot; Xiaotong Yuan &middot; Jiashi Feng*
 > As an incremental-gradient algorithm, the hybrid stochastic gradient descent (HSGD)  enjoys  merits of both stochastic and full gradient methods for finite-sum minimization problem. However, the existing rate-of-convergence analysis for HSGD is made under with-replacement sampling (WRS) and is restricted to convex problems. It is not clear whether HSGD still carries these advantages under the common practice of without-replacement sampling (WoRS) for non-convex problems. In this paper, we affirmatively answer this open question by showing that under WoRS and for both convex and non-convex problems, it is still possible for HSGD (with constant step-size) to match full gradient descent in rate of convergence, while maintaining comparable sample-size-independent incremental first-order oracle  complexity to stochastic gradient descent. For a special class of finite-sum problems with linear prediction models, our convergence results can be further improved in some cases. Extensive numerical results confirm our theoretical affirmation and demonstrate the favorable efficiency of WoRS-based HSGD.
899. **FD-GAN: Pose-guided Feature Distilling GAN for Robust Person Re-identification** --*Yixiao Ge &middot; Zhuowan Li &middot; Haiyu Zhao &middot; Shuai Yi &middot; Xiaogang Wang &middot; hongsheng Li*
 > Person re-identification (reID) is an important task that requires to retrieve a person's images from an image dataset, given one image of the person of interest. For learning robust person features, the pose variation of person images is one of the key challenges. Existing works targeting the problem either perform human alignment, or learn human-region-based representations. Extra pose information and computational cost is generally required for inference. To solve this issue, a Feature Distilling Generative Adversarial Network (FD-GAN) is proposed for learning identity-related and pose-unrelated representations. It is a novel framework based on a Siamese structure with multiple novel discriminators on human poses and identities. In addition to the discriminators, a novel same-pose loss is also integrated, which requires appearance of a same person's generated images to be similar. After learning pose-unrelated person features with pose guidance, no auxiliary pose information and additional computational cost is required during testing. Our proposed FD-GAN achieves state-of-the-art performance on three person reID datasets, which demonstrates that the effectiveness and robust feature distilling capability of the proposed FD-GAN.
900. **Alternating optimization of decision trees, with application to learning sparse oblique trees** --*Miguel A. Carreira-Perpinan &middot; Pooya Tavallali*
 > Learning a decision tree from data is a difficult optimization problem. The most widespread algorithm in practice, dating to the 1980s, is based on a greedy growth of the tree structure by recursively splitting nodes, and possibly pruning back the final tree. The parameters (decision function) of an internal node are approximately estimated by minimizing an impurity measure. We give an algorithm that, given an input tree (structure and nodes' parameters), produces a new tree with the same structure but new parameter values that provably lower or leave unchanged the misclassification error. It is based on 1) a separability condition that allows us to optimize in parallel over the nodes at the same depth in the tree, and alternate over depths; and 2) the fact that optimizing a single node becomes a smaller binary classification problem over the node's decision function. This can be applied to both axis-aligned and oblique trees and our experiments show it consistently outperforms various other algorithms while being highly scalable to large datasets and trees. Further, the same algorithm can handle a sparsity penalty, so it can learn "sparse oblique trees", having a structure that is a subset of the original tree and few nonzero parameters. This combines the best of axis-aligned and oblique trees: flexibility to model correlated data, low generalization error, fast inference and interpretable nodes that involve only a few features in their decision.
901. **Toddler-Inspired Visual Object Learning** --*Sven Bambach &middot; David Crandall &middot; Linda Smith &middot; Chen Yu*
 > Real-world learning systems have practical limitations on the size of the training datasets that they can collect and consider. Given a fixed training size budget, how should a system go about choosing a subset of the possible training examples that still allows for learning accurate, generalizable models? To help address this question, we draw inspiration from a highly efficient practical learning system: the human child. Using head-mounted cameras, eye gaze trackers, and a model of foveated vision, we collected first-person (egocentric) imagery that represents a highly accurate approximation of the "training data" that toddlers' visual systems collect in everyday, naturalistic learning contexts. We used state-of-the-art computer vision learning models (convolutional neural networks) to help characterize the structure of this data, and found that it produces significantly better object models than egocentric data experienced by adults in exactly the same environment. By using the CNNs as a modeling tool to investigate the properties of the child data that may enable this rapid learning, we found that the data exhibits a unique combination of quality and diversity, with not only many similar large, high-quality object views but also  a greater number and diversity of rare views. This novel methodology of analyzing the visual "training data" used by children may not only reveal insights to improve machine learning, but also may suggest new experimental tools to better understand infant learning in developmental psychology.
902. **Evolutionary Reinforcement Learning** --*Shauharda Khadka &middot; Kagan Tumer*
 > Deep Reinforcement Learning (DRL) algorithms have been successfully applied to a range of challenging control tasks. However, these methods typically suffer from three core difficulties: temporal credit assignment with sparse rewards, lack of effective exploration, and brittle convergence properties that are extremely sensitive to hyperparameters. Collectively, these challenges severely limit the applicability of these approaches to real world problems. Evolutionary Algorithms (EAs), a class of black box optimization techniques inspired by natural evolution, are well suited to address each of these three challenges. However, EAs typically suffer with high sample complexity and struggle to solve problems that require optimization of a large number of parameters. In this paper, we introduce Evolutionary Reinforcement Learning (ERL), a hybrid algorithm that leverages the population of an EA to provide diversified data to train an RL agent, and reinserts the RL agent into the EA population periodically to inject gradient information into the EA. ERL inherits EA's ability of temporal credit assignment with a fitness metric, effective exploration with a diverse set of policies, and stability of a population-based approach and complements it with off-policy DRL's ability to leverage gradients for higher sample efficiency and faster learning. Experiments in a range of challenging continuous control benchmark tasks demonstrate that ERL significantly outperforms prior DRL and EA methods, achieving state-of-the-art performances. 
903. **Robustness of classifiers under generative models** --*Alhussein Fawzi &middot; Hamza Fawzu &middot; Omar Fawzi*
 > Despite achieving impressive performance, state-of-the-art classifiers remain highly vulnerable to small, imperceptible, adversarial perturbations.  This vulnerability has proven empirically to be very intricate to address. In this paper, we study the phenomenon of adversarial perturbations under the assumption that the data is generated with a smooth generative model. We derive fundamental upper bounds on the robustness to perturbations of any classification function, and prove the existence of adversarial perturbations that transfer well across different classifiers with small risk. Our analysis of the robustness also provides insights onto key properties of generative models, such as their smoothness and dimensionality of latent space. We conclude with numerical experimental results showing that our bounds provide informative baselines to the maximal achievable robustness on several datasets.
904. **Synthesize Policies for Transfer and Adaptation across Environments and Tasks** --*Hexiang Hu &middot; Liyu CHEN &middot; Boqing Gong &middot; Fei Sha*
 > The ability to transfer in reinforcement learning is key towards building an agent of general artificial intelligence. In this paper, we consider the problem of learning to simultaneously transfer across both environments (\scene) and tasks, probably more importantly, by learning from only sparse (\scene, \task) pairs out of all the possible combinations. We propose a novel compositional neural network architecture which depicts a meta rule for composing policies from the environment and task embeddings. Notably, one of the main challenges is that the embeddings have to be learned jointly with the meta rule. We further propose new training methods to disentangle the embeddings, making them both distinctive signatures of the environments and tasks and effective building blocks for composing the policies. Experiments on \gridworld and \thor, of which the agent takes as input an egocentric view, show that our approach gives rise to high success rates on all the (\scene, \task) pairs after learning from only 40\% of them.
905. **How To Make the Gradients Small Stochastically** --*Zeyuan Allen-Zhu*
 > (This is a theory paper.)
906. **Video-to-Video Synthesis** --*Ting-Chun Wang &middot; Ming-Yu Liu &middot; Jun-Yan Zhu &middot; Guilin Liu &middot; Andrew Tao &middot; Jan Kautz &middot; Bryan Catanzaro*
 > We study the problem of video-to-video synthesis, whose goal is to learn a mapping function from an input source video (e.g., a sequence of semantic segmentation masks) to an output photorealistic video that precisely depicts the content of the source video. While its image counterpart, the image-to-image synthesis problem, is a popular topic, the video-to-video synthesis problem is less explored in the literature. Without understanding temporal dynamics, directly applying existing image synthesis approaches to an input video often results in temporally incoherent videos of low visual quality. In this paper, we propose a novel approach for the video-to-video synthesis problem under adversarial learning framework. Through the introduction of new generator and discriminator architectures, coupled with a spatial-temporal adversarial objective, we achieve high-resolution, photorealistic, temporally coherent video results. Experiments on multiple benchmarks show the advantage of our method compared to strong baselines.  In particular, our model is capable of synthesizing 2K resolution videos of street scenes up to 30 seconds long, not possible before our work. Finally, we apply our approach to future video prediction, outperforming several state-of-the-art competing systems. (Note: using Adobe Reader is highly recommended to view the paper.)
907. **Global Geometry of Multichannel Sparse Blind Deconvolution on the Sphere** --*Yanjun Li &middot; Yoram Bresler*
 > Multichannel blind deconvolution is the problem of recovering an unknown signal $f$ and multiple unknown channels $x_i$ from convolutional measurements $y_i=x_i \circledast f$ ($i=1,2,\dots,N$). We consider the case where $x_i$'s are sparse, and convolution with $f$ is invertible. Our nonconvex optimization formulation solves for a filter $h$ on the unit sphere that produces sparse output $y_i\circledast h$. Under some technical assumptions, we show that all local minima of the objective function correspond to the inverse filter of $f$ up to an inherent sign and shift ambiguity, and all saddle points have strictly negative curvatures. This geometric structure allows successful recovery of $f$ and $x_i$ using a simple manifold gradient descent algorithm with random initialization. Our theoretical findings are complemented by numerical experiments, which demonstrate superior performance of the proposed approach over the previous methods. 
908. **Interactive Structure Learning with Structural Query-by-Committee** --*Christopher J Tosh &middot; Sanjoy Dasgupta*
 > In this work, we introduce interactive structure learning, a framework that unifies many different interactive learning tasks. We present a generalization of the query-by-committee active learning algorithm for this setting, and we study its consistency and rate of convergence, both theoretically and empirically, with and without noise.
909. **A Game-Theoretic Approach to Recommendation Systems with Strategic Content Providers** --*Omer Ben Porat &middot; Moshe Tennenholtz*
 > We introduce a game-theoretic approach to the study of recommendation systems with strategic content providers. Such systems should be fair and stable. Showing that traditional approaches fail to satisfy these requirements, we propose the Shapley mediator. We show that the Shapley mediator satisfies the fairness and stability requirements, runs in linear time, and is the only economically efficient mechanism satisfying these properties.
910. **Efficient nonmyopic batch active search** --*Shali Jiang &middot; Roman Garnett &middot; Benjamin Moseley &middot; Gustavo Malkomes &middot; Matthew Abbott*
 > Active search is a learning paradigm for actively identifying as many members of a given class as possible. Important applications include drug discovery, fraud detection, and product recommendation. All existing work focuses on sequential policies, i.e., selecting one point to query at a time. However, in many real applications, it is possible to evaluate multiple points simultaneously. We investigate batch active search, the first such study we know of in the literature. We first derive the Bayesian optimal policy for batch active search, and prove a lower bound on the performance gap between sequential and batch optimal policies. Then we propose novel batch policies inspired by state-of-the-art sequential policies, and develop an aggressive pruning technique that can speed up computation by up-to nearly 50 times. We conduct thorough experiments on three application domains: a citation network, material science, and drug discovery, testing all proposed policies (14 total) for a wide range of batch sizes. Results show that the empirical gap matches our theoretical bound; nonmyopic policies usually beat myopic ones significantly; we also find diversity to be an important consideration for batch policy design.
911. **Neural Nearest Neighbors Networks for Image Restoration** --*Tobias Plötz &middot; Stefan Roth*
 > Non-local methods exploiting the self-similarity of natural signals have been well studied, for example in image analysis and restoration. Existing approaches, however, rely on k-nearest neighbor (KNN) matching in a fixed feature space. The main hurdle in optimizing this feature space w.r.t. application performance is the non-differentiability of the KNN selection rule. To overcome this, we propose a continuous deterministic relaxation of the KNN selection that maintains differentiability w.r.t. pairwise distances, but retains the original KNN as the limit of a temperature parameter approaching zero. To exploit our relaxation, we propose the neural nearest neighbors block (N^3 block), a novel non-local processing layer that leverages the principle of self-similarity and can be used as building block in modern neural network architectures. We show its effectiveness using image denoising and single image super-resolution as example applications, where we outperform strong CNN baselines and recent non-local models that rely on KNN selection in hand-chosen features spaces. Benefits of our continuous relaxation can also be expected in other relevant input domains such as text or set-valued data.
912. **$\ell_1$-regression with Heavy-tailed Distributions** --*Lijun Zhang &middot; Zhi-Hua Zhou*
 > In this paper, we consider the problem of linear regression with heavy-tailed distributions. Different from previous studies that use the squared loss to measure the performance, we choose the absolute loss, which is more robust in the presence of large prediction errors. To address the challenge that both the input and output could be heavy-tailed, we propose a truncated minimization problem, and demonstrate that it enjoys an $O(\sqrt{d/n})$ excess risk, where $d$ is the dimensionality and $n$ is the number of samples. Compared with traditional work on $\ell_1$-regression, the main advantage of our result is that we achieve a high-probability risk bound without exponential moment conditions on the input and output. Furthermore, if the input is bounded, we show that the classical empirical risk minimization is competent for $\ell_1$-regression even when the output is heavy-tailed. 
913. **A Block Coordinate Ascent Algorithm for Mean-Variance Optimization** --*Tengyang Xie &middot; Bo Liu &middot; Yangyang Xu &middot; Mohammad Ghavamzadeh &middot; Yinlam Chow &middot; Daoming Lyu &middot; Daesub Yoon*
 > Risk management in dynamic decision problems is a primary concern in many fields, including financial investment, autonomous driving, and healthcare. The mean-variance function is one of the most widely used objective functions in risk management due to its simplicity and interpretability. Existing algorithms for mean-variance optimization are based on multi-time-scale stochastic approximation, whose learning rate schedules are often hard to tune, and have only asymptotic convergence proof. In this paper, we develop a model-free policy search framework for mean-variance optimization with finite-sample error bound analysis (to local optima). Our starting point is a reformulation of the original mean-variance function with its Fenchel dual, from which we propose a stochastic block coordinate ascent policy search algorithm. Both the asymptotic convergence guarantee of the last iteration's solution and the convergence rate of the randomly picked solution are provided, and their applicability is demonstrated on several benchmark domains.
914. **Quadratic Decomposable Submodular Function Minimization** --*Pan Li &middot; Niao He &middot; Olgica Milenkovic*
 > We introduce a new convex optimization problem, termed quadratic decomposable submodular function minimization. The problem arises in many learning on graphs and hypergraphs settings and is closely related to decomposable submodular function minimization. We approach the problem via a new dual strategy and describe an objective that may be optimized via random coordinate descent (RCD) methods and projections onto cones. We also establish the linear convergence rate of the RCD algorithm and develop efficient projection algorithms with provable performance guarantees. Numerical experiments in transductive learning on hypergraphs confirm the efficiency of the proposed algorithm and demonstrate the significant improvements in prediction accuracy with respect to state-of-the-art methods. 
915. **Frequency-Domain Dynamic Pruning for Convolutional Neural Networks** --*Zhenhua Liu &middot; Ji-Zeng Xu &middot; Xiulian Peng &middot; Ruiqin Xiong*
 > Deep convolutional neural networks have demonstrated their powerfulness in a variety of applications. However, the storage and computational requirements have largely restricted their further extensions on mobile devices. Recently, pruning of unimportant parameters has been used for both network compression and acceleration. Considering that there are spatial redundancy within most filters in a CNN, we propose a frequency-domain dynamic pruning scheme to exploit the spatial correlations. The frequency-domain coefficients are pruned dynamically in each iteration and different frequency bands are pruned discriminatively, given their different importance on accuracy. Experimental results demonstrate that the proposed scheme can outperform previous spatial-domain counterparts by a large margin. Specifically, it can achieve a compression ratio of 8x and an inference speed-up of 8.9x for ResNet-110, while the accuracy is even better than the reference model on CIFAR-10.
916. **Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding** --*Kexin Yi &middot; Jiajun Wu &middot; Chuang Gan &middot; Antonio Torralba &middot; Pushmeet Kohli &middot; Josh Tenenbaum*
 > We marry two powerful ideas: deep representation learning for visual recognition and language understanding, and symbolic program execution for reasoning. Our visual question answering (VQA) system first recovers a structural scene representation from the image and a program trace from the question. It then executes the program on the scene representation to obtain an answer. Incorporating symbolic structure as prior knowledge offers three advantages. First, executing programs on a symbolic space is more robust to long program traces. Our model can solve complex reasoning tasks better, achieving an accuracy of 99.8% on the CLEVR dataset. Second, the model is more data- and memory-efficient: it learns to perform well on a small number of training data; it can also encode an image into a compact representation and answer questions offline, using only 1% of the storage needed by the best competing methods. Third, symbolic program execution offers full transparency to the reasoning process; we are thus able to interpret and diagnose each execution step. Our model recovers the ground truth programs precisely.
917. **Domain-Invariant Projection Learning for Zero-Shot Recognition** --*An Zhao &middot; Mingyu Ding &middot; Jiechao Guan &middot; Zhiwu Lu &middot; Tao Xiang &middot; Ji-Rong Wen*
 > Zero-shot learning (ZSL) aims to recognize unseen object classes without any training samples, which can be regarded as a form of transfer learning from seen classes to unseen ones. This is made possible by learning a projection between a feature space and a semantic space (e.g. attribute space). Key to ZSL is thus to learn a projection function that is robust against the often large domain gap between the seen and unseen classes. In this paper, we propose a novel ZSL model termed domain-invariant projection learning (DIPL). Our model has two novel components: (1) A domain-invariant feature self-reconstruction task is introduced to the seen/unseen class data, resulting in a simple linear formulation that casts ZSL into a min-min optimization problem. Solving the problem is non-trivial, and a novel iterative algorithm is formulated as the solver, with rigorous theoretic algorithm analysis provided. (2) To further align the two domains via the learned projection, shared semantic structure among seen and unseen classes is explored via forming superclasses in the semantic space. Extensive experiments show that our model outperforms the state-of-the-art alternatives by significant margins.
918. **Boosted Sparse and Low-Rank Tensor Regression** --*Lifang He &middot; Kun Chen &middot; Wanwan Xu &middot; Jiayu Zhou &middot; Fei Wang*
 > We propose a sparse and low-rank tensor regression model to relate a univariate outcome to a feature tensor, in which each unit-rank tensor from the CP decomposition of the coefficient tensor is assumed to be sparse. This structure is both parsimonious and highly interpretable, as it implies that the outcome is related to the features through a few distinct pathways, each of which may only involve subsets of feature dimensions. We take a divide-and-conquer strategy to simplify the task into a set of sparse unit-rank tensor regression problems. To make the computation efficient and scalable, for the unit-rank tensor regression, we propose a stagewise estimation procedure to efficiently trace out its entire solution path. We show that as the step size goes to zero, the stagewise solution paths converge exactly to those of the corresponding regularized regression. The superior performance of our approach is demonstrated on various real-world and synthetic examples.
919. **MetaReg: Towards Domain Generalization using Meta-Regularization** --*Yogesh Balaji &middot; Swami Sankaranarayanan &middot; Rama Chellappa*
 > Training models that generalize to unseen domains at test time is a problem of fundamental importance in machine learning. In this work, we propose using regularization to capture this notion of domain generalization. We pose the problem of finding such a regularization function in a Learning to Learn (or) Meta Learning framework. The notion of domain generalization is explicitly captured by learning a regularizer that makes the model trained on one domain to perform well on another domain. Experimental validations on computer vision and natural language datasets indicate that our method can learn regularizers that achieve good cross-domain generalization.
920. **Learning semantic similarity in a continuous space** --*Michel Deudon*
 > We address the problem of learning semantic representation of questions to measure similarity between pairs as a continuous distance metric. Our work naturally extends Word Mover’s Distance (WMD) [1] by representing text documents as normal distributions instead of bags of embedded words. Our learned metric measures the dissimilarity between two questions as the minimum amount of distance the intent (hidden representation) of one question needs to "travel" to match the intent of another question. We first learn to repeat, reformulate questions to infer intents as normal distributions with a deep generative model [2] (variational auto encoder). Semantic similarity between pairs is then learned discriminatively as an optimal transport distance metric (Wasserstein 2) with our novel variational siamese framework. Among known models that can read sentences individually, our proposed framework achieves state-of-the-art results on Quora duplicate questions dataset, outperforming standard siamese networks [3] [4] [5] [6] [7] by 4% to 10% (relative). Our work sheds light on how deep generative models can approximate distributions (semantic representations) to effectively measure semantic similarity with meaningful distance metrics from Information Theory.
921. **Low-shot Learning via Covariance-Preserving Adversarial Augmentation Network** --*Hang Gao &middot; Zheng Shou &middot; Alireza Zareian &middot; Qianru Sun &middot; Shih-Fu Chang*
 > Feature augmentation attacks low-shot learning problem by easing the catastrophic forgetting and over-fitting issues in low data regime. However, previous works either erroneously assume that any intra-class variances of base classes are sharable with any novel classes, or employ naive generation methods to hallucinate finite examples without modeling their latent distributions. In this work, we propose Covariance-Preserving Adversarial Augmentation Network for low-shot learning to overcome existing limits. We regulate generation within base-novel class pairs that are semantically adaptable. For better feature generation quality, a new Generative Adversarial Network is designed to model the latent distribution of each novel class given its base counterparts. Since direct estimation on novel classes can be inductively biased, we explicitly preserve covariance information as the ``variability'' of base examples during generation process. Empirical evidences show that our model can generate realistic yet diverse examples, leading to substantial improvements on the ImageNet benchmark over the state-of-the-art.
922. **Empirical Risk Minimization in Non-interactive Local Differential Privacy Revisited** --*Di Wang &middot; Marco Gaboardi &middot; Jinhui Xu*
 > In this paper, we revisit the Empirical Risk Minimization problem in the non-interactive local model of differential privacy. In the case of constant or low dimensions ($p\ll n$), we first show that if the  loss function is $(\infty, T)$-smooth,  we can avoid a dependence of the  sample complexity, to achieve error $\alpha$, on the exponential of the dimensionality $p$ with base $1/\alpha$  ({\em i.e.,} $\alpha^{-p}$),  which answers a question in \cite{smith2017interaction}.  Our approach is based on polynomial approximation. Then, we propose player-efficient algorithms with $1$-bit communication complexity and $O(1)$ computation cost for each player. The error bound is asymptotically the same as the original one. With some additional assumptions, we also give an efficient algorithm for the server.   In the case of high dimensions ($n\ll p$),   we show that if the loss function is a convex generalized linear function,  the error  can be bounded by using the Gaussian width of the constrained set, instead of $p$, which improves the one in       \cite{smith2017interaction}.   Our techniques can be extended to some related problems, such as $k$-way marginal queries and smooth queries.  
923. **A Deep Bayesian Policy Reuse Approach Against Non-Stationary Agents** --*yan zheng &middot; Zhaopeng Meng &middot; Jianye Hao &middot; Zongzhang Zhang &middot; Tianpei Yang*
 > In multiagent domains, coping with non-stationary agents that change behaviors constantly is a challenging problem, where an agent is usually required to be able to quickly detect the other agent's policy during online interaction, and then adapt its own policy accordingly. This paper studies efficient policy detecting and reusing techniques when playing against non-stationary agents in Markov games. We propose a new deep BPR+ algorithm by extending the recent BPR+ algorithm with a neural network as the value-function approximator. To detect policy accurately, we propose the \textit{rectified belief model} taking advantage of the \textit{opponent model} to infer the other agent's policy from reward signals and its behaviors. Instead of directly storing individual policies as BPR+, we introduce \textit{distilled policy network} that serves as the policy library in BPR+, using policy distillation to achieve efficient online policy learning and reuse. Deep BPR+ inherits all the advantages of BPR+ and empirically shows better performance in terms of detection accuracy, cumulative rewards and speed of convergence compared to existing algorithms in complex Markov games with raw visual inputs.
924. **A flexible model for training action localization with varying levels of supervision** --*Guilhem Chéron &middot; Jean-Baptiste Alayrac &middot; Ivan Laptev &middot; Cordelia Schmid*
 > Spatio-temporal action detection in videos is typically addressed in a fully-supervised setup with manual annotation of training videos required at every frame.  Since such annotation is extremely tedious and prohibits scalability, there is a clear need to minimize the amount of manual supervision. In this work we propose a unifying framework that can handle and combine varying types of less demanding weak supervision. Our model is based on discriminative clustering and integrates different types of supervision as constraints on the optimization. We investigate applications of such a model to training setups with alternative supervisory signals ranging from video-level class labels over temporal points or sparse action bounding boxes to the full per-frame annotation of action bounding boxes. Experiments on the challenging UCF101-24 and DALY datasets demonstrate competitive performance of our method at a fraction of supervision used by previous methods. The flexibility of our model enables joint learning from data with different levels of annotation. Experimental results demonstrate a significant gain by adding a few fully supervised examples to otherwise weakly labeled videos.
925. **Posterior Concentration for Sparse Deep Learning** --*Veronika Rockova &middot; nicholas polson*
 > We introduce Spike-and-Slab Deep Learning (SS-DL), a fully Bayesian  alternative to dropout for improving generalizability of deep ReLU networks. This new type of regularization enables  provable recovery of smooth input-output maps with {\sl unknown} levels of smoothness. Indeed, we  show that  the posterior distribution concentrates at the near minimax rate for alpha-Holder smooth maps, performing as well as if we knew the smoothness level alpha ahead of time. Our result sheds light on architecture design for deep neural networks, namely the choice of depth, width and sparsity level. These network attributes typically depend on  unknown smoothness  in order to be optimal. We obviate this constraint with the fully Bayes construction. As an aside, we show that SS-DL does not overfit in the sense that the posterior concentrates on smaller networks with fewer (up to the  optimal number of) nodes and links. Our results provide new theoretical justifications for deep ReLU networks from a Bayesian point of view.
926. **DropMax: Adaptive Variational Softmax** --*Hae Beom Lee &middot; Juho Lee &middot; Saehoon Kim &middot; Eunho Yang &middot; Sung Ju Hwang*
 > We propose DropMax, a stochastic version of softmax classifier which at each iteration drops non-target classes according to dropout probabilities adaptively decided for each instance. Specifically, we overlay binary masking variables over class output probabilities, which are input-adaptively learned via variational inference. This stochastic regularization has an effect of building an ensemble classifier out of exponentially many classifiers with different decision boundaries. Moreover, the learning of dropout rates for non-target classes on each instance allows the classifier to focus more on classification against the most confusing classes. We validate our model on multiple public datasets for classification, on which it obtains significantly improved accuracy over the regular softmax classifier and other baselines. Further analysis of the learned dropout probabilities shows that our model indeed selects confusing classes more often when it performs classification.
927. **Uncertainty-Aware Attention for Reliable Interpretation and Prediction** --*  &middot; Hae Beom Lee &middot; Saehoon Kim &middot; Juho Lee &middot; Kwang Joon Kim &middot; Eunho Yang &middot; Sung Ju Hwang*
 > Attention mechanism is effective in both focusing the deep learning models on relevant features and interpreting them. However, attentions may be unreliable since the networks that generate them are often trained in a weakly-supervised manner. To overcome this limitation, we introduce the notion of input-dependent uncertainty to the attention mechanism, such that it generates attention for each feature with varying degrees of noise based on the given input, to learn larger variance on instances it is uncertain about. We learn this Uncertainty-aware Attention (UA) mechanism using variational inference, and validate it on various risk prediction tasks from electronic health records on which our model significantly outperforms existing attention models. The analysis of the learned attentions shows that our model generates attentions that comply with clinicians' interpretation, and provide richer interpretation via learned variance. Further evaluation of both the accuracy of the uncertainty calibration and the prediction performance with "I don't know'' decision show that UA yields networks with high reliability as well.
928. **Reinforced Continual Learning** --*Ju Xu &middot; Zhanxing Zhu*
 > Most artificial intelligence models   have  limitation to solve new tasks without forgetting previously acquired knowledge. The recently emerging paradigm of continual learning aims to solve this issue, in which the model learns various tasks in a sequential fashion. In this work, a novel approach for continual learning is proposed,  which  searches for the best neural architecture for each coming task via sophisticatedly designed reinforcement learning strategies.  We name it as Reinforced Continual Learning (RCL). Our method not only has good performance on preventing catastrophic forgetting but also fits new tasks well. The experiments on sequential classification tasks for variants of MNIST and CIFAR-100 datasets demonstrate that the proposed approach outperforms existing continual learning alternatives with deep networks.
929. **On Word Embedding Dimensionality** --*Zi Yin &middot; Yuanyuan Shen*
 > In this paper, we analyze the effect of dimensionality on word embeddings. Motivated by the unitary-invariance observation of word embedding, we propose the Pairwise Inner Product (PIP) loss, a novel metric for word embedding similarity. By connecting the embedding training process to noisy matrix factorization, we reveal a fundamental bias-variance trade-off in dimensionality selection. This bias-variance trade-off sheds light on many empirical observations which were not previously explained, for example the existence of an optimal dimensionality. Moreover, new insights and discoveries, like when and how word embeddings are robust to over-parametrization, are revealed. By optimizing over the bias-variance trade-off of the PIP loss, we can explicitly answer the open question of dimensionality selection for word embedding.
930. **Discrimination-aware Channel Pruning for Deep Neural Networks** --*Zhuangwei Zhuang &middot; Mingkui Tan &middot; Bohan  Zhuang &middot; Jing Liu &middot; Jiezhang Cao &middot;   &middot; Junzhou Huang &middot; Jinhui Zhu*
 > Channel pruning is one of the predominant approaches for deep model compression. Existing pruning methods either (i) train from scratch with sparsity constraints on channels, or (ii) minimize the reconstruction error between the pre-trained feature maps and the compressed ones. Both strategies suffer from limitations: the former kind is computationally expensive and difficult to converge, whilst the latter kind optimizes the reconstruction error but ignores the discriminative power of channels. To overcome these drawbacks, we investigate a simple-yet-effective method, named discrimination-aware channel pruning (DCP), which seeks to select those channels that really contribute to discriminative power. To this end, we introduce additional losses into the network to increase the discriminative power of intermediate layers. We then propose to select the most discriminative channels for each layer, where both an additional loss and the reconstruction error are considered. Last, we propose a greedy algorithm to make channel selection and parameter optimization in an iterative way. Extensive experiments demonstrate the effectiveness of our method. For example, on ILSVRC-12, our pruned ResNet-50 with 30% reduction of channels even outperforms the original model by 0.39% in top-1 accuracy.
931. **Solving Large Sequential Games with the Excessive Gap Technique** --*Christian Kroer &middot; Gabriele Farina &middot; Tuomas Sandholm*
 > There has been tremendous recent progress on equilibrium-finding algorithms for extensive-form zero-sum games, but there has been a puzzling gap between theory and practice. \emph{First-order methods} have significantly better theoretical convergence rates than any \emph{counterfactual-regret minimization (CFR)} variant. Despite this, CFR variants have been favored in practice. Experiments with first-order methods have only been conducted on small- and medium-sized games because those methods are complicated to implement in this setting, and because CFR variants have been enhanced extensively for over a decade they perform well in practice. In this paper we show that a particular first-order method, a state-of-the-art variant of the \emph{excessive gap technique}---instantiated with the \emph{dilated entropy distance function}---can efficiently solve large real-world problems competitively with CFR and its variants. We show this on large endgames encountered by the \emph{Libratus} poker AI, which recently beat top human poker specialist professionals at no-limit Texas hold'em. We show experimental results on our variant of the excessive gap technique as well as a prior version. We introduce a numerically friendly implementation of the smoothed best response computation associated with first-order methods for extensive-form game solving. We present, to our knowledge, the first GPU implementation of a first-order method for extensive-form games. We present comparisons of several excessive gap technique and CFR variants.
932. **Generalizing Graph Matching beyond Quadratic Assignment Model** --*Tianshu Yu &middot; Junchi Yan &middot; Yilin Wang &middot; Wei Liu &middot; baoxin Li*
 > Graph matching has received persistent attention over decades, which can be formulated as a quadratic assignment problem (QAP). We show that a large family of functions, which we define as Separable Functions, can approximate discrete graph matching in the continuous domain asymptotically by varying the approximation controlling parameters. We also study the properties of global optimality and devise convex/concave-preserving extensions to the widely used Lawler's QAP form. Our theoretical findings show the potential for deriving new algorithms and techniques for graph matching. We deliver solvers based on two specific instances of Separable Functions, and the state-of-the-art performance of our method is verified on popular benchmarks.
933. **Large Margin Deep Networks for Classification** --*Dilip Krishnan &middot; Hossein Mobahi &middot; Samy Bengio &middot; Kevin Regan &middot; Gamaleldin Elsayed*
 > We present a formulation of deep learning that aims at  producing a large margin classifier. The notion of \emc{margin}, minimum distance to a decision boundary, has served as the foundation of several theoretically profound and empirically successful results for both classification and regression tasks. However, most large margin algorithms are applicable only to shallow models with a preset feature representation; and conventional margin methods for neural networks only enforce margin at the output layer. Such methods are therefore not well suited for deep networks. In this work, we propose a novel loss function to impose a margin on any chosen set of layers of a deep network (including input and hidden layers). Our formulation allows choosing any $l_p$ norm ($p \geq 1$) on the metric measuring the margin. We demonstrate that the decision boundary obtained by our loss has nice properties compared to standard classification loss functions. Specifically, we show improved empirical results on the MNIST, CIFAR-10 and ImageNet datasets on multiple tasks: generalization from small training sets, corrupted labels, and robustness against adversarial perturbations. The resulting loss is general and complementary to existing data augmentation (such as random/adversarial input transform) and regularization techniques (weight decay, dropout, and batch norm). 
934. **Connectionist Temporal Classification with Maximum Entropy Regularization** --*Hu Liu &middot; Sheng Jin &middot; Changshui Zhang*
 > Connectionist Temporal Classification (CTC) is an objective function for end-to-end sequence learning, which adopts dynamic programming algorithms to directly learn the mapping between sequences. CTC has shown promising results in many sequence learning applications including speech recognition and scene text recognition. However, CTC tends to produce highly peaky and overconfident distributions, which is a symptom of overfitting. To remedy this, we propose a regularization method based on maximum conditional entropy which penalizes peaky distributions and encourages exploration. We also introduce an entropy-based pruning method to dramatically reduce the number of CTC feasible paths by ruling out unreasonable alignments. Experiments on scene text recognition show that our proposed methods consistently improve over the CTC baseline without the need to adjust training settings. The code will be made publicly available upon the acceptance of the paper.
935. **PointCNN** --*Yangyan Li &middot; Rui Bu &middot; Mingchao Sun &middot; Wei Wu &middot; XINHAN DI &middot; Baoquan Chen*
 > We present a simple and general framework for feature learning from point cloud. The key to the success of CNNs is the convolution operator that is capable of leveraging spatially-local correlation in data represented densely in grids (e.g. images). However, point cloud are irregular and unordered, thus a direct convolving of kernels against the features associated with the points will result in deserting the shape information while being variant to the orders. To address these problems, we propose to learn a X-transformation from the input points, which is used for simultaneously weighting the input features associated with the points and permuting them into latent potentially canonical order. Then element-wise product and sum operations of typical convolution operator are applied on the X-transformed features. The proposed method is a generalization of typical CNNs into learning features from point cloud, thus we call it PointCNN. Experiments show that PointCNN achieves on par or better performance than state-of-the-art methods on multiple challenging benchmark datasets and tasks.
936. **Informative Features for Model Comparison** --*Wittawat Jitkrittum &middot; Heishiro Kanagawa &middot; Patsorn Sangkloy &middot; James Hays &middot; Bernhard Schölkopf &middot; Arthur Gretton*
 > Given two candidate models, and a set of target observations, we address the problem of measuring the relative goodness of fit of the two models. We propose two new statistical tests which are nonparametric, computationally efficient (runtime complexity is linear in the sample size), and interpretable. As a unique advantage, our tests can produce a set of examples (informative features) indicating the regions in the data domain where one model fits significantly better than the other. In a real-world problem of comparing GAN models, the test power of our new test matches that of the state-of-the-art test of relative goodness of fit, while being one order of magnitude faster.
937. **Greedy Hash: Towards Fast Optimization for Accurate Hash Coding in CNN** --*Shupeng Su &middot; Chao Zhang &middot; Kai Han &middot; Yonghong Tian*
 > To convert the input into binary code, hashing algorithm has been widely used for approximate nearest neighbor search on large-scale image sets due to its computation and storage efficiency. Deep hashing further improves the retrieval quality by combining the hash coding with deep neural network. However, a major difficulty in deep hashing lies in the discrete constraints imposed on the network output, which generally makes the optimization NP hard. In this work, we adopt the greedy principle to tackle this NP hard problem by iteratively updating the network toward the probable optimal discrete solution in each iteration. A hash coding layer is designed to implement our approach which strictly uses the sign function in forward propagation to maintain the discrete constraints, while in back propagation the gradients are transmitted intactly to the front layer to avoid the vanishing gradients. In addition to the theoretical derivation, we provide a new perspective to visualize and understand the effectiveness and efficiency of our algorithm. Experiments on benchmark datasets show that our scheme outperforms state-of-the-art hashing methods in both supervised and unsupervised tasks.
938. **Long short-term memory and Learning-to-learn in networks of spiking neurons** --*Guillaume Bellec &middot; Darjan Salaj &middot; Anand Subramoney &middot; Robert Legenstein &middot; Wolfgang Maass*
 > The brain carries out demanding computations and learning processes with recurrent networks of spiking neurons (RSNNs). But computing and learning capabilities of currently available RSNN models have remained poor, especially in comparison with the performance of recurrent networks of artificial neurons, such as Long Short-Term Memory (LSTM) networks. In this article, we investigate whether deep learning can improve RSNN performance. We applied backpropagation through time (BPTT), augmented by biologically inspired heuristics for synaptic rewiring, to RSNNs whose inherent time constants were enriched through simple models for adapting spiking neurons. We found that the resulting RSNNs approximate, for the first time, the computational power of LSTM networks on two common benchmark tasks. Furthermore, our results show that recent successes with applications of Learning-to-Learn (L2L) to LSTM networks can be ported to RSNNs. This opens the door to the investigation of L2L in data-based models for neural networks of the brain, whose activity can -- unlike that of LSTM networks -- be compared directly with recordings from neurons in the brain. In particular, L2L shows that RSNNs can learn large families of non-linear transformations from very few examples, using previously unknown network learning mechanisms. Furthermore, meta-reinforcement learning (meta-RL) shows that LSNNs can learn and execute complex exploration and exploitation strategies.
939. **Distilling Knowledge with Adversarial Networks** --*Xiaojie Wang &middot; Yu Sun &middot; Jianzhong Qi &middot; Rui Zhang*
 > Knowledge distillation (KD) aims to train a lightweight classifier suitable to provide accurate inference with constrained resources in multi-label learning. Instead of directly consuming feature-label pairs, the classifier is trained by a teacher, i.e., a high-capacity model whose training may be resource-hungry. The accuracy of the classifier trained this way is usually suboptimal because it is difficult to learn the true data distribution from the teacher. An alternative method is to adversarially train the classifier against a discriminator in a two-player game akin to generative adversarial networks (GAN), which can ensure the classifier to learn the true data distribution at the equilibrium of this game. However, it may take excessively long time for such a two-player game to reach equilibrium due to high-variance gradient updates. To address these limitations, we propose a three-player game named KDGAN consisting of a classifier, a teacher, and a discriminator. The classifier and the teacher learn from each other via distillation losses and are adversarially trained against the discriminator via adversarial losses. By simultaneously optimizing the distillation and adversarial losses, the classifier will learn the true data distribution at the equilibrium. We approximate the discrete distribution learned by the classifier (or the teacher) with a concrete distribution. From the concrete distribution, we generate continuous samples to obtain low-variance gradient updates, which speed up the training. Extensive experiments using real datasets confirm the superiority of KDGAN in both accuracy and training speed.
940. **Visual Memory for Robust Path Following** --*Ashish Kumar &middot; Saurabh Gupta &middot; David Fouhey &middot; Sergey Levine &middot; Jitendra Malik*
 > Humans routinely retrace a path in a novel environment both forwards and backwards despite uncertainty in their motion. In this paper, we present an approach for doing so. Given a demonstration of a path, a first network generates an abstraction of the path. Equipped with this abstraction, a second network then observes the world and decides how to act in order to retrace the path under noisy actuation and a changing environment. The two networks are optimized end-to-end at training time. We evaluate the method in two realistic simulators, performing path following both forwards and backwards. Our experiments show that our approach outperforms both a classical approach to solving this task as well as a number of other baselines.
941. **FishNet: the Beauty of Feature Preservation and Refinement** --*Shuyang Sun &middot; Jiangmiao Pang &middot; Jianping Shi &middot; Shuai Yi &middot; Wanli Ouyang*
 > The basic principles in designing convolutional neural network (CNN) structures for predicting objects on different levels, e.g., image-level, region-level, and pixel-level, are diverging. Generally, network structures designed specifically for image classification are directly used as default backbone structure for other tasks including detection and segmentation, but there is seldom network structure designed under the consideration of unifying the advantages of networks designed for pixel-level or region-level predicting tasks, which may require features with high-resolution to be preserved and more semantically descriptive. Towards this goal, we design a fish-like network, called FishNet. In FishNet, the information of all resolutions is preserved and refined for the final task. Besides, we observe that existing works still cannot \emph{directly} propagate the gradient information from deep layer to shallow layers. Our design can better handle this problem. Extensive experiments have been conducted to demonstrate that the remarkable performance of the FishNet. In particular, on ImageNet-1k, the  accuracy of FishNet is able to surpass the performance of DenseNet and ResNet with fewer parameters. When used as the backbone network for state-of-the-art approaches on the challenging dataset MS COCO, the FishNet improves the absolute AP by 2.8% for object detection and 2.3% for instance segmentation without bells or whistles. The code of our network for both training and testing will be available online upon acceptance.
942. **Deep Neural Nets with Interpolating Function as Output Activation** --*Bao Wang &middot; Xiyang Luo &middot; Wei Zhu &middot; Zhen Li &middot; Zuoqiang  Shi &middot; Stanley Osher*
 > We replace the output layer of deep neural nets, typically the softmax function, by a novel interpolating function. And we propose end-to-end training and testing algorithms for this new architecture. Compared to classical neural nets with softmax function as output activation, the surrogate with interpolating function as output activation combines advantages of both deep and manifold learning. The new framework demonstrates the following major advantages: First, it is better applicable to the case with insufficient training data. Second, it significantly improves the generalization accuracy on a wide variety of networks. The algorithm is implemented in PyTorch, and code will  be made publicly available.
943. **Sparse Covariance Modeling in High Dimensions with Gaussian Processes** --*Rui Li &middot; Kishan KC &middot; Feng Cui &middot; Justin Domke &middot; Anne Haake*
 > This paper studies statistical relationships among elements of high-dimensional observations varying across non-random covariates. We propose to model the observation elements' changing covariances as sparse multivariate stochastic processes. In particular, our novel covariance modeling method reduces dimensionality by relating the observation vectors to a lower dimensional subspace. To characterize the changing correlations, we jointly model the latent factors and the factor loadings as collections of basis functions that vary with the covariates as Gaussian processes. Automatic relevance determination (ARD) encodes basis sparsity through their coefficients to account for the inherent redundancy. Experiments conducted across domains show superior performances to the state-of-the-art methods.
944. **Do Less, Get More: Streaming Submodular Maximization with Subsampling** --*Moran Feldman &middot; Amin Karbasi &middot; Ehsan Kazemi*
 > In this paper, we develop the first one-pass streaming algorithm for submodular maximization  that does not evaluate the entire stream even once. By carefully subsampling each element of data stream, our algorithm enjoys the tightest approximation guarantees in various settings while having the smallest memory footprint and requiring the lowest number of function evaluations. More specifically, for a monotone submodular function and a $p$-matchoid constraint, our randomized algorithm achieves a $4p$ approximation ratio (in expectation) with $O(k)$ memory and $O(km/p)$ queries per element ($k$ is the size of the largest feasible solution and $m$ is the number of matroids used to define the constraint). For the non-monotone case, our approximation ratio increases only slightly to $4p+2-o(1)$.  To the best or our knowledge, our algorithm is the first that combines the benefits of streaming and subsampling in a novel way in order to truly scale submodular maximization to massive machine learning problems. To showcase its practicality, we empirically evaluated the performance of our algorithm on a video summarization application and observed that it outperforms the state-of-the-art algorithm by up to fifty fold, while maintaining practically the same utility. We also evaluated the scalability of our algorithm on a large dataset of Uber pick up locations. 
945. **Improved few-shot learning with task conditioning and metric scaling** --*Boris Oreshkin &middot; Alexandre Lacoste &middot;  *
 > Few-shot learning has become essential for producing models that generalize from few examples. In this work, we identify that metric scaling and metric task conditioning are important to improve the performance of few-shot algorithms. Our analysis reveals that a simple metric scaling completely changes the nature of learner parameter updates and closes the accuracy gap for the cosine similarity, providing improvements up to 14\% in accuracy on the mini-Imagenet 5-way 5-shot classification task. We further propose a simple and effective way of conditioning a learner on the task sample set, resulting in learning a task-dependent metric space. Moreover, we propose and empirically test a practical end-to-end optimization procedure based on auxiliary task co-training to learn a task-dependent metric space. The resulting few-shot learning model based on the task-dependent scaled metric achieves state of the art on mini-Imagenet. We confirm these results on another few-shot dataset that we introduce in this paper based on CIFAR100.
946. **Learning Disentangled Joint Continuous and Discrete Representations** --*Emilien Dupont*
 > We present a framework for learning disentangled and interpretable jointly continuous and discrete representations in an unsupervised manner. By augmenting the continuous latent distribution of variational autoencoders with a relaxed discrete distribution and controlling the amount of information encoded in each latent unit, we show how continuous and categorical factors of variation can be discovered automatically from data. Experiments show that the framework disentangles continuous and discrete generative factors on various datasets and outperforms current disentangling methods when a discrete generative factor is prominent.
947. **Are GANs Created Equal? A Large-Scale Study** --*Mario Lucic &middot; Karol Kurach &middot; Marcin Michalski &middot; Sylvain Gelly &middot; Olivier Bousquet*
 > Generative adversarial networks (GAN) are a powerful subclass of generative models. Despite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform better than others.  We conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evaluation measures. We find that most models can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes.  To overcome some limitations of the current metrics, we also propose several data sets on which precision and recall can be computed.  Our experimental results suggest that future GAN research should be based on more systematic and objective evaluation procedures. Finally, we did not find evidence that any of the tested algorithms consistently outperforms the non-saturating GAN introduced in \cite{goodfellow2014generative}.
948. **Near-Optimal Non-Convex Optimization via Stochastic Path-Integrated Differential Estimator** --*Cong Fang &middot; Chris Junchi Li &middot; Zhouchen Lin &middot; Tong Zhang*
 > In this paper, we propose a new technique named \textit{Stochastic Path-Integrated Differential EstimatoR} (SPIDER), which can be used to track many deterministic quantities of interests with significantly reduced gradient computation. Combining SPIDER with the method of normalized gradient descent, we propose two new algorithms, namely SPIDER-SFO and SPIDER-SSO, that solve non-convex stochastic optimization problems using stochastic gradients only. We prove that both SPIDER-SFO and SPIDER-SSO algorithms achieve a record-breaking $\tilde{\mathcal{O}}(\epsilon^{-3} )$ gradient computation cost to find an $\epsilon$-approximate first-order and second-order stationary point, respectively. In addition, we prove that SPIDER-SFO nearly matches the algorithmic lower bound for finding stationary point under the gradient Lipschitz assumption in the finite-sum setting.
949. **Dialog-based Interactive Image Retrieval** --*Xiaoxiao Guo &middot; Hui Wu &middot; Yu Cheng &middot; Steven Rennie &middot; Gerald Tesauro &middot; Rogerio S Feris*
 > Inspired by the enormous growth of huge online media collections of many types (e.g. images, audio, video, e-books, etc.), and the paucity of intelligent retrieval systems, this paper introduces a novel approach to interactive visual content retrieval. The proposed retrieval framework is guided by free-form natural language feedback from users, allowing for more natural and effective communication. Such a system constitutes a multi-modal dialog protocol where in each dialog turn, a user submits a natural language request to a retrieval agent, which then attempts to retrieve the optimal object. We formulate the retrieval task as a reinforcement learning problem, and reward the dialog system for improving the rank of the target object during each dialog turn. This framework can be applied to a variety of visual media types (images, videos, graphics, etc.), and in this paper, we study in-depth its application on the task of interactive image retrieval. To avoid the cumbersome and costly process of collecting human-machine conversations as the dialog system learns, we train the dialog system with a user simulator, which is itself trained to describe the differences between target and candidate images. The efficacy of our approach is demonstrated in a footwear image retrieval application. Extensive experiments on both simulated and real-world data show that: 1) our proposed learning framework achieves better accuracy than other supervised and reinforcement learning baselines; and 2) user feedback based on natural language rather than pre-specified attributes leads to more effective retrieval results, and a more natural and expressive communication interface. 
950. **Quantifying Learning Guarantees for Convex but Inconsistent Surrogates** --*Kirill Struminsky &middot; Simon Lacoste-Julien &middot; Anton Osokin*
 > We study the consistency properties of machine learning methods based on minimizing convex surrogates. We extend the recent framework of Osokin et al. (2017) for quantitative analysis of the consistency properties to the case of inconsistent surrogates. Our key technical contribution consists in the new lower bound on the calibration function for the quadratic surrogate, which is non-trivial (not always zero) for inconsistent cases. The new bound allows to quantify the level of inconsistency of the setting and shows how learning with inconsistent surrogates can have guarantees on sample complexity and optimization difficulty. We apply our theory in two concrete cases: hierarchical classification with the tree-structured loss and ranking with the mean average precision loss. The results show the approximation-computation trade-offs caused by inconsistent surrogates and their potential benefits.
