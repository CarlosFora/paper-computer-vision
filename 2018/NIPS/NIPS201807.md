751. **Demystifying excessively volatile human learning: A Bayesian persistent prior and a neural approximation** --*Chaitanya Ryali &middot; Gautam Reddy &middot; Angela J Yu*
 > Understanding how humans and animals learn about statistical regularities in stable and volatile environments, and utilize these regularities to make predictions and decisions, is an important problem in neuroscience and psychology. Using a Bayesian modeling framework, specifically the Dynamic Belief Model (DBM), it has previously been shown that humans tend to make the {\it default} assumption that environmental statistics undergo abrupt, unsignaled changes, even when environmental statistics are actually stable. Because exact Bayesian inference in this setting, an example of switching state space models, is computationally intense, a number of approximately Bayesian and heuristic algorithms have been proposed to account for learning/prediction in the brain. Here, we examine a neurally plausible algorithm, a special case of leaky integration dynamics we denote as EXP (for exponential filtering), that is significantly simpler than all previously suggested algorithms except for the delta-learning rule, and which far outperforms the delta rule in approximating Bayesian prediction performance. We derive the theoretical relationship between DBM and EXP, and show that EXP gains computational efficiency by foregoing the representation of inferential uncertainty (as does the delta rule), but that it nevertheless achieves near-Bayesian performance due to its ability to incorporate a "persistent prior" influence unique to DBM and absent from the other algorithms. Furthermore, we show that EXP is comparable to DBM but better than all other models in reproducing human behavior in a visual search task, suggesting that human learning and prediction also incorporates an element of persistent prior. More broadly, our work demonstrates that when observations are information-poor, detecting changes or modulating the learning rate is both {\it difficult} and (thus) {\it unnecessary} for making Bayes-optimal predictions.
752. **Analytic solution and stationary phase approximation for the Bayesian lasso and elastic net** --*Tom Michoel*
 > The lasso and elastic net linear regression models impose a double-exponential prior distribution on the model parameters to achieve   regression shrinkage and variable selection,  allowing the inference of robust models from large data sets.  However, there has been limited success in deriving estimates for the full posterior distribution of regression coefficients in these models, due to a need to evaluate analytically intractable partition function integrals. Here, the Fourier transform is used to express these integrals as complex-valued oscillatory integrals over "regression frequencies". This results in an analytic expansion and stationary phase approximation for the partition functions of the Bayesian lasso and elastic net, where the non-differentiability of the double-exponential prior has so far eluded such an approach. Use of this approximation leads to highly accurate numerical estimates for the expectation values and marginal posterior distributions of the regression coefficients, and allows for Bayesian inference of much higher dimensional models than previously possible.
753. **Paraphrasing Complex Network: Network Compression via Factor Transfer** --*Jangho Kim &middot; Seonguk Park &middot; Nojun Kwak*
 > Many researchers have sought ways of model compression to reduce the size of a deep neural network DNN with minimal performance degradation in order to use DNNs in embedded systems. Among the model compression methods, a method called knowledge transfer is to train a student network with a stronger teacher network. In this paper, we propose a novel knowledge transfer method which uses convolutional operations to paraphrase teacher's knowledge and to translate it for the student. This is done by two convolutional modules, which are called a paraphraser and a translator. The paraphraser is trained in an unsupervised manner to extract the teacher factors which are defined as paraphrased information of the teacher network. The translator located at the student network extracts the student factors and helps to translate the teacher factors by mimicking them. We observed that our student network trained with the proposed factor transfer method outperforms the ones trained with conventional knowledge transfer methods.
754. **Computing Higher Order Derivatives of Matrix and Tensor Expressions** --*Joachim Giesen &middot; Soeren Laue &middot; Matthias Mitterreiter*
 > Optimization is an integral part of most machine learning systems and most numerical optimization schemes rely on the computation of derivatives. Therefore, frameworks for computing derivatives are an active area of machine learning research. Surprisingly, as of yet, no existing framework is capable of computing higher order matrix and tensor derivatives directly.  Here, we close this fundamental gap and present an algorithmic framework for computing matrix and tensor derivatives that extends seamlessly to higher order derivatives. The framework can be used for symbolic as well as for forward and reverse mode automatic differentiation. Experiments show a speedup between one and four orders of magnitude over state-of-the-art frameworks when evaluating higher order derivatives.
755. **Optimal Algorithms for Non-Smooth Distributed Optimization in Networks** --*Kevin Scaman &middot; Francis Bach &middot; Sebastien Bubeck &middot; Laurent Massoulié &middot; Yin Tat Lee*
 > In this work, we consider the distributed optimization of non-smooth convex functions using a network of computing units. We investigate this problem under two regularity assumptions: (1) the Lipschitz continuity of the global objective function, and (2) the Lipschitz continuity of local individual functions. Under the local regularity assumption, we provide the first optimal first-order decentralized algorithm called multi-step primal-dual (MSPD) and its corresponding optimal convergence rate. A notable aspect of this result is that, for non-smooth functions, while the dominant term of the error is in $O(1/\sqrt{t})$, the structure of the communication network only impacts a second-order term in $O(1/t)$, where $t$ is time. In other words, the error due to limits in communication resources decreases at a fast rate even in the case of non-strongly-convex objective functions. Under the global regularity assumption, we provide a simple yet efficient algorithm called distributed randomized smoothing (DRS) based on a local smoothing of the objective function, and show that DRS is within a $d^{1/4}$ multiplicative factor of the optimal convergence rate, where $d$ is the underlying dimension.
756. **Safe Active Learning for Time-Series Modeling with Gaussian Processes** --*Christoph Zimmer &middot; Mona Meister &middot; Duy Nguyen-Tuong*
 > Learning time-series models is useful for many applications, such as simulation and forecasting. In this study, we consider the problem of actively learning time-series models, while taking given safety constraints into account. For time-series modeling, we employ a Gaussian process with nonlinear exogenous input structure. The proposed approach generates data, i.e. input and output trajectories, appropriate for time-series model learning by dynamically exploring the input space. The basic idea behind the proposed approach is to parametrize the input trajectory as consecutive trajectory sections, which are determined stepwise given safety requirements and past observations. We analyze the proposed algorithm and, empirically, evaluate it on a technical application. The results show the effectiveness of our approach in a realistic industrial setting.
757. **Processing of missing data by neural networks** --*Marek Śmieja &middot; Łukasz Struski &middot; Jacek Tabor &middot; Bartosz Zieliński &middot; PRzemysław Spurek*
 > We propose a general, theoretically justified mechanism for processing missing data by neural networks. Our idea is to replace typical neuron's response in the first hidden layer by its expected value. This approach can be applied for various types of networks at minimal cost in their modification. Moreover, in contrast to recent approaches, it does not require complete data for training. Experimental results performed on different types of architectures show that our method gives better results than typical imputation strategies and other methods dedicated for incomplete data.
758. **Learning Hierarchical Semantic Image Manipulation through Structured Representations** --*Seunghoon Hong &middot; Xinchen Yan &middot; Honglak Lee &middot; Thomas Huang*
 > Understanding, reasoning, and manipulating semantic concepts of images have been a fundamental research problem for decades. Previous work mainly focused on direct manipulation of natural image manifold through color strokes, key-points, textures, and holes-to-fill. In this work, we present a novel hierarchical framework for semantic image manipulation. Key to our hierarchical framework is that we employ structured semantic layout as our intermediate representations for manipulation. Initialized with coarse-level bounding boxes, our layout generator first creates pixel-wise semantic layout capturing the object shape, object-object interactions, and object-scene relations. Then our image generator fills in the pixel-level textures guided by the semantic layout. Such framework allows a user to manipulate images at object-level by adding, removing, and moving one bounding box at a time. Experimental evaluations demonstrate the advantages of the hierarchical manipulation framework over existing image generation and context hole-filing models, both qualitatively and quantitatively. Benefits of the hierarchical framework are further demonstrated in applications such as semantic object manipulation, interactive image editing, and data-driven image manipulation.
759. **Provable Variational Inference for Constrained Log-Submodular Models** --*Josip Djolonga &middot; Stefanie Jegelka &middot; Andreas Krause*
 > Submodular maximization problems appear in several areas of machine learning and data science, as many useful modelling concepts such as diversity and coverage satisfy this natural diminishing returns property. Because the data defining these functions, as well as the decisions made with the computed solutions are subject to statistical noise and randomness, it is arguably necessary to go beyond computing a single approximate optimum and quantify its inherent uncertainty. To this end, we define a rich class of probabilistic models associated with constrained submodular maximization problems. These capture log-submodular dependencies of arbitrary order between the variables, but also satisfy hard combinatorial constraints. Namely, the variables are assumed to take on one of — possibly exponential — set of states, which form the bases of a matroid. To perform inference in these models we design novel variational inference algorithms, which carefully leverage the combinatorial and probabilistic properties of these objects. In addition to providing completely tractable and well-understood variational approximations, our approach results in the minimization of a convex upper bound on the log-partition function. The bound can be efficiently evaluated using greedy algorithms and optimized using any first-order method. Moreover, for the case of facility location and weighted coverage functions, we prove the first constant factor guarantee in this setting — an efficiently certifiable e/(e-1) approximation of the log-partition function. Finally, we empirically demonstrate the effectiveness of our approach on several instances.
760. **Minimax Statistical Learning with Wasserstein distances** --*Jaeho Lee &middot; Maxim Raginsky*
 > As opposed to standard empirical risk minimization (ERM), distributionally robust optimization aims to minimize the worst-case risk over a larger ambiguity set containing the original empirical distribution of the training data. In this work, we describe a minimax framework for statistical learning with ambiguity sets given by balls in Wasserstein space. In particular, we prove a generalization bound that involves the covering number properties of the original ERM problem. As an illustrative example, we provide generalization guarantees for transport-based domain adaptation problems where the Wasserstein distance between the source and target domain distributions can be reliably estimated from unlabeled samples.
761. **Natasha 2: Faster Non-Convex Optimization Than SGD** --*Zeyuan Allen-Zhu*
 > (this is a theory paper)  We design a stochastic algorithm to find $\varepsilon$-approximate local minima of any smooth nonconvex function in rate $O(\varepsilon^{-3.25})$, with only oracle access to stochastic gradients. The best result was essentially $O(\varepsilon^{-4})$ by stochastic gradient descent (SGD).
762. **Causal Inference on Discrete Data using Hidden Compact Representation** --*Ruichu Cai &middot; Jie Qiao &middot; Kun Zhang &middot; Zhenjie Zhang &middot; Zhifeng Hao*
 > <pre><code>Inferring causal relations from a set of observations is one of the fundamental problems across several disciplines. For continuous variables, recently a number of causal discovery methods have demonstrated their effectiveness in distinguishing the cause from effect by exploring certain properties of the conditional distribution, but causal discovery on categorical data still remains to be a challenging problem, because it is generally not easy to find a compact description of the causal mechanism for the true causal direction. In this paper we make an attempt to find a way to solve this problem by assuming a two-stage causal process: the first stage maps the cause to a hidden variable of a lower cardinality, and the second stage generates the effect from the hidden representation. In this way, the causal mechanism admits a simple, compact representation. We show that under this model, the causal direction is identifiable under some weak technique conditions on the true causal mechanism. We also provide an effective solution to recover the above hidden compact representation model under the likelihood framework. Empirical studies verify the effectiveness of the proposed approach on both synthetic and real-world data. </code></pre> 
763. **Out of the Box: Reasoning with Graph Convolution Nets for Factual Visual Question Answering** --*Medhini Narasimhan &middot; Svetlana Lazebnik &middot; Alexander Schwing*
 > Accurately answering a question about a given image requires combining observations with general knowledge.  While this is effortless for humans, reasoning with general knowledge remains an algorithmic challenge. To advance research in this direction, a novel <code>fact-based' visual question answering (FVQA) task has been introduced recently along with a large set of curated facts which link two entities, i.e., two possible answers, via a relation. Given a question-image pair, deep net techniques have been employed to successively reduce the large set of facts until one of the two entities of the final remaining fact is predicted as the answer. We observe that a successive process which considers one fact at a time to form a local decision is sub-optimal. Instead, we develop an entity graph and a graph convolutional net method to</code>reason' about the correct answer by jointly considering all entities. We show on the challenging FVQA dataset that this leads to an improvement in accuracy of around 10% compared to the state-of-the-art.
764. **Representation Balancing MDPs for Off-policy Policy Evaluation** --*Yao Liu &middot; Omer Gottesman &middot; Aniruddh Raghu &middot; Matthieu Komorowski &middot; Aldo A Faisal &middot; Finale Doshi-Velez &middot; Emma Brunskill*
 > We study the problem of off-policy policy evaluation (OPPE) in RL. In contrast to prior work, we consider how to estimate both the individual policy value and average policy value accurately. We draw inspiration from recent work in causal reasoning, and propose a new finite sample generalization error bound for value estimates from MDP models. Using this upper bound as an objective, we develop a learning algorithm of an MDP model with a balanced representation, and show that our approach can yield substantially lower MSE in a common synthetic domain and on a challenging real-world sepsis management problem.
765. **Representation Learning for Treatment Effect Estimation from Observational Data** --*Liuyi Yao &middot; Sheng Li &middot; Yaliang Li &middot; Mengdi Huai &middot; Jing Gao &middot; Aidong Zhang*
 > Estimating individual treatment effect (ITE) is a challenging problem in causal inference, due to missing counterfactuals and the selection bias. Existing ITE estimation methods mainly focus on balancing the distributions of control and treated groups, but ignore the local similarity information that is helpful. In this paper, we propose a local similarity preserved individual treatment effect (SITE) estimation method based on deep representation learning. SITE preserves local similarity and balances data distributions simultaneously, by focusing on several hard samples in each mini-batch. Experimental results on synthetic and three real-world datasets demonstrate the advantages of the proposed SITE method, compared with the state-of-the-art ITE estimation methods.
766. **Contextual bandits with surrogate losses: Margin bounds and efficient algorithms** --*Dylan Foster &middot; Akshay Krishnamurthy*
 > We use surrogate losses to obtain several new regret bounds and new algorithms for contextual bandit learning. Using the ramp loss, we derive a new margin-based regret bound in terms of standard sequential complexity measures of a benchmark class of real-valued regression functions. Using the hinge loss, we derive an efficient algorithm with a $\sqrt{dT}$-type mistake bound against benchmark policies induced by $d$-dimensional regressors. Under realizability assumptions, our results also yield classical regret bounds. 
767. **Isolating Sources of Disentanglement in Variational Autoencoders** --*Tian Qi Chen &middot;   &middot; Roger Grosse &middot; David Duvenaud*
 > We decompose the evidence lower bound to show the existence of a term measuring the total correlation between latent variables. We use this to motivate the beta-TCVAE (Total Correlation Variational Autoencoder) algorithm, a refinement and plug-in replacement of the beta-VAE for learning disentangled representations, requiring no additional hyperparameters during training. We further propose a principled classifier-free measure of disentanglement called the mutual information gap (MIG). We perform extensive quantitative and qualitative experiments, in both restricted and non-restricted settings, and show a strong relation between total correlation and disentanglement, when the model is trained using our framework.
768. **Online Learning with an Unknown Fairness Metric** --*Stephen Gillen &middot; Christopher Jung &middot; Michael Kearns &middot; Aaron Roth*
 > We consider the problem of online learning in the linear contextual bandits setting, but in which there are also strong individual fairness constraints governed by an unknown similarity metric. These constraints demand that we select similar actions or individuals with approximately equal probability DHPRZ12, which may be at odds with optimizing reward, thus modeling settings where profit and social policy are in tension. We assume we learn about an unknown Mahalanobis similarity metric from only weak feedback that identifies fairness violations, but does not quantify their extent. This is intended to represent the interventions of a regulator who "knows unfairness when he sees it" but nevertheless cannot enunciate a quantitative fairness metric over individuals. Our main result is an algorithm in the adversarial context setting that has a number of fairness violations that depends only logarithmically on T, while obtaining an optimal O(sqrt(T)) regret bound to the best fair policy.
769. **A Unified Feature Disentangler for Multi-Domain Image Translation and Manipulation** --*Alexander H Liu &middot; Yen-Cheng Liu &middot; Yu-Ying Yeh &middot; Yu-Chiang Frank Wang*
 > We present a novel and unified deep learning framework which is capable of learning domain-invariant representation from data across multiple domains. Realized by adversarial training with additional ability to exploit domain-specific information, the proposed network is able to perform continuous cross-domain image translation and manipulation, and produces desirable output images accordingly. In addition, the resulting feature representation exhibits superior performance of unsupervised domain adaptation, which also verifies the effectiveness of the proposed model in learning disentangled features for describing cross-domain data.
770. **Answerer in Questioner's Mind: Information Theoretic Approach to Goal-Oriented Visual Dialog** --*Sang-Woo Lee &middot; Yujung Heo &middot; Byoung-Tak Zhang*
 > Goal-oriented dialog has been given attention due to its numerous applications in artificial intelligence. Goal-oriented dialogue tasks occur when a questioner asks an action-oriented question and an answerer responds with the intent of letting the questioner know a correct action to take.  To ask the adequate question, deep learning and reinforcement learning have been recently applied.  However, these approaches struggle to find a competent recurrent neural questioner, owing to the complexity of learning a series of sentences. Motivated by theory of mind, we propose <code>Answerer in Questioner's Mind'' (AQM), a novel algorithm for goal-oriented dialog.  With AQM, a questioner asks and infers based on an approximated probabilistic model of the answerer. The questioner figures out the answerer’s intention via selecting a plausible question by explicitly calculating the information gain of the candidate intentions and possible answers to each question. We test our framework on two goal-oriented visual dialog tasks:</code>MNIST Counting Dialog'' and ``GuessWhat?!.'' In our experiments, AQM outperforms comparative algorithms by a large margin.
771. **Structural Causal Bandits: Where to Intervene?** --*Sanghack Lee &middot; Elias Bareinboim*
 > We study the problem of identifying the best action in a sequential decision-making setting when the reward distributions of the arms exhibit non-trivial dependencies, which are governed by the underlying causal structure of the domain where the agent is deployed. In this setting, playing an arm corresponds to intervening on a set of variables and setting them to specific values. In this paper, we start by showing that whenever the causal model relating the arms is unknown, the strategy of simultaneously intervening in all variables can, in general, lead to a sub-optimal policy (regardless of the number of iterations performed in the environment). We then derive structural properties implied by the given causal model, which is assumed to be known, albeit without parametrization. We further propose an algorithm that takes as input the causal structure and finds a minimal, sound, and complete set of qualified arms that the agent can play so as to maximize its reward. We empirically demonstrate that this algorithm leads to optimal, order of magnitude faster convergence rates when compared with its causal-insensitive counterparts.
772. **Batch-Instance Normalization for Adaptively Style-Invariant Neural Networks** --*Hyeonseob Nam &middot; Hyo-Eun Kim*
 > Real-world image recognition is often challenged by the variability of visual styles including object textures, lighting conditions, filter effects, etc. Although these variations have been deemed to be implicitly handled by more training data and deeper networks, recent advances in image style transfer suggest that it is also possible to explicitly manipulate the style information. Extending this idea to general visual recognition problems, we present Batch-Instance Normalization (BIN) to explicitly normalize unnecessary styles from images. Considering certain style features play an essential role in discriminative tasks, BIN learns to selectively normalize only disturbing styles while preserving useful styles. The proposed normalization module is easily incorporated into existing network architectures such as Residual Networks, and surprisingly improves the recognition performance in various scenarios. Furthermore, experiments verify that BIN effectively adapts to completely different tasks like object classification and style transfer, by controlling the trade-off between preserving and removing style variations.
773. **Tree-to-tree Neural Networks for Program Translation** --*Xinyun Chen &middot; Chang Liu &middot; Dawn Song*
 > Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to employ deep neural networks toward tackling this problem. We observe that program translation is a modular procedure, in which a sub-tree of the source tree is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source tree to guide the expansion of the decoder. We evaluate the program translation capability of our tree-to-tree model against several state-of-the-art approaches. Compared against other neural translation models, we observe that our approach is consistently better than the baselines with a margin of up to 15 points. Further, our approach can improve the previous state-of-the-art program translation approaches by a margin of 20 points on the translation of real-world projects.
774. **Active Learning for Non-Parametric Regression Using Purely Random Trees** --*Jack Goetz &middot; Ambuj Tewari &middot; Paul Zimmerman*
 > Active learning is the task of using labelled data to select additional points to label, with the goal of fitting the most accurate model with a fixed budget of labelled points. In binary classification active learning is known to produce faster rates than passive learning for a broad range of settings. However in regression restrictive structure and tailored methods have been needed to obtain theoretically superior rates. In this paper we propose an intuitive tree based active learning algorithm for non-parametric regression with provable improvement over random sampling. When implemented with Mondrian Trees our algorithm is tuning parameter free, consistent and minimax optimal for Lipschitz functions.
775. **A Linear Speedup Analysis of Distributed Deep Learning with Sparse and Quantized Communication** --*Peng Jiang &middot; Gagan Agrawal*
 > The large communication overhead has imposed a bottleneck on the performance of distributed Stochastic Gradient Descent (SGD) for training deep neural networks.  Previous works have demonstrated the potential of using gradient sparsification and quantization to reduce the communication cost.  However,  there is still a lack of understanding about how sparse and quantized communication  affects the convergence rate of  the training algorithm. In this paper, we study the convergence rate of distributed SGD for non-convex optimization with two communication reducing strategies: sparse parameter averaging and gradient quantization.  We show that $O(1/\sqrt{MK})$ convergence rate can be achieved if  the sparsification and quantization hyperparameters are configured properly.  We also propose a strategy called periodic quantized averaging (PQASGD) that further reduces the communication cost  while preserving the $O(1/\sqrt{MK})$ convergence rate. Our evaluation validates our theoretical results and shows that our PQASGD can converge as fast as full-precision SGD with only $3\%-5\%$ communication data size.  
776. **Supervised Local Modeling for Interpretability** --*Gregory Plumb &middot; Denali Molitor &middot; Ameet Talwalkar*
 > Model interpretability is an increasingly important component of practical machine learning systems, with example-based, local, and global explanations representing some of the most common forms of explanations. We present a novel explanation system called SLIM that leverages favorable properties of all three of these explanation types. By combining local linear modeling techniques with dual interpretations of random forests (as a supervised neighborhood approach and  as a feature selection method), we present a novel local explanation system with several favorable properties. First, SLIM sidesteps the typical accuracy-interpretability trade-off, as it is  highly accurate while also providing both example-based and local explanations. Second, while SLIM does not provide global explanations, it can detect global patterns and thus diagnose limitations in its local explanations. <br /> Third, SLIM can select an appropriate explanation for a new test point when restricted to an existing set of exemplar explanations.  Finally, in addition to providing faithful self-explanations, SLIM can be deployed as a black-box explanation system.
777. **Leveraged volume sampling for linear regression** --*Michal Derezinski &middot; Manfred Warmuth &middot; Daniel Hsu*
 > Suppose an n x d design matrix in a linear regression problem is given,  but the response for each point is hidden unless explicitly requested.  The goal is to sample only a small number k &lt;&lt; n of the responses,  and then produce a weight vector whose sum of squares loss over <em>all</em> points is at most 1+epsilon times the minimum.  When k is very small (e.g., k=d), jointly sampling diverse subsets of points is crucial. One such method called "volume sampling" has a unique and desirable property that the weight vector it produces is an unbiased estimate of the optimum. It is therefore natural to ask if this method offers the optimal unbiased estimate in terms of the number of responses k needed to achieve a 1+epsilon loss approximation.
778. **Verifiable Reinforcement Learning via Policy Extraction** --*Osbert Bastani &middot; Yewen Pu &middot; Armando Solar-Lezama*
 > While deep reinforcement learning has successfully solved many challenging control tasks, its real-world applicability has been limited by the inability to ensure the safety of learned policies. We propose an approach to verifiable reinforcement learning by training decision tree policies, which can represent complex policies (since they are nonparametric), yet can be efficiently verified using existing techniques (since they are highly structured). The challenge is that decision tree policies are difficult to train. We propose VIPER, an algorithm that combines ideas from model compression and imitation learning to learn decision tree policies guided by a DNN policy (called the oracle) and its Q-function, and show that it substantially outperforms two baselines. We use VIPER to (i) learn a provably robust decision tree policy for a variant of Atari Pong with a symbolic state space, (ii) learn a decision tree policy for a toy game based on Pong that provably never loses, and (iii) learn a provably stable decision tree policy for cart-pole. In each case, the decision tree policy achieves performance equal to that of the original DNN policy.
779. **How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift)** --*Shibani Santurkar &middot; Dimitris Tsipras &middot; Andrew Ilyas &middot; Aleksander Madry*
 > Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called "internal covariate shift". In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training. These findings bring us closer to a true understanding of our DNN training toolkit.
780. **Wasserstein Variational Inference** --*Luca Ambrogioni &middot; Umut Güçlü &middot; Yağmur Güçlütürk &middot; Max Hinne &middot; Marcel A. J. van Gerven &middot; Eric Maris*
 > This paper introduces Wasserstein variational inference, a new form of approximate Bayesian inference based on optimal transport theory. Wasserstein variational inference uses a new family of divergences that includes both f-divergences and the Wasserstein distance as special cases. The gradients of the Wasserstein variational loss are obtained by backpropagating through the Sinkhorn iterations. This technique results in a very stable likelihood-free training method that can be used with implicit distributions and probabilistic programs. Using the Wasserstein variational inference framework, we introduce several new forms of autoencoders and test their robustness and performance against existing variational autoencoding techniques. 
781. **Ridge Regression and Provable Deterministic Ridge Leverage Score Sampling** --*Shannon McCurdy*
 > Ridge leverage scores provide a balance between low-rank approximation and regularization, and are ubiquitous in randomized linear algebra and machine learning.  Deterministic algorithms are also of interest in the moderately big data regime, because deterministic algorithms provide interpretability to the practitioner by having no failure probability and always returning the same results. We provide provable guarantees for deterministic column sampling using ridge leverage scores.   The matrix sketch returned by our algorithm is a column subset of the original matrix, yielding additional interpretability.  Like the randomized counterparts, the deterministic algorithm provides $(1+\epsilon)$  error column subset selection, $(1+\epsilon)$ error projection-cost preservation, and an additive-multiplicative spectral bound.  We also show that under the assumption of power-law decay of ridge leverage scores, this deterministic algorithm is provably as accurate as randomized algorithms. Lastly, ridge regression is frequently used to regularize ill-posed linear least-squares problems.  While ridge regression provides shrinkage for the regression coefficients, many of the coefficients remain small but non-zero. Performing ridge regression with the matrix sketch returned by our algorithm and a particular regularization parameter forces coefficients to zero and has a provable $(1+\epsilon)$ bound on the statistical risk.  As such, it is an interesting alternative to elastic net regularization.
782. **Recurrent World Models Facilitate Policy Evolution** --*David Ha &middot; Jürgen Schmidhuber*
 > A generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments through compressed spatio-temporal representations. The world model's extracted features are fed into compact and simple policies trained by evolution, achieving state of the art results in various environments. We also train our agent entirely inside of an environment generated by its own internal world model, and transfer this policy back into the actual environment. Interactive version of this paper: https://nipsanon.github.io
783. **A theory on the absence of spurious optimality** --*Cedric Josz &middot; Yi Ouyang &middot; Richard Zhang &middot; Javad Lavaei &middot; Somayeh Sojoudi*
 > We study the set of continuous functions that admit no spurious local optima (i.e. local minima that are not global minima) which we term global functions. They satisfy various powerful properties for analyzing nonconvex and nonsmooth optimization problems. For instance, they satisfy a theorem akin to the fundamental uniform limit theorem in the analysis regarding continuous functions. Global functions are also endowed with useful properties regarding the composition of functions and change of variables. Using these new results, we show that a class of non-differentiable nonconvex optimization problems arising in tensor decomposition applications are global functions. This is the first result concerning nonconvex methods for nonsmooth objective functions. Our result provides a theoretical guarantee for the widely-used $\ell_1$ norm to avoid outliers in nonconvex optimization.
784. **Query Complexity of Bayesian Private Learning** --*Kuang Xu*
 > We study the query complexity of Bayesian Private Learning: a learner wishes to locate a random target within an interval by submitting queries, in the presence of an adversary who observes all of her queries but not the responses. How many queries are necessary and sufficient in order for the learner to accurately estimate the target, while simultaneously concealing the target from the adversary?   Our main result is a query complexity lower bound that is tight up to the first order. We show that if the learner wants to estimate the target within an error of $\epsilon$, while ensuring that no adversary estimator can achieve a constant additive error with probability greater than $1/L$, then the query complexity is on the order of $L\log(1/\epsilon)$ as $\epsilon \to 0$. Our result demonstrates that increased privacy, as captured by $L$, comes at the expense of a \emph{multiplicative} increase in query complexity. The proof  builds on Fano's inequality and properties of certain proportional-sampling estimators.
785. **Learning to Navigate in Cities Without a Map** --*Piotr Mirowski &middot; Matt Grimes &middot; Mateusz Malinowski &middot; Karl Moritz Hermann &middot; Keith Anderson &middot; Denis Teplyashin &middot; Karen Simonyan &middot; koray kavukcuoglu &middot; Andrew Zisserman &middot; Raia Hadsell*
 > Navigating through unstructured environments is a basic capability of intelligent creatures, and thus is of fundamental interest in the study and development of artificial intelligence. Long-range navigation is a complex cognitive task that relies on developing an internal representation of space, grounded by recognisable landmarks and robust visual processing, that can simultaneously support continuous self-localisation ("I am here") and a representation of the goal ("I am going there"). Building upon recent research that applies deep reinforcement learning to maze navigation problems, we present an end-to-end deep reinforcement learning approach that can be applied on a city scale. Recognising that successful navigation relies on integration of general policies with locale-specific knowledge, we propose a dual pathway architecture that allows locale-specific features to be encapsulated, while still enabling transfer to multiple cities. A key contribution of this paper is an interactive navigation environment that uses Google Street View for its photographic content and worldwide coverage. Our baselines demonstrate that deep reinforcement learning agents can learn to navigate in multiple cities and to traverse to target destinations that may be kilometres away. A video summarizing our research and showing the trained agent in diverse city environments as well as on the transfer task is available at: https://sites.google.com/view/learn-navigate-cities-nips18
786. **Modular Networks: Learning to Decompose Neural Computation** --*Louis Kirsch &middot; Julius Kunze &middot; David Barber*
 > Scaling model capacity has been vital in the success of deep learning. For a typical network, necessary compute resources and training time grow dramatically with model size. Conditional computation is a promising way to increase the number of parameters with a relatively small increase in resources. We propose a training algorithm that flexibly chooses neural modules based on the data to be processed. Both the decomposition and modules are learned end-to-end. In contrast to existing approaches, training does not rely on regularization to enforce diversity in module use. We apply modular networks both to image recognition and language modeling tasks, where we achieve superior performance compared to several baselines. Introspection reveals that modules specialize to interpretable contexts.
787. **Meta-Gradient Reinforcement Learning** --*Zhongwen Xu &middot; Hado van Hasselt &middot; David Silver*
 > The goal of reinforcement learning algorithms is to estimate and/or optimise the value function. However, unlike supervised learning, no teacher or oracle is available to provide the true value function. Instead, the majority of reinforcement learning algorithms estimate and/or optimise a proxy for the value function. This proxy is typically based on a sampled and bootstrapped approximation to the true value function, known as a \emph{return}. The particular choice of return is one of the chief components determining the nature of the algorithm: the rate at which future rewards are discounted; when and how values should be bootstrapped; or even the nature of the rewards themselves. It is well-known that these decisions are crucial to the overall success of RL algorithms. We introduce a novel, gradient-based meta-learning algorithm that is able to adapt the nature of the return, online, whilst interacting and learning from the environment. When applied to 57 games on the Atari 2600 environment over 200 million frames, our algorithm achieved a new state-of-the-art.
788. **Gaussian Process Conditional Density Estimation** --*Vincent Dutordoir &middot; Hugh Salimbeni &middot; James Hensman &middot; Marc Deisenroth*
 > Conditional Density Estimation (CDE) models deal with estimating conditional distributions. The conditions imposed on the distribution are the inputs of the model. CDE is a challenging task as there is a fundamental trade-off between model complexity, representational capacity and overfitting. In this work, we propose to extend the model's input with latent variables and use Gaussian processes (GP) to map this augmented input onto samples from the conditional distribution. Our Bayesian approach allows for the modeling of small datasets, but we also provide the machinery for it to be applied to big data using stochastic variational inference. Our approach can be used to model densities even in sparse data regions, and allows for sharing learned structure between conditions. We illustrate the effectiveness and wide-reaching applicability of our model on a variety of real-world problems, such as spatio-temporal density estimation of taxi drop-offs, non-Gaussian noise modeling, and few-shot learning on omniglot images.
789. **Local Differential Privacy for Evolving Data** --*Matthew  Joseph &middot; Aaron Roth &middot; Jonathan Ullman &middot; Bo Waggoner*
 > There are now several large scale deployments of differential privacy used to collect statistical information about users. However, these deployments periodically recollect the data and recompute the statistics using algorithms designed for a single use. As a result, these systems do not provide meaningful privacy guarantees over long time scales. Moreover, existing techniques to mitigate this effect do not apply in the ``local model'' of differential privacy that these systems use.
790. **MetaGAN: An Adversarial Approach to Few-Shot Learning** --*Ruixiang ZHANG &middot; Tong Che &middot; Zoubin Ghahramani &middot; Yoshua Bengio &middot; Yangqiu Song*
 > In this paper, we propose a conceptually simple and general framework called MetaGAN for few-shot learning problems. Most state-of-the-art few-shot classification models can be integrated with MetaGAN in a principled and straightforward way. By introducing an adversarial generator conditioned on tasks, we augment vanilla few-shot classification models with the ability to discriminate between real and fake data.  We argue that this GAN-based approach can help few-shot classifiers to learn sharper decision boundary, which could generalize better. We show that with our MetaGAN framework, we can extend supervised few-shot learning models to naturally cope with unsupervised data. Different from previous work in semi-supervised few-shot learning, our algorithms can deal with semi-supervision at both sample-level and task-level. We give theoretical justifications of the strength of MetaGAN, and validate the effectiveness of MetaGAN on challenging few-shot image classification benchmarks.
791. **Non-monotone Submodular Maximization in Exponentially Fewer Iterations** --*Eric Balkanski &middot; Adam Breuer &middot; Yaron Singer*
 > In this paper we consider parallelization for applications whose objective can be expressed as maximizing a non-monotone submodular function under a cardinality constraint. Our main result is an algorithm whose approximation is arbitrarily close to 1/2e in O(log^2 n) adaptive rounds, where n is the size of the ground set. This is an exponential speedup in parallel running time over any previously studied algorithm for constrained non-monotone submodular maximization. Beyond its provable guarantees, the algorithm performs well in practice. Specifically, experiments on traffic monitoring and personalized data summarization applications show that the algorithm finds solutions whose values are competitive with state-of-the-art algorithms while running in exponentially fewer parallel iterations.
792. **Modelling sparsity, heterogeneity, reciprocity and community structure in temporal interaction data** --*Xenia Miscouridou &middot; Francois Caron &middot; Yee Whye Teh*
 > We propose a novel class of network models for temporal dyadic interaction data. Our objective is to capture important features often observed in social interactions: sparsity, degree heterogeneity, community structure and reciprocity. We use mutually-exciting Hawkes processes to model the interactions between each (directed) pair of individuals. The intensity of each process allows interactions to arise as responses to opposite interactions (reciprocity), or due to shared interests between individuals (community structure). For sparsity and degree heterogeneity, we build the non time dependent part of the intensity function on compound random measures following Todeschini et al., 2016.  We conduct experiments on real-world temporal interaction data and show that the proposed model outperforms competing approaches for link prediction, and leads to interpretable parameters.
793. **GIANT: Globally Improved Approximate Newton Method for Distributed Optimization** --*Shusen Wang &middot; Farbod Roosta-Khorasani &middot; Peng Xu &middot; Michael W Mahoney*
 > For distributed computing environment, we consider the empirical risk minimization problem and propose a distributed and communication-efficient Newton-type optimization method. At every iteration, each worker locally finds an Approximate NewTon (ANT) direction, which is sent to the main driver. The main driver, then, averages all the ANT directions received from workers to form a Globally Improved ANT (GIANT) direction. GIANT is highly communication efficient and naturally exploits the trade-offs between local computations and global communications in that more local computations result in fewer overall rounds of communications. Theoretically, we show that GIANT enjoys an improved convergence rate as compared with first-order methods and existing distributed Newton-type methods. Further, and in sharp contrast with many existing distributed Newton-type methods, as well as popular first-order methods, a highly advantageous practical feature of GIANT is that it only involves one tuning parameter. We conduct large-scale experiments on a computer cluster and, empirically, demonstrate the superior performance of GIANT.
794. **Structured Local Minima in Sparse Blind Deconvolution** --*Yuqian Zhang &middot; Han-wen Kuo &middot; John Wright*
 > Blind deconvolution is a ubiquitous problem of recovering two unknown signals from their convolution. Unfortunately, this is an ill-posed problem in general. This paper focuses on the {\em short and sparse} blind deconvolution problem, where the one unknown signal is short and the other one is sparsely and randomly supported. This variant captures the structure of the unknown signals in several important applications. We assume the short signal to have unit $\ell^2$ norm and cast the blind deconvolution problem as a nonconvex optimization problem over the sphere. We demonstrate that (i) in a certain region of the sphere, every local optimum is close to some shift truncation of the ground truth, and (ii) for a generic short signal of length $k$, when the sparsity of activation signal $\theta\lesssim k^{-2/3}$ and number of measurements $m\gtrsim\poly\paren{k}$, a simple initialization method together with a descent algorithm which escapes strict saddle points recovers a near shift truncation of the ground truth kernel.  
795. **Breaking the Span Assumption Yields Fast Finite-Sum Minimization** --*Robert Hannah &middot; Yanli Liu &middot; Daniel O'Connor &middot; Wotao Yin*
 > In this paper, we show that SVRG and SARAH can be modified to be fundamentally faster than all of the other standard algorithms that minimize the sum of $n$ smooth functions, such as SAGA, SAG, SDCA, and SDCA without duality. Most finite sum algorithms follow what we call the ``span assumption'': Their updates are in the span of a sequence of component gradients chosen in a random IID fashion. In the big data regime, where the condition number $\kappa=O(n)$, the span assumption prevents algorithms from converging to an approximate solution of accuracy $\epsilon$ in less than $n\ln(1/\eps)$ iterations. SVRG and SARAH do not follow the span assumption since they are updated with a hybrid of full-gradient and component-gradient information. We show that because of this, they can be up to $\Omega(1+(\ln(n/\kappa))_+)$ times faster. In particular, to obtain an accuracy $\epsilon = 1/n^\alpha$ for $\kappa=n^\beta$ and $\alpha,\beta\in(0,1)$, modified SVRG requires $O(n)$ iterations, whereas algorithms that follow the span assumption require $\cO\p{n\ln\p{n}}$ iterations. Moreover, we present lower bound results that show this speedup is optimal, and provide analysis to help explain why this speedup exists. With the understanding that the span assumption is a point of weakness of finite sum algorithms, future work may purposefully exploit this to yield even faster algorithms in the big data regime.
796. **Overfitting or perfect fitting? Risk bounds for classification and regression rules that interpolate** --*Mikhail Belkin &middot; Daniel Hsu &middot; Partha Mitra*
 > Many modern machine learning models are trained to achieve zero or near-zero training error in order to obtain near-optimal (but non-zero) test error. This phenomenon of strong generalization performance for ``overfitted'' / interpolated classifiers appears to be  ubiquitous in high-dimensional data, having been observed in deep networks, kernel machines, boosting and random forests. Their performance is robust  even when the data contain large amounts of label noise.   Very little theory is available to explain these observations. The vast majority of theoretical analyses of generalization allows for interpolation only when there is little or no label noise. This paper takes a step toward a theoretical foundation for interpolated classifiers by analyzing local interpolating schemes, including  geometric simplicial interpolation algorithm and weighted $k$-nearest neighbor schemes. Consistency or near-consistency is proved for these schemes in  classification and regression problems. These schemes have an inductive bias that benefits from higher dimension, a kind of ``blessing of dimensionality''. Finally, connections to kernel machines, random forests, and adversarial examples in the interpolated regime are discussed.
797. **Multi-View Silhouette and Depth Decomposition for High Resolution 3D Object Representation** --*Edward Smith &middot; Scott Fujimoto &middot; David Meger*
 > We consider the problem of scaling deep generative shape models to high-resolution. Drawing motivation from the canonical view representation of objects, we introduce a novel method for the fast up-sampling of 3D objects in voxel space through networks that perform super-resolution on the six orthographic depth projections. This allows us to efficiently generate high-resolution objects, without the cubic computational costs associated with voxel data. We further decompose the learning problem into silhouette and depth prediction to capture both structure and fine detail, easing the burden of an individual network generating sharp edges. We evaluate our work on multiple experiments concerning high-resolution 3D objects, and show our system is capable of accurately producing objects at resolutions as large as 512x512x512 -- the highest resolution reported for this task, to our knowledge. We achieve state-of-the-art performance on 3D object reconstruction from RGB images on the ShapeNet dataset, and further demonstrate the first effective 3D super-resolution method. 
798. **Smoothed analysis of the low-rank approach for smooth semidefinite programs** --*Thomas Pumir &middot; Samy Jelassi &middot; Nicolas Boumal*
 >   We consider semidefinite programs (SDPs) of size $n$ with equality constraints.   In order to overcome the scalability issues arising for large instances, Burer and Monteiro proposed a factorized approach based on optimizing over a matrix $Y$ of size $n\times k$ such that $X=YY^*$ is the SDP variable. The advantages of such formulation are twofold: the dimension of the optimization variable is reduced and positive semidefiniteness is naturally enforced. However, problem in $Y$ is non-convex. In prior work, it has been shown that, when the constraints on the factorized variable regularly define a smooth manifold, almost all second-order stationary points (SOSPs) are optimal. Nevertheless, in practice, one can only compute points which approximately satisfy necessary optimality conditions, so that it is crucial to know whether such points are also approximately optimal. To this end, and under similar assumptions, we use smoothed analysis to show that ASOSPs for a randomly perturbed objective function are approximate global optima, as long as the number of constraints scales sub-quadratically with the desired rank of the optimal solution. In this setting, an approximate optimum $Y$ maps to the approximate optimum $X=YY^*$ of the  SDP. We particularize our results to SDP relaxations of phase retrieval.
799. **BourGAN: Generative Networks with Metric Embeddings** --*Chang Xiao &middot; Peilin Zhong &middot; Changxi Zheng*
 > This paper addresses the mode collapse for generative adversarial networks (GANs). We view modes as a geometric structure of data distribution in a metric space. Under this geometric lens, we embed subsamples of the dataset from an arbitrary metric space into the `2 space, while preserving their pairwise distance distribution. Not only does this metric embedding determine the dimensionality of the latent space automatically, it also enables us to construct a mixture of Gaussians to draw latent space random vectors. We use the Gaussian mixture model in tandem with a simple augmentation of the objective function to train GANs. Every major step of our method is supported by theoretical analysis, and our experiments on real and synthetic data confirm that the generator is able to produce samples spreading over most of the modes while avoiding unwanted samples, outperforming several recent GAN variants on a number of metrics and offering new features.
800. **On the Generalization of Single-View 3D Reconstruction Algorithms** --*Xiuming Zhang &middot; Zhoutong Zhang &middot; Chengkai Zhang &middot; Jiajun Wu &middot; Josh Tenenbaum &middot; Bill Freeman*
 > From a single view, humans are able to hallucinate the full 3D shape of the object in the image, even if it is from a novel, unseen category. Contemporary AI systems for single-image 3D reconstruction often lack this ability, because the shape priors they learned is often tied to the training object classes. In this paper, we study the task of single-image 3D reconstruction, but attempting to recover the full 3D shape of an object outside the training categories. Our model combines 2.5D sketches (depths and silhouettes), spherical shape representations, and 3D voxels in a principled manner. Experiments demonstrate that it achieves state-of-the-art results on generalizing to diverse novel object categories.
801. **A Practical Algorithm for Distributed Clustering and Outlier Detection** --*Jiecao Chen &middot; Erfan Sadeqi Azer &middot; Qin Zhang*
 > We study the classic k-means/median clustering, which are fundamental problems in unsupervised learning, in the setting where data are partitioned across multiple sites, and where we are allowed to discard a small portion of the data by labeling them as outliers.  We propose a simple approach based on constructing small summary for the original dataset. The proposed method is time and communication efficient, has good approximation guarantees, and can identify the global outliers effectively. <br /> To the best of our knowledge, this is the first practical algorithm with theoretical guarantees for distributed clustering with outliers. Our experiments on both real and synthetic data have demonstrated the clear superiority of our algorithm against all the baseline algorithms in almost all metrics. 
802. **Unsupervised Adversarial Invariance** --*Ayush Jaiswal &middot; Rex Yue Wu &middot; Wael Abd-Almageed &middot; Prem Natarajan*
 > Data representations that contain all the information about target variables but are invariant to nuisance factors benefit supervised learning algorithms by preventing them from learning associations between these factors and the targets, thus reducing overfitting. We present a novel unsupervised invariance induction framework for neural networks that learns a split representation of data through competitive training between the prediction task and a reconstruction task coupled with disentanglement, without needing any labeled information about nuisance factors or domain knowledge. We describe an adversarial instantiation of this framework and provide analysis of its working. Our unsupervised model outperforms state-of-the-art methods, which are supervised, at inducing invariance to inherent nuisance factors, effectively using synthetic data augmentation to learn invariance, and domain adaptation. Our method can be applied to any prediction task, eg., binary/multi-class classification or regression, without loss of generality.
803. **Active Geometry-Aware Visual Recognition in Cluttered Scenes** --*Ricson Cheng &middot;   &middot; Katerina Fragkiadaki*
 > Cross-object occlusions remain an important source of failures for current state-of-the-art object detectors. Actively selecting camera views for undoing  occlusions and recovering missing information has been identified as an important field of research since as early as 1980's, under the name active vision. Yet, 1980's active vision was not equipped with deep neural detectors, memory modules, or view selection policies, and often attempted tasks and imagery that would appear elementary with current detectors, even from a single camera view. On the other hand, the recent resurrection of active view selection policies has focused on reconstructing or classifying isolated objects. This work presents a paradigm for active object recognition under heavy occlusions, as was the original premise. It introduces a geometry-aware 3D neural memory that accumulates information of the full scene across multiple camera views into a 3D feature tensor in a geometrically consistent manner: information regarding the same 3D physical point is placed nearby in the tensor using egomotion-aware feature warping and (learned) depth-aware unprojection operations. Object detection, segmentation, and 3D reconstruction is then carried out directly using the accumulated 3D feature memory. The proposed model does not need to commit early to object detections, as current geometry-unaware object detection in 2D videos, and generalizes much better than geometry-unaware LSTM/GRU memories. The lack of geometric constraints on previous architectures appears to be the bottleneck for handling combinatorial explosions of visual data due to cross-object occlusions. The proposed model handles heavy occlusions even when trained with very little training data, by moving the head of the active observer between nearby views, seamlessly combining geometry with learning from experience.
804. **Power-law efficient neural codes provide general link between perceptual bias and discriminability** --*Michael J Morais &middot; Jonathan W Pillow*
 > Recent work in theoretical neuroscience has shown that information-theoretic "efficient" neural codes, which allocate neural resources to maximize the mutual information between stimuli and neural responses, give rise to a lawful relationship between perceptual bias and discriminability that is observed across a wide variety of psychophysical tasks in human observers (Wei &amp; Stocker 2017). Here we generalize these results to show that the same law arises under a much larger family of optimal neural codes, introducing a unifying framework that we call power-law efficient coding. Specifically, we show that the same lawful relationship between bias and discriminability arises whenever Fisher information is allocated proportional to any power of the prior distribution. This family includes neural codes that are optimal for minimizing Lp error for any p, indicating that the lawful relationship observed in human psychophysical data does not require information-theoretically optimal neural codes. Furthermore, we derive the exact constant of proportionality governing the relationship between bias and discriminability for different power laws (which includes information-theoretically optimal codes, where the power is 2, and so-called discrimax codes, where power is 1/2), and different choices of optimal decoder. As a bonus, our framework provides new insights into "anti-Bayesian" perceptual biases, in which percepts are biased away from the center of mass of the prior. We derive an explicit formula that clarifies precisely which combinations of neural encoder and decoder can give rise to such biases. 
805. **Revisiting Decomposable Submodular Function Minimization with Incidence Relations** --*Pan Li &middot; Olgica Milenkovic*
 > We introduce a new approach to decomposable submodular function minimization (DSFM) that exploits incidence relations. Incidence relations describe which variables effectively influence the component functions, and when properly utilized, they allow for improving the convergence rates of DSFM solvers. Our main results include the precise parametrization of the DSFM problem based on incidence relations, the development of new scalable alternative projections and parallel coordinate descent methods and an accompanying rigorous analysis of their convergence rates. 
806. **A Smoothed Analysis of the Greedy Algorithm for the Linear Contextual Bandit Problem** --*Sampath Kannan &middot; Jamie Morgenstern &middot; Aaron Roth &middot; Bo Waggoner &middot; Zhiwei  Steven Wu*
 > Bandit learning is characterized by the tension between long-term exploration and short-term exploitation.  However, as has recently been noted, in settings in which the choices of the learning algorithm correspond to important decisions about individual people (such as criminal recidivism prediction, lending, and sequential drug trials), exploration corresponds to explicitly sacrificing the well-being of one individual for the potential future benefit of others. In such settings, one might like to run a ``greedy'' algorithm, which always makes the optimal decision for the individuals at hand --- but doing this can result in a catastrophic failure to learn. In this paper, we consider the linear contextual bandit problem and revisit the performance of the greedy algorithm.
807. **The Description Length of Deep Learning models** --*Léonard Blier &middot; Yann Ollivier*
 > Deep learning models often have more parameters than observations, and still perform well. This is sometimes described as a paradox. In this work, we show experimentally that despite their huge number of parameters, deep neural networks can compress the data losslessly \emph{even when taking the cost of encoding the parameters into account}. Such a compression viewpoint originally motivated the use of \emph{variational methods} in neural networks \cite{Hinton,Schmidhuber1997}. However, we show that these variational methods provide surprisingly poor compression bounds, despite being explicitly built to minimize such bounds. This might explain the relatively poor practical performance of variational methods in deep learning. Better encoding methods, imported from the Minimum Description Length (MDL) toolbox, yield much better compression values on deep networks.
808. **Trajectory Convolution for Action Recognition** --*Yue Zhao &middot; Yuanjun Xiong &middot; Dahua Lin*
 > How to leverage the temporal dimension is a key question in video analysis. Recent work suggests an efficient approach to video feature learning, namely, factorizing 3D convolutions into separate components respectively for spatial and temporal convolutions. The temporal convolution, however, comes with an implicit assumption – the feature maps across time steps are well aligned so that the features at the same locations can be aggregated. This assumption may be overly strong in practical applications, especially in action recognition where the motion of people or objects is a crucial aspect. In this work, we propose a new CNN architecture TrajectoryNet, which incorporates trajectory convolution, a new operation for integrating features along the temporal dimension, to replace the standard temporal convolution. This operation explicitly takes into account the location changes caused by deformation or motion, allowing the visual features to be aggregated along the the motion paths. On two very large-scale action recognition datasets, namely, Something-Something and Kinetics, the proposed network architecture achieves notable improvement over strong baselines.
809. **Mixture Matrix Completion** --*Daniel Pimentel-Alarcon*
 > Completing a data matrix X has become an ubiquitous problem in modern data science, with motivations in recommender systems, computer vision, and networks inference, to name a few. One typical assumption is that X is low-rank. A more general model assumes that each column of X corresponds to one of several low-rank matrices. This paper generalizes these models to what we call mixture matrix completion (MMC): the case where each entry of X corresponds to one of several low-rank matrices. MMC is a more accurate model for recommender systems, and brings more flexibility to other completion and clustering problems. We make four fundamental contributions about this new model. First, we show that MMC is theoretically possible (well-posed). Second, we give its precise information-theoretic identifiability conditions. Third, we derive the sample complexity of MMC. Finally, we give a practical algorithm for MMC with performance comparable to the state-of-the-art for simpler related problems, both on synthetic and real data.
810. **MULAN: A Blind and Off-Grid Method for Multichannel Echo Retrieval** --*Helena Peic Tukuljac &middot; Antoine Deleforge*
 > This paper addresses the general problem of blind echo retrieval, i.e., given M sensors measuring in the discrete-time domain M mixtures of K delayed and attenuated copies of an unknown source signal, can the echo location and weights be recovered? This problem has broad applications in fields such as sonars, seismology, ultrasounds or room acoustics. It belongs to the broader class of blind channel identification problems, which have been intensively studied in signal processing. All existing methods proceed in two steps: (i) blind estimation of sparse discrete-time filters and (ii) echo information retrieval by peak picking. The precision of these methods is fundamentally limited by the rate at which the signals are sampled: estimated echo locations are necessary on-grid, and since true locations never match the sampling grid, the weight estimation precision is also strongly limited. This is the so-called basis-mismatch problem in compressed sensing. We propose a radically different approach to the problem, building on top of the framework of finite-rate-of-innovation sampling. The approach operates directly in the parameter-space of echo locations and weights, and enables near-exact blind and off-grid echo retrieval from discrete-time measurements. It is shown to outperform conventional methods by several orders of magnitudes in precision.
811. **Dual Principal Component Pursuit: Improved Analysis and Efficient Algorithms** --*Zhihui Zhu &middot; Yifan Wang &middot; Daniel Robinson &middot; Daniel Naiman &middot; Rene Vidal &middot; Manolis Tsakiris*
 > Recent methods for learning a linear subspace from data corrupted by outliers are based on convex L1 and nuclear norm optimization and require the dimension of the subspace and the number of outliers to be sufficiently small [1]. In sharp contrast, the recently proposed Dual Principal Component Pursuit (DPCP) method [2] can provably handle subspaces of high dimension by solving a non-convex L1 optimization problem on the sphere. However, its geometric analysis is based on quantities that are difficult to interpret and are not amenable to  statistical analysis. In this paper we provide a refined geometric analysis and a new statistical analysis that show that DPCP can tolerate as many outliers as the square of the number of inliers, thus improving upon other provably correct robust PCA methods. We also propose a scalable Projected Sub-Gradient Descent method (DPCP-PSGD) for solving the DPCP problem and show it admits linear convergence even though the underlying optimization problem is non-convex and non-smooth. Experiments on road plane detection from 3D point cloud data demonstrate that DPCP-PSGD can be more efficient than the traditional RANSAC algorithm, which is one of the most popular methods for such computer vision applications.
812. **Norm matters: efficient and accurate normalization schemes in deep networks** --*Elad Hoffer &middot; Ron Banner &middot; Itay Golan &middot; Daniel Soudry*
 > Over the past few years, Batch-Normalization has been commonly used in deep networks, allowing faster training and high performance for a wide variety of applications. However, the reasons behind its merits remained unanswered, with several shortcomings that hindered its use for certain tasks. In this work, we present a novel view on the purpose and function of normalization methods and weight-decay, as tools to decouple weights' norm from the underlying optimized objective. This property highlights the connection between practices such as normalization, weight decay and learning-rate adjustments. We suggest several alternatives to the widely used $L^2$ batch-norm, using normalization in $L^1$ and $L^\infty$ spaces that can substantially improve numerical stability in low-precision implementations as well as provide computational and memory benefits. We demonstrate that such methods enable the first batch-norm alternative to work for half-precision implementations. Finally, we suggest a modification to weight-normalization, which improves its performance on large-scale tasks.
813. **DeepExposure: Learn to Expose Photos with Asynchronously Reinforced Adversarial Learning** --*Runsheng Yu &middot; Wenyu Liu &middot; Yasen Zhang &middot; Zhi Qu &middot; Bo Zhang &middot; Deli Zhao*
 > The accurate exposure is the key of capturing high-quality photos in computational photography, especially for mobile phones that are limited by sizes of camera modules. Inspired by exposure blending with luminosity masks usually applied by professional photographers, in this paper, we develop a novel algorithm for learning local exposures with deep reinforcement learning and adversarial learning. To be specific, we segment an image into sub-images that can reflect variations of dynamic range exposures according to raw low-level features. Based on these sub-images, a local exposure for each sub-image is automatically learned by virtue of policy network sequentially while the reward of learning is globally designed for striking a balance of overall exposures. The aesthetic evaluation function is approximated by discriminator in generative adversarial networks. The reinforcement learning and the adversarial learning are trained collaboratively by asynchronous deterministic policy gradient and generative loss approximation. To further simply the algorithmic architecture, we also prove the feasibility of leveraging the discriminator as the value function. Further more, we employ each local exposure to retouch the raw input image respectively, thus delivering multiple retouched images under different exposures which are fused with exposure blending. The extensive experiments verify that our algorithms are superior to state-of-the-art methods in terms of quantitative accuracy and visual illustration.
814. **Algorithmic Linearly Constrained Gaussian Processes** --*Markus Lange-Hegermann*
 > We algorithmically construct multi-output Gaussian process priors which satisfy linear differential equations. Our approach attempts to parametrize all solutions of the equations using Gröbner bases. If successful, a push forward Gaussian process along the paramerization is the desired prior. We consider several examples from physics, geomathmatics and control, among them the full inhomogeneous system of Maxwell's equations. By bringing together stochastic learning and computeralgebra in a novel way, we combine noisy observations with precise algebraic computations.
815. **Overlapping Clustering, and One (class) SVM to Bind Them All** --*Xueyu Mao &middot; Purnamrita Sarkar &middot; Deepayan Chakrabarti*
 > People belong to multiple communities, words belong to multiple topics, and books cover multiple genres; overlapping clusters are commonplace. Many existing overlapping clustering methods model each person (or word, or book) as a non-negative weighted combination of "exemplars" who belong solely to one community, with some small noise. Geometrically, each person is a point on a cone whose corners are these exemplars. This basic form encompasses the widely used Mixed Membership Stochastic Blockmodel of networks and its degree-corrected variants, as well as topic models such as LDA. We show that a simple one-class SVM yields provably consistent parameter inference for all such models, and scales to large datasets. Experimental results on several simulated and real datasets show our algorithm (called \svmcone) is both accurate and scalable.
816. **Regularizing by the Variance of the Activations' Sample-Variances** --*Etai Littwin &middot; Lior Wolf*
 > Normalization techniques play an important role in supporting efficient and often more effective training of deep neural networks. While conventional methods explicitly normalize the activations, we suggest to add a loss term instead. This new loss term encourages the variance of the activations to be stable and not vary from one random mini-batch to the next. As we prove, this encourages the activations to be distributed around a few distinct modes. We also show that if the inputs are from a mixture of two Gaussians, the new loss would either join the two together, or separate between them optimally in the LDA sense, depending on the prior probabilities. Finally, we are able to link the new regularization term to the batchnorm method, which provides it with a regularization perspective. Our experiments demonstrate an improvement in accuracy over the batchnorm technique for both CNNs and fully connected networks. 
817. **One-Shot Unsupervised Cross Domain Translation** --*Sagie Benaim &middot; Lior Wolf*
 > Given a single image $x$ from domain $A$ and a set of images from domain $B$, our task is to generate the analogous of $x$ in $B$. We argue that this task could be a key AI capability that underlines the ability of cognitive agents to act in the world and present empirical evidence that the existing unsupervised domain translation methods fail on this task. Our method follows a two step process. First, a variational autoencoder for domain $B$ is trained. Then, given the new sample $x$, we create a variational autoencoder for domain $A$ by adapting the layers that are close to the image in order to directly fit $x$, and only indirectly adapt the other layers. Our experiments indicate that the new method does as well, when trained on one sample $x$, as the existing domain transfer methods, when these enjoy a multitude of training samples from domain $A$. Our code will be made publicly available. 
818. **Automatic Program Synthesis of Long Programs with a Learned Garbage Collector** --*Amit Zohar &middot; Lior Wolf*
 > We consider the problem of generating automatic code given sample input-output pairs. We train a neural network to map from the current state and the outputs to the program's next statement. The neural network optimizes multiple tasks concurrently: The next operation out of a set of high level commands, the operands of the next statement, and which variables can be dropped from memory. Using our method we are able to create programs that are more than twice as long as existing state-of-the-art solutions, while improving the success rate for comparable lengths, and cutting the run-time by two orders of magnitude. 
819. **SEGA: Variance Reduction via Gradient Sketching** --*Filip Hanzely &middot; Konstantin Mishchenko &middot; Peter Richtarik*
 > We propose a novel randomized first order optimization method---SEGA (SkEtched GrAdient method)---which progressively throughout its iterations builds a variance-reduced estimate of the gradient from random linear measurements (sketches) of the gradient provided  at each iteration by an oracle. In each iteration, SEGA updates the current estimate of the gradient  through a sketch-and-project operation using the information provided by the latest sketch, and this is subsequently used to compute an unbiased estimate of the true gradient through a random relaxation procedure. This unbiased estimate is then used to perform a gradient step. Unlike standard subspace descent methods, such as coordinate descent, SEGA can be used for optimization problems with  a non-separable proximal term. We provide a general convergence analysis and prove linear convergence for strongly convex objectives. In the special case of  coordinate sketches, SEGA can be enhanced with various techniques such as importance sampling, minibatching and acceleration, and its rate is up to a small constant factor identical to the best-known rate of coordinate descent. 
820. **Nonparametric learning for Bayesian models via randomized objective functions** --*Simon Lyddon &middot; Stephen Walker &middot; Chris C Holmes*
 > We present a Bayesian nonparametric (NP) approach to learning from data that is centered around a conventional probabilistic model, but does not assume that this model is true. This affords a trivially parallelizable, scalable Monte Carlo sampling scheme based on the notion of randomized objective functions,  which map posterior samples from the baseline model into posterior samples from the NP update. This is particularly attractive for regularizing NP methods or correcting approximate models, such as variational Bayes (VB). We demonstrate the approach on a number of examples including VB classifiers and Bayesian random forests.
821. **Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning** --*Supasorn Suwajanakorn &middot; Mohammad Norouzi &middot; Noah Snavely &middot; Jonathan Tompson*
 > This paper presents KeypointNet, an end-to-end geometric reasoning framework to learn an optimal set of category-specific keypoints, along with their detectors to predict 3D keypoints in a single 2D input image. We demonstrate this framework on 3D pose estimation task by proposing a differentiable pose objective that seeks the optimal set of keypoints for recovering the relative pose between two views of an object. Our network automatically discovers a consistent set of keypoints across viewpoints of a single object as well as across all object instances of a given object class. Importantly, we find that our end-to-end approach using no ground-truth keypoint annotations outperforms a fully supervised baseline using the same neural network architecture for the pose estimation task.  The discovered 3D keypoints across the car, chair, and plane categories of ShapeNet are visualized at https://keypoints.github.io/
822. **Sequential Context Encoding for Duplicate Removal** --*Lu Qi &middot; Shu Liu &middot; Jianping Shi &middot; Jiaya Jia*
 > Duplicate removal is a critical step to accomplish a reasonable amount of predictions in prevalent proposal-based object detection frameworks. Albeit simple and effective, most previous algorithms utilized a greedy process without making sufficient use of properties of input data. In this work, we design a new two-stage framework to effectively select the appropriate proposal candidate for each object. The first stage suppresses most of easy negative object proposals, while the second stage selects true positives in the reduced proposal set. These two stages share the same network structure, an encoder and a decoder formed as recurrent neural networks (RNN) with global attention and context gate. The encoder scans proposal candidates in a sequential manner to capture the global context information, which is then fed to the decoder to extract optimal proposals. In our extensive experiments, the proposed method outperforms other alternatives by a large margin.
823. **Learning Optimal Reserve Price against Non-myopic Bidders** --*Jinyan Liu &middot; Zhiyi Huang &middot; Xiangning Wang*
 > We consider the problem of learning optimal reserve price in repeated auctions against non-myopic bidders, who may bid strategically in order to gain in future rounds even if the single-round auctions are truthful. Previous algorithms, e.g., empirical pricing, do not provide non-trivial regret rounds in this setting in general. We introduce algorithms that obtain small regret against non-myopic bidders either when the market is large, i.e., no bidder appears in a constant fraction of the rounds, or when the bidders are impatient, i.e., they discount future utility by some factor mildly bounded away from one. Our approach carefully controls what information is revealed to each bidder, and builds on techniques from differentially private online learning as well as the recent line of works on jointly differentially private algorithms.
824. **Querying Complex Networks in Vector Space** --*Will Hamilton &middot; Payal Bajaj &middot; Marinka Zitnik &middot; Dan Jurafsky &middot; Jure Leskovec*
 > Learning vector embeddings of complex networks is a powerful approach used to predict missing or unobserved edges in network data. However, an open challenge in this area is developing techniques that can reason about subgraphs in network data, which can involve the logical conjunction of several edge relationships. Here we introduce a framework to make predictions about conjunctive logical queries---i.e., subgraph relationships---on heterogeneous network data. In our approach, we embed network nodes in a low-dimensional space and represent logical operators as learned geometric operations (e.g., translation, rotation) in this embedding space. We prove that a small set of geometric operations are sufficient to represent conjunctive logical queries on a network, and we introduce a series of increasingly strong implementations of these operators. We demonstrate the utility of this framework in two application studies on networks with millions of edges: predicting unobserved subgraphs in a network of drug-gene-disease interactions and in a network of social interactions derived from a popular web forum. These experiments demonstrate how our framework can efficiently make logical predictions such as ``what drugs are likely to target proteins involved with both diseases X and Y?'' Together our results highlight how imposing logical structure can make network embeddings more useful for large-scale knowledge discovery.
825. **Neural Architecture Search with Bayesian Optimisation and Optimal Transport** --*Kirthevasan Kandasamy &middot; Willie Neiswanger &middot; Jeff Schneider &middot; Barnabas Poczos &middot; Eric Xing*
 > Bayesian Optimisation (BO) refers to a class of methods for global optimisation of a function f which is only accessible via point evaluations. It is typically used in settings where f is expensive to evaluate. A common use case for BO in machine learning is model selection, where it is not possible to analytically model the generalisation performance of a statistical model, and we resort to noisy and expensive training and validation procedures to choose the best model. Conventional BO methods have focused on Euclidean and categorical domains, which, in the context of model selection, only permits tuning scalar hyper-parameters of machine learning algorithms. However, with the surge of interest in deep learning, there is an increasing demand to tune neural network architectures. In this work, we develop NASBOT, a Gaussian process based BO framework for neural architecture search. To accomplish this, we develop a distance metric in the space of neural network architectures which can be computed efficiently via an optimal transport program. This distance might be of independent interest to the deep learning community as it may find applications outside of BO. We demonstrate that NASBOT outperforms other alternatives for architecture search in several cross validation based model selection tasks on multi-layer perceptrons and convolutional neural networks.
826. **Generalized Zero-Shot Learning with Deep Calibration Network** --*Shichen Liu &middot; Mingsheng Long &middot; Jianmin Wang &middot; Michael Jordan*
 > A technical challenge of deep learning is recognizing target classes without seen data. Zero-shot learning leverages semantic representations such as attributes or class prototypes to bridge source and target classes. Existing standard zero-shot learning methods may be prone to overfitting the seen data of source classes as they are blind to the semantic representations of target classes. In this paper, we study generalized zero-shot learning that assumes accessible to target classes for unseen data during training, and prediction on unseen data is made by searching on both source and target classes. We propose a novel Deep Calibration Network (DCN) approach towards this generalized zero-shot learning paradigm, which enables simultaneous calibration of deep networks on the confidence of source classes and uncertainty of target classes. Our approach maps visual features of images and semantic representations of class prototypes to a common embedding space such that the compatibility of seen data to both source and target classes are maximized. We show superior accuracy of our approach over the state of the art on benchmark datasets for generalized zero-shot learning, including AwA, CUB, SUN, and aPY.
827. **SplineNets: Continuous Neural Decision Graphs** --*Cem Keskin &middot; Shahram Izadi*
 > We present SplineNets, a practical and novel approach for using conditioning in convolutional neural networks (CNNs). Our method dramatically reduces runtime complexity and computation costs of CNNs, while maintaining or even increasing accuracy. SplineNets employ a unified loss function with a desired level of smoothness over both the network and decision parameters, while allowing for sparse activation of a subset of nodes for individual samples. Thus, functions of SplineNets are both dynamic (i.e., conditioned on the input) and hierarchical (i.e., conditioned on the computational path). In particular, we embed weights of functions on smooth, low dimensional manifolds parameterized by compact B-splines, and define decisions as choosing a position on these hyper-surfaces. We further show that by maximizing the mutual information between these latent coordinates and data or its labels, the network can be optimally utilized and specialized. Experiments on various image classification datasets show the power of this new paradigm over regular CNNs. 
828. **Efficient Stochastic Gradient Hard Thresholding** --*Pan Zhou &middot; Xiaotong Yuan &middot; Jiashi Feng*
 > Stochastic gradient hard thresholding methods have recently been shown to work favorably in solving large-scale empirical risk minimization problems under sparsity or rank constraint. Despite the improved iteration complexity over full gradient methods, the gradient evaluation and hard thresholding complexity of the existing stochastic algorithms usually scales linearly with data size, which could still be expensive when data is huge and the hard thresholding step could be as expensive as singular value decomposition in rank-constrained problems. To address these deficiencies, we propose an efficient hybrid stochastic gradient hard thresholding (HSG-HT) method that can be provably shown to have sample-size-independent gradient evaluation and hard thresholding complexity bounds. Specifically, we prove that the stochastic gradient evaluation complexity of HSG-HT scales linearly with inverse of sub-optimality and its hard thresholding complexity scales logarithmically. By applying the heavy ball acceleration technique, we further propose an accelerated variant of HSG-HT which can be shown to have improved factor dependence on restricted condition number. Numerical results confirm our theoretical affirmation and demonstrate the computational efficiency of the proposed methods.
829. **Bayesian Model Selection Approach to Boundary Detection with Non-Local Priors** --*  &middot; Guosheng Yin*
 > We propose a Bayesian model selection (BMS) boundary detection procedure using non-local prior distributions for a sequence of data with multiple systematic mean changes.  By using the non-local priors in the Bayesian model selection framework, the BMS method can effectively suppress the non-boundary spike points with large instantaneous changes. Further, we speed up the algorithm by reducing the multiple change points to a series of single change point detection problems. We establish the consistency of the estimated number and locations of the change points under various prior distributions. Extensive simulation studies  are conducted to compare the BMS with existing methods, and our method is  illustrated with application to the magnetic resonance imaging guided radiation therapy data.
830. **Universal Growth in Production Economies** --*Simina Branzei &middot; Ruta Mehta &middot; Noam Nisan*
 > We study a simple variant of the von Neumann model of an expanding economy, in which multiple producers produce goods according to their production function. The players trade their goods at the market and then use the bundles received as inputs for the production in the next round. We show that a simple decentralized dynamic, where players update their  bids on the goods in the market proportionally to how useful the investments were, leads to growth of the economy in the long term (whenever growth is possible) but also creates unbounded inequality, i.e. very rich and very poor players emerge. We analyze several other phenomena, such as how the relation of a player with others influences its development and the Gini index of the system.
831. **Pelee: A Real-Time Object Detection System on Mobile Devices** --*Jun Wang*
 > An increasing need of running Convolutional Neural Network (CNN) models on mobile devices with limited computing power and memory resource encourages studies on efficient model design. A number of efficient architectures have been proposed in recent years, for example, MobileNet, ShuffleNet, and NASNet-A. However, all these models are heavily dependent on depthwise separable convolution which lacks efficient implementation in most deep learning frameworks. In this study, we propose an efficient architecture named PeleeNet, which is built with conventional convolution instead. On ImageNet ILSVRC 2012 dataset, our proposed PeleeNet achieves a higher accuracy by 0.6% (71.3% vs. 70.7%) and 11% lower computational cost than MobileNet, the state-of-the-art efficient architecture. Meanwhile, PeleeNet is only 66% of the model size of MobileNet. We then propose a real-time object detection system by combining PeleeNet with Single Shot MultiBox Detector (SSD) method and optimizing the architecture for fast speed. Our proposed detection system, named Pelee, achieves 76.4% mAP (mean average precision) on PASCAL VOC2007 and 22.4 mAP on MS COCO dataset at the speed of 17.1 FPS on iPhone 6s and 23.6 FPS on iPhone 8. The result on COCO outperforms YOLOv2 in consideration of a higher precision, 13.6 times lower computational cost and 11.3 times smaller model size. The code and models are open sourced.
832. **Attention in Convolutional LSTM for Gesture Recognition** --*Guangming Zhu &middot; Liang Zhang &middot; Lin Mei &middot; Peiyi Shen &middot; Syed Afaq Ali  Shah &middot; Mohammed Bennamoun*
 > Convolutional long short-term memory (LSTM) networks have been widely used for action/gesture recognition, and different attention mechanisms have also been embedded into the LSTM or the convolutional LSTM (ConvLSTM) networks. Based on the previous gesture recognition architectures which combine the three-dimensional convolution neural network (3DCNN) and ConvLSTM, this paper explores the effects of attention mechanism in ConvLSTM. Several variants of ConvLSTM are evaluated: (a) Removing the convolutional structures of the three gates in ConvLSTM, (b) Applying the attention mechanism on the input of ConvLSTM, (c) Reconstructing the input and (d) output gates respectively with the modified channel-wise attention mechanism. The evaluation results demonstrate that the spatial convolutions in the three gates scarcely contribute to the spatiotemporal feature fusion, and the attention mechanisms embedded into the input and output gates cannot improve the feature fusion. In other words, ConvLSTM mainly contributes to the temporal fusion along with the recurrent steps to learn the long-term spatiotemporal features, when taking as input the spatial or spatiotemporal features. On this basis, a new variant of LSTM is derived, in which the convolutional structures are only embedded into the input-to-state transition of LSTM. The code of the LSTM variants is publicly available.
833. **Virtual Class Enhanced Discriminative Embedding Learning** --*Binghui Chen &middot; Weihong Deng*
 > Recently, learning discriminative features to improve the recognition performances gradually becomes the primary goal of deep learning, and numerous remarkable works have emerged. In this paper, we propose a novel yet extremely simple method Virtual Softmax to enhance the discriminative property of learned features by injecting a dynamic virtual negative class into the original softmax. Injecting virtual class aims to enlarge inter-class margin and compress intra-class distribution by strengthening the decision boundary constraint. Although it seems weird to optimize with this additional virtual class, we show that our method derives from an intuitive and clear motivation, and it indeed encourages the features to be more compact and separable. This paper empirically and experimentally demonstrates the superiority of Virtual Softmax, improving the performances on a variety of object classification and face verification tasks.
834. **Deep Attentive Tracking via Reciprocative Learning** --*Shi Pu &middot; Yibing Song &middot; Chao Ma &middot; Honggang Zhang &middot; Ming-Hsuan Yang*
 > Visual attention, derived from cognitive neuroscience, facilitates human perception on the most pertinent subset of the sensory data. Recently, significant efforts have been made to exploit attention schemes to advance computer vision systems. For visual tracking, it is often challenging to track target objects undergoing large appearance changes. Attention maps facilitate visual tracking by selectively paying attention to temporally invariant motion patterns. Existing tracking-by-detection approaches mainly use additional attention modules to generate feature weights as the classifiers are not equipped with such mechanisms. In this paper, we propose a reciprocative learning algorithm to exploit visual attention for training deep classifiers. The proposed algorithm consists of feed-forward and backward operations to generate attention maps, which serve as regularization terms coupled with the original classification loss function for training. The deep classifier learns to attend to the regions of target objects robust to appearance changes. Extensive experiments on large-scale benchmark datasets show that the proposed attentive tracking method performs favorably against the state-of-the-art approaches.
835. **Evaluating Range-Based Anomaly Detectors** --*Nesime Tatbul &middot; Tae Jun Lee &middot; Stan Zdonik &middot; Mejbah Alam &middot; Justin Gottschlich*
 > Classical anomaly detection is principally concerned with point-based anomalies, those anomalies that occur at a single point in time. Yet, many real-world anomalies are range-based, meaning they occur over a period of time. In this paper, we present a new model that more accurately measures the correctness of anomaly detection systems for range-based anomalies, while subsuming the classical model's ability to classify point-based anomaly detection systems.
836. **Distributed Stochastic Optimization via Adaptive SGD** --*Ashok Cutkosky &middot; Róbert Busa-Fekete*
 > Stochastic convex optimization algorithms are the most popular way to train machine learning models on large-scale data. Scaling up the training process of these models is crucial, but the most popular algorithm, Stochastic Gradient Descent (SGD), is a serial method that is surprisingly hard to parallelize. In this paper, we propose an efficient distributed stochastic optimization method by combing adaptive step sizes with variance reduction techniques. We achieve a linear speedup in the number of machines, constant memory footprint, and only a logarithmic number of communication rounds. Critically, our approach is a black-box reduction that parallelizes any serial SGD algorithm, allowing us to leverage the significant progress that has been made in designing adaptive SGD algorithms. In particular, we achieve optimal convergence rates without any prior knowledge of smoothness parameters, yielding a more robust algorithm that reduces the need for hyperparameter tuning. We implement our algorithm in the Spark distributed framework and exhibit dramatic performance gains on large-scale logistic regression problems.
837. **Random Feature Stein Discrepancies** --*Jonathan Huggins &middot; Lester Mackey*
 > Computable Stein discrepancies (SDs) have been deployed for a variety of applications, ranging from sampler selection in posterior inference to goodness-of-fit testing.  Existing convergence-determining SDs admit strong theoretical guarantees but suffer from a computational cost that grows quadratically in the sample size. While linear-time SDs have been proposed for goodness-of-fit testing, they exhibit significant degradations in testing power---even when power is explicitly optimized. To address these shortcomings, we introduce feature Stein discrepancies (FSDs), a new family of quality measures that can be cheaply approximated using importance sampling. We show how to construct FSDs that provably determine the convergence of a sample to its target and develop high-accuracy approximations---random FSDs (RFSDs)---which are computable in near-linear time. In our experiments with sampler selection for approximate posterior inference and goodness-of-fit testing, RFSDs typically perform as well or better than quadratic-time KSDs while being orders of magnitude faster to compute.
838. **3D-Aware Scene Manipulation via Inverse Graphics** --*Shunyu Yao &middot; Tzu Ming Hsu &middot; Jun-Yan Zhu &middot; Jiajun Wu &middot; Antonio Torralba &middot; Bill Freeman &middot; Josh Tenenbaum*
 > We aim to obtain an interpretable, expressive and disentangled scene representation that contains comprehensive structural and textural information for each object. Previous representations learned by neural networks are often uninterpretable, limited to a single object, or lack 3D knowledge. In this work, we address the above issues by integrating 3D modeling into a deep generative model. We adopt a differentiable shape renderer to decode geometrical object attributes into a shape, and a neural generator to decode learned latent codes to texture. The encoder is therefore forced to perform an inverse graphics task and transform a scene image into a structured representation with 3D attributes of objects and learned texture latent codes. The representation supports reconstruction and a variety of 3D-aware scene manipulation applications. The disentanglement of structure and texture in our representation allows us to rotate and move objects freely while maintaining consistent texture, as well as changing the object appearance without affecting their structures. We systematically evaluate our representation and demonstrate that our editing scheme is superior to 2D counterparts.
839. **Partially-Supervised Image Captioning** --*Peter Anderson &middot; Stephen Gould &middot; Mark Johnson*
 > Image captioning models are becoming increasingly successful at describing the content of images in restricted domains. However, if these models are to function in the wild --- for example, as aids for the visually impaired --- a much larger number and variety of visual concepts must be understood. In this work, we teach image captioning models new visual concepts with partial supervision, such as available from object detection and image label datasets. As these datasets contain text fragments rather than complete captions, we formulate this problem as learning from incomplete data. To flexibly characterize our uncertainty about the unobserved complete sequence, we represent each incomplete training sequence with its own finite state automaton encoding acceptable completions. We then propose a novel algorithm for training sequence models, such as recurrent neural networks, on incomplete sequences specified in this manner. In the context of image captioning, our method lifts the restriction that previously required image captioning models to be trained on paired image-sentence corpora only, or otherwise required specialized model architectures to take advantage of alternative data modalities. Applying our approach to an existing neural captioning model, we achieve state of the art results on the novel object captioning task using the COCO dataset. We further show that we can train a captioning model to describe new visual concepts from the Open Images dataset while maintaining competitive COCO evaluation scores.
840. **DVAE#: Discrete Variational Autoencoders with Relaxed Boltzmann Priors** --*Arash Vahdat &middot; Evgeny Andriyash &middot; William Macready*
 > Boltzmann machines are powerful distributions that have been shown to be an effective prior over binary latent variables in variational autoencoders (VAEs). However, previous methods for training discrete VAEs have used the evidence lower bound and not the tighter importance-weighted bound. We propose two approaches for relaxing Boltzmann machines to continuous distributions that permit training with importance-weighted bounds. These relaxations are based on generalized overlapping transformations and the Gaussian integral trick. Experiments on the MNIST and OMNIGLOT datasets show that these relaxations outperform previous discrete VAEs with Boltzmann priors.
841. **Symbolic Graph Reasoning Meets Convolutions** --*Xiaodan Liang &middot; Zhiting Hu &middot; Hao Zhang &middot; Liang Lin &middot; Eric Xing*
 > Beyond local convolution networks, we explore how to harness various external human knowledge for endowing the networks with the capability of semantic global reasoning. Rather than using separate graphical models (e.g. CRF) or constraints for modeling broader dependencies, we propose a new Symbolic Graph Reasoning (SGR) layer, which performs reasoning over a group of symbolic nodes whose outputs explicitly represent different properties of each semantic in a prior knowledge graph. To cooperate with local convolutions, each SGR is constituted by three modules: a) a primal local-to-semantic voting module where the features of all symbolic nodes are generated by voting from local representations; b) a graph reasoning module propagates information over knowledge graph to achieve global semantic coherency; c) a dual semantic-to-local mapping module learns new associations of the evolved symbolic nodes with local representations, and accordingly enhances local features. The SGR layer can be injected between any convolution layers and instantiated with distinct prior graphs. Extensive experiments show incorporating SGR significantly improves plain ConvNets on three semantic segmentation tasks and one image classification task. More analyses show the SGR layer learns shared symbolic representations for domains/datasets with the different label set given a universal knowledge graph, demonstrating its superior generalization capability.
842. **High Dimensional Linear Regression using Lattice Basis Reduction** --*Ilias Zadik &middot; David Gamarnik*
 > We consider a high dimensional linear regression problem where the goal is to efficiently recover an unknown vector \beta^* from n noisy linear observations Y=X\beta^<em>+W \in \mathbb{R}^n, for known X \in \mathbb{R}^{n \times p} and unknown W \in \mathbb{R}^n. Unlike most of the literature on this model we make no sparsity assumption on \beta^</em>. Instead we adopt a regularization based on assuming that the underlying vectors \beta^* have rational entries with the same denominator Q \in \mathbb{Z}_{&gt;0}. We call this Q-rationality assumption.  We propose a new polynomial-time algorithm for this task which is based on the seminal Lenstra-Lenstra-Lovasz (LLL) lattice basis reduction algorithm.  We establish that under the Q-rationality assumption, our algorithm recovers exactly the vector \beta^* for a large class of distributions for the iid entries of X and non-zero noise W. We prove that it is successful under small noise, even when the learner has access to only one observation (n=1). Furthermore, we prove that in the case of the Gaussian white noise for W, n=o(p/\log p) and Q sufficiently large, our algorithm tolerates a nearly optimal information-theoretic level of the noise.
843. **Collaborative Learning for Deep Neural Networks** --*Guocong Song &middot; Wei Chai*
 > We introduce collaborative learning in which multiple classifier heads of the same network are simultaneously trained on the same training data to improve generalization and robustness to label noise with no extra inference cost. It acquires the strengths from auxiliary training, multi-task learning and knowledge distillation. There are two important mechanisms involved in collaborative learning. First, the consensus of multiple views from different classifier heads on the same example provides supplementary information as well as regularization to each classifier, thereby improving generalization. Second, intermediate-level representation (ILR) sharing with backpropagation rescaling aggregates the gradient flows from all heads, which not only reduces training computational complexity, but also facilitates supervision to the shared layers. The empirical results on CIFAR and ImageNet datasets demonstrate that deep neural networks learned as a group in a collaborative way significantly reduce the generalization error and increase the robustness to label noise.
844. **Entropy and mutual information in models of deep neural networks** --*Marylou Gabrié &middot; Andre Manoel &middot; Clément  Luneau &middot; jean barbier &middot; Nicolas Macris &middot; Florent Krzakala &middot; Lenka Zdeborová*
 > We examine a class of deep learning models with a tractable method to compute information-theoretic quantities. Our contributions are three-fold: (i) We show how entropies and mutual informations can be derived from heuristic statistical physics methods, under the assumption that weight matrices are independent and orthogonally-invariant. (ii) We extend particular cases in which this result is known to be rigorously exact by providing a proof for two-layers networks with Gaussian random weights, using the recently introduced adaptive interpolation method. (iii) We propose an experiment framework with generative models of synthetic datasets, on which we train deep neural networks with a weight constraint designed so that the assumption in (i) is verified during learning. We study the behavior of entropies and mutual information throughout learning and conclude that, in the proposed setting, the relationship between compression and generalization remains elusive.
845. **Generating Informative and Diverse Conversational Responses via Adversarial Information Maximization** --*Yizhe Zhang &middot; Michel Galley &middot; Zhe Gan &middot; Xiujun Li &middot; Chris Brockett &middot; Jianfeng Gao &middot;  *
 > Responses generated by neural conversational models tend to lack informativeness and diversity. We present Adversarial Information Maximization (AIM), an adversarial learning framework that addresses these two related but distinct problems. To foster response diversity, we leverage adversarial training that allows distributional matching of synthetic and real responses. To improve informativeness, our framework explicitly optimizes a variational lower bound on pairwise mutual information between query and response. Empirical results from automatic and human evaluations demonstrate that our methods significantly boost informativeness and diversity.
846. **Simple random search of static linear policies is competitive for reinforcement learning** --*Horia Mania &middot; Aurelia Guy &middot; Benjamin Recht*
 > We introduce a random search method for training static, linear policies for continuous control problems, matching state-of-the-art sample efficiency on the benchmark MuJoCo locomotion tasks. We evaluate the performance of our method over hundreds of random seeds and many different hyperparameter configurations for each benchmark task. Our simulations highlight a high variability in performance in these benchmark tasks, indicating that commonly used estimations of sample efficiency do not adequately evaluate the performance of RL algorithms.
847. **The Pessimistic Limits and Possibilities of Margin-based Losses in Semi-supervised Learning** --*Jesse Krijthe &middot; Marco Loog*
 > Consider a classification problem where we have both labeled and unlabeled data available.  We show that for linear classifiers defined by convex margin-based surrogate losses that are decreasing,  it is impossible to construct any semi-supervised approach that is able to guarantee an improvement over the supervised classifier measured by this surrogate loss on the labeled and unlabeled data. For convex margin-based loss functions that also increase, we demonstrate safe improvements are possible.
848. **Temporal Regularization for Markov Decision Process** --*  &middot; Audrey Durand &middot; Joelle Pineau &middot; Doina Precup*
 > Several applications of Reinforcement Learning suffer from instability due to high variance. This is especially prevalent in high dimensional domains. Regularization is a commonly used technique in machine learning to reduce variance, at the cost of introducing some bias. Most existing regularization techniques focus on spatial (perceptual) regularization.  Yet in reinforcement learning, due to the nature of the Bellman equation, there is an opportunity to also exploit temporal regularization based on smoothness in value estimates over trajectories.  This paper explores a class of methods for temporal regularization. We formally characterize the bias induced by this technique using Markov chain concepts. We illustrate the various characteristics of temporal regularization via a sequence of simple discrete and continuous MDPs, and show that the technique provides improvement even in high-dimensional Atari games.
849. **Enhancing the Accuracy and Fairness of Human Decision Making** --*Isabel Valera &middot; Adish Singla &middot; Manuel Gomez Rodriguez*
 > Societies often rely on human experts to take a wide variety of decisions affecting their members, from jail-or-release decisions taken by judges and stop-and-frisk decisions taken by police officers to accept-or-reject decisions taken  by academics. In this context, each decision is taken by an expert who is typically chosen uniformly at random from a pool of experts. However, these decisions may be imperfect due to limited experience, implicit biases, or faulty probabilistic reasoning. Can we improve the accuracy and fairness of the overall decision making process by optimizing the assignment between experts and decisions?
850. **Fighting Boredom in Recommender Systems with Linear Reinforcement Learning** --*Romain WARLOP &middot; Alessandro Lazaric &middot; Jérémie Mary*
 > A common assumption in recommender systems (RS) is the existence of a best fixed recommendation strategy. Such strategy may be simple and work at the item level (e.g., in multi-armed bandit it is assumed one best fixed arm/item exists) or implement more sophisticated RS (e.g., the objective of A/B testing is to find the best fixed RS and execute it thereafter). We argue that this assumption is rarely verified in practice, as the recommendation process itself may impact the user’s preferences. For instance, a user may get bored by a strategy, while she may gain interest again, if enough time passed since the last time that strategy was used. In this case, a better approach consists in alternating different solutions at the right frequency to fully exploit their potential. In this paper, we first cast the problem as a Markov decision process, where the rewards are a linear function of the recent history of actions, and we show that a policy considering the long-term influence of the recommendations may outperform both fixed-action and contextual greedy policies. We then introduce an extension of the UCRL algorithm ( L IN UCRL ) to effectively balance exploration and exploitation in an unknown environment, and we derive a regret bound that is independent of the number of states. Finally, we empirically validate the model assumptions and the algorithm in a number of realistic scenarios.
